[
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "Consulting",
    "section": "",
    "text": "From time to time I am approached about contract consulting work. This page outlines a few details that you’ll need to know. Before you read further, please realize:\n\n\n\n\n\n\nImportant\n\n\n\nThis is for decision makers looking to work with me directly. I am not interested in working with third party firms. I am happy with where I am at and what I do and don’t need/want help finding additional work.\n\n\n\nWhy me?\nI have three core competencies and passions that combine to make me very valuable.\n\n\n\n\n\n\nTechnical\n\n\n\nA good technical consultant has significant technical skills. I have expertise in a large range of technical topics including:\n\nFundamentals\n\nA/B testing\nMetric design and how measures relate to behavior\nExperimentation mindset and design\n\nFundamental modeling\n\nLife Time Value\nMulti-armed bandits\nDecision Tree processes\n\nAI Modeling\n\nDeep Learning (computer vision & NLP)\nTabular data (random forest, XGboost, linear/logistic regression, etc.)\nClustering (K-Means, Meanshift, Hierarchical, etc.)\n\nCommunication & Logistics\n\nModel and application design and deployment\nCloud Infrastructure\nDocumentation\n\n\n\n\n\n\n\n\n\n\nTeaching\n\n\n\nA good consultant is a teacher. This is important because:\n\nBuying a solution is not enough to know how to use it proficiently. A good consultant teaches you so you are not dependent on them forever.\nBusinesses are made up of people. A good consultant enriches people as they work. The more skills your people have the faster you can move. Employees that are learning and developing will be more engaged and motivated\n\n\n\n\n\n\n\n\n\nBusiness\n\n\n\nI have worked as a business process engineer. I have worked in product management. I have been a product owner. I have managed many projects as head of data science.\nI understand how to identify, prioritize, and manage what’s important. More importantly I know how to design processes to maintain the important things. When you move on to a new project or priority it’s critical that you can still maintain the value created by previous projects.\n\n\n\n\nHow to Hire\n\n\n\n\n\n\nHow to hire me\n\n\n\n\nReach out to me with a short simple description of what you are trying to accomplish and how. It doesn’t need to be formal and if you don’t know something that’s ok! I love working with people who admit when they don’t know something!\nGive me relevant links I can research so I can learn more about your problem, industry, or related problems. Before we talk I want to have done some research so I am not wasting your time or mine discussing things that could have been a google search.\nMy freelance rate is $250 per hour. I may be willing to work with you on price. Here are things that make me willing to negotiate on price:\n\nAre you doing work for the good of others (ie nonprofits, charities, other)?\nAre you a minority or female owned small business?\nAre you doing something novel, interesting, or exciting (in my opinion)?\nWhen you communicated with me, was it clear and concise? Do you have a clear vision for what you want? Unclear communication and unclear vision leads to extra back-and-forth and extra risk. As with any business I expect to be compensated for additional risk.\nHow busy am I? Or to make it sound more professional, how much demand is there in the market for my services?"
  },
  {
    "objectID": "posts/Statistics/BasicTesting.html",
    "href": "posts/Statistics/BasicTesting.html",
    "title": "Introduction To Statistical Testing",
    "section": "",
    "text": "from fastcore.all import *\nfrom IPython.display import clear_output\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom polygon import RESTClient\nfrom utils import view_source_code, get_dollars\nfrom datetime import datetime, timedelta, date\nimport math, time\n\n\npath = Path('../data')\n\n\n\nIn a previous post we created models and created actions we want to take for multiple approaches for stock trading. The question now is, how do we know if they are profitable? How should we measure them? How do we know if we simply got lucky, or if they are reliable?\nTesting is the most important part of the process. If done well you have a good way to determine what strategies should be implemented, and if done poorly you run the risk of implementing non-profitable strategies. I believe you should strive to never sacrifice testing principles because effective testing is your only objective indication to whether you are doing something useful or not. Without effective testing you are “flying blind”.\nThis post will lay the groundwork and cover the basics of testing. The goal of this post is to introduce concept and the three core things that need to be carefully considered for effective testing.\n\nWhat data should you use for testing?\nWhat metric should you use for testing?\nWhat test should you use?\n\nWe are going to walk through the concepts to see each step and understand the importance.\n\n\n\nThe first question we have to ask is what data to we use for testing? Ideally we have 3 subsets of our data (training, validation, and test). Let’s go through what they are used for and why they are important.\n\n\nThe training set is unique because it has no restrictions on what we can do with it. We can look at any piece of data in it. We can normalize data using values in the training set. We can train machine learning models on the training set. This is often the largest subset of our data.\nThis training set is pretty explanatory - we use this for understanding our data and developing our model.\nWe can load it in using the same method as we did in the previous post\n\nraw = pd.read_csv(path/'eod-quotemedia.csv',parse_dates=['date'])\ndf = raw.pivot(index='date', columns='ticker',values='adj_close')\ntrain = df.loc[:pd.Timestamp('2017-1-1')]\n\n\n\n\nThe goal of creating a trading strategy is to have it perform well on data that it was not developed using. We may use data from 2015 - 2020 to create a trading strategy, but the goal is to apply it to 2021 and 2022 to make a profit.\nBecause we want our model to perform on unseen data, we create some restriction to how we use the validation set. We do not train any models on it, and we do not use statistics or data from the validation set when creating our model. It’s data our model has never seen. The validation set is something we can only use to see how well our strategy or model performs.\nThe entire purpose of the validation set is to give us unseen data to evaluate our approaches on. By having this separate validation set we can more accurately determine what works and what doesn’t.\nWe can get our validation set using the same method as we did in the previous post\n\nvalid = df.loc[pd.Timestamp('2017-1-1'):]\n\n\n\n\nThe Test set is very similar to the validation set, but it takes things a step further. It has further restrictions in that is is the final model step before deployment. The main difference is how often you can use it. For the validation set, you can test anything on the validation set as many times as you want. For the test set you only get to look at the test set once for your particular approach.\nFor example, you may try 300 different approaches and parameter changes to your strategy to see what works best. You can check the profitability on each of them using the validation set. Then once you have chosen a strategy, you do a final check to ensure it also performs on the test set. Once you have done that you need a new test set or your project is over.\nThe reason this is important is that you want to ensure that you didn’t get lucky and find a configuration out of your 300 attempts that just happens to work on the validation set but doesn’t work elsewhere. If you try enough combinations eventually you will find something that works, but the test set gives you confidence that your model works because it’s a good strategy and not that you just tried enough things to find something that works on coincidence.\n:::{note} Many people re-use or have more lax rules on the test set. Many people do not use one at all. In this text I am laying out the ideal state I believe we should strive for. If you choose to loosen these restrictions on the test set or do without one, I would strongly encourage you to think hard about it.\nTo get our test set, we could have split our initial data into 3. Because we are a bit concerned about survivorship bias, let’s pull a new test set that uses recent data to and test how these strategies would perform over the last year and a half.\nWe need to get adjusted close price. There are a variety of services that have APIs to pull from, I have picked polgygon to use here because it’s free for what we need.\n\nclient = RESTClient(polygon_free_api_key)\n\n\nif not (path/'polytest_eod-quotemedia.csv').exists():\n    dfs = L()\n    errors = L()\n    for ticker in valid:\n        try:\n            aggs = client.get_aggs(ticker, 1, \"day\", \"2021-01-01\", \"2022-05-31\",adjusted=True)\n            close = {ticker:[o.close for o in aggs]}\n            \n            # Convert millisecond time stamp to date\n            date = L(o.timestamp/1e3 for o in aggs).map(datetime.fromtimestamp)\n            dfs.append(pd.DataFrame(close,index=date))\n        except:\n            errors.append(aggs)\n            print(f\"FAILURE: {ticker}\")\n        \n        # Free api gives 5 API calls / minute - so we need to pace our api calls!\n        time.sleep(60/5)\n    df_test = pd.concat(dfs,axis=1)\n    df_test.to_csv(path/'polytest_eod-quotemedia.csv')\n\ndf_test = pd.read_csv(path/'polytest_eod-quotemedia.csv',index_col=0,parse_dates=True)\ndf_test.index.name='date'\ndf_test.columns.name='ticker'\n\n\ndf_test.iloc[:5,:5]\n\n\n\n\n\n  \n    \n      ticker\n      A\n      AAL\n      AAP\n      AAPL\n      ABBV\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2021-01-04\n      118.64\n      15.13\n      157.34\n      129.41\n      105.41\n    \n    \n      2021-01-05\n      119.61\n      15.43\n      157.17\n      131.01\n      106.50\n    \n    \n      2021-01-06\n      122.89\n      15.52\n      166.25\n      126.60\n      105.58\n    \n    \n      2021-01-07\n      126.16\n      15.38\n      167.67\n      130.92\n      106.71\n    \n    \n      2021-01-08\n      127.06\n      15.13\n      170.06\n      132.05\n      107.27\n    \n  \n\n\n\n\n\n\n\n\nNow that we understand what data we will use for testing, let’s start figuring out how well our first model from the previous post performs\nThe next step is to figure out an appropriate metric. There are a variety of ways to measure this and we will walk through a few first steps in this section\n\n\nLet’s take our first model from the previous post and measure how well it does in terms of dollars. After all dollars is what we want to make, so it seems like a reasonable starting point.\n\nfrom SimpleTimeSeries import get_momentum_actions\n\n\nvalid_mom = get_momentum_actions(valid,28,0.08).reset_index()\n\n\ntransactions = pd.melt(valid_mom,id_vars='date',value_name='action')\ntransactions = transactions.loc[transactions.action != '']\ntransactions.columns = ['open_date'] + L(*transactions.columns[1:])\n\n\ntransactions.head()\n\n\n\n\n\n  \n    \n      \n      open_date\n      ticker\n      action\n    \n  \n  \n    \n      0\n      2017-02-14\n      A\n      Buy\n    \n    \n      2\n      2017-02-16\n      A\n      Buy\n    \n    \n      11\n      2017-03-02\n      A\n      Buy\n    \n    \n      13\n      2017-03-04\n      A\n      Buy\n    \n    \n      14\n      2017-03-07\n      A\n      Buy\n    \n  \n\n\n\n\nNow we have a dataframe with all the positions we are going to take and when to take them. But we are missing one crucial piece! When should we close those positions. We cannot make money by simplying buying a stock (ignoring dividends for now) - the profit comes when we actually close the position and sell the stock. Let’s close the position 28 days after opening.\n\ntransactions['close_date'] = transactions.open_date + timedelta(28)\ntransactions = transactions.loc[transactions.open_date < (transactions.open_date.max() - timedelta(28))]\n\n\ntransactions.head()\n\n\n\n\n\n  \n    \n      \n      open_date\n      ticker\n      action\n      close_date\n    \n  \n  \n    \n      0\n      2017-02-14\n      A\n      Buy\n      2017-03-14\n    \n    \n      2\n      2017-02-16\n      A\n      Buy\n      2017-03-16\n    \n    \n      11\n      2017-03-02\n      A\n      Buy\n      2017-03-30\n    \n    \n      13\n      2017-03-04\n      A\n      Buy\n      2017-04-01\n    \n    \n      14\n      2017-03-07\n      A\n      Buy\n      2017-04-04\n    \n  \n\n\n\n\nNext we need to get the stock price on the date of our initial action when we open to position, as well as when we close our position. Let’s start with the price on the day we open.\n\ndf_valid_long = valid.melt(var_name='ticker',value_name='adj_close',ignore_index=False).reset_index()\ndf_valid_long.columns = ['dte','ticker','adj_close']\n\n\ntransactions['open_date'] = pd.to_datetime(transactions.open_date)\ndf_valid_long['dte']      = pd.to_datetime(df_valid_long.dte)\npd.merge(left=transactions,left_on=['open_date','ticker'],\n         right=df_valid_long,right_on=['dte','ticker'],\n         how='left').head(10)\n\n\n\n\n\n  \n    \n      \n      open_date\n      ticker\n      action\n      close_date\n      dte\n      adj_close\n    \n  \n  \n    \n      0\n      2017-02-14\n      A\n      Buy\n      2017-03-14\n      2017-02-14\n      49.703267\n    \n    \n      1\n      2017-02-16\n      A\n      Buy\n      2017-03-16\n      2017-02-16\n      50.147135\n    \n    \n      2\n      2017-03-02\n      A\n      Buy\n      2017-03-30\n      2017-03-02\n      50.679775\n    \n    \n      3\n      2017-03-04\n      A\n      Buy\n      2017-04-01\n      NaT\n      NaN\n    \n    \n      4\n      2017-03-07\n      A\n      Buy\n      2017-04-04\n      2017-03-07\n      50.512092\n    \n    \n      5\n      2017-03-11\n      A\n      Buy\n      2017-04-08\n      NaT\n      NaN\n    \n    \n      6\n      2017-03-16\n      A\n      Buy\n      2017-04-13\n      2017-03-16\n      52.327016\n    \n    \n      7\n      2017-03-18\n      A\n      Buy\n      2017-04-15\n      NaT\n      NaN\n    \n    \n      8\n      2017-05-24\n      A\n      Buy\n      2017-06-21\n      2017-05-24\n      58.568656\n    \n    \n      9\n      2017-05-25\n      A\n      Buy\n      2017-06-22\n      2017-05-25\n      58.637875\n    \n  \n\n\n\n\nUh oh - We have a join that isn’t working correctly and get NaT and NaN! We created our model assuming that we could make transactions any day we want, but the stock market is not open every day. There are limitations to when we can trade openly in the stock market we need to start accounting for.\nWhen we trade using the adjusted close price we added a day because we wouldn’t be able to actually place the trade until the following day. If that day ended up being a Saturday in reality we would have to wait until Monday to place that trade (assuming that monday isn’t a holiday).\nLet’s fix that by getting the next available trading day for each date. Because we know this same thing applies to our close_date, we will fix it there as well.\n\ndef get_next_trading_day(dte,unique_dates):\n    _dates_dict = unique_dates.val2idx()\n    for i in range(10):\n        out = _dates_dict.get(dte.date()+timedelta(i),False) \n        if out != False: return unique_dates[out]\n    raise Exception\n\n\nf = bind(get_next_trading_day,unique_dates=L(*valid.index.date))\n\ntransactions['open_date'] = transactions.open_date.apply(f)\ntransactions['close_date'] = transactions.close_date.apply(f)\n\n\ntransactions['open_date'] = pd.to_datetime(transactions.open_date)\ntransactions['close_date'] = pd.to_datetime(transactions.close_date)\n\nNow we can merge in the price correctly!\n\ntransactions = pd.merge(left=transactions,left_on=['open_date','ticker'],\n                         right=df_valid_long,right_on=['dte','ticker'],\n                          how='left')\ntransactions = pd.merge(left=transactions,left_on=['close_date','ticker'],\n                         right=df_valid_long,right_on=['dte','ticker'],\n                          how='left',\n                          suffixes=('_atOpen','_atClose'))\n\n\ntransactions[['ticker','action',\n             'dte_atOpen','adj_close_atOpen',\n             'dte_atClose','adj_close_atClose']].head(3)\n\n\n\n\n\n  \n    \n      \n      ticker\n      action\n      dte_atOpen\n      adj_close_atOpen\n      dte_atClose\n      adj_close_atClose\n    \n  \n  \n    \n      0\n      A\n      Buy\n      2017-02-14\n      49.703267\n      2017-03-14\n      51.498464\n    \n    \n      1\n      A\n      Buy\n      2017-02-16\n      50.147135\n      2017-03-16\n      52.327016\n    \n    \n      2\n      A\n      Buy\n      2017-03-02\n      50.679775\n      2017-03-30\n      52.593336\n    \n  \n\n\n\n\nOk - now let’s figure out out profit. I am going to create various columns\n\ndef f_committed(x):\n    if x.action in ('Buy','Short'): return x.adj_close_atOpen  \n    else: return 0\ntransactions['committed'] = transactions.apply(f_committed,axis=1)\n\ndef f_revenue(x):\n    if x.action==\"Buy\": return x.adj_close_atClose\n    else:               return x.adj_close_atOpen\ntransactions['revenue'] = transactions.apply(f_revenue,axis=1)\n\ndef f_cost(x):\n    if x.action == 'Buy': return x.adj_close_atOpen  \n    else:                 return x.adj_close_atClose\ntransactions['cost'] = transactions.apply(f_cost,axis=1)\n\ntransactions['profit'] = transactions.revenue - transactions.cost\n\n\ntransactions['committed'] = transactions.apply(f_committed,axis=1)\ntransactions['revenue'] = transactions.apply(f_revenue,axis=1)\ntransactions['cost'] = transactions.apply(f_cost,axis=1)\ntransactions['profit'] = transactions.revenue - transactions.cost\n\n\nget_dollars(transactions[transactions.action=='Buy'].profit.sum()), \\\nget_dollars(transactions[transactions.action=='Short'].profit.sum()), \\\nget_dollars(transactions.profit.sum())\n\n('$7457.36', '$190.98', '$7648.35')\n\n\nGreat! So according to our validation set we made a good chunk of profit (pre-tax). We could buy/short in higher volumes (ie Buy = 10x buys, Short = 10x shorts) to make this profit larger.\nHowever, this really isn’t enough information to determine whether that is a good idea of feasible. I would love to loan someone 100 dollars if they would give me one-thousand dollars back a week later. I would hate to loan someone 1,000,000 dollars on the promise that they would pay me 1,000,900 dollars back in 20 years. The reward just wouldn’t be worth the risk, and I can use that money better in a 20-year span than that.\nLet’s see if we can come up with a better metric that accounts for this.\n\n\n\nInstead of measuring raw dollars, lets consider how much money (capital) we needed in order to make that 90 dollars profit. To do this we need to keep track of our money more carefully than just looking at how much we made at the end. Let’s track out financials by day instead of by transaction o calculate this.\n\n\n\n\n\n\nNote\n\n\n\nI am using “committed” to be the amount we have invested + the amount leveraged. For now, let’s assume that we won’t take out debt and borrow stocks (shorting) if we do not have the capital to cover the initial price we borrowed at\n\n\n\ndf = pd.DataFrame()\nfor cols in [['open_date','committed'],\n             ['close_date','profit'],\n             ['close_date','revenue'],\n             ['open_date','cost']]:\n    _tmp = transactions[cols].groupby(cols[0]).sum()\n    df = pd.merge(df,_tmp,how='outer',left_index=True,right_index=True)\ndf.fillna(0,inplace=True)\ndf.sort_index(inplace=True)\ndf.sample(8)\n\n\n\n\n\n  \n    \n      \n      committed\n      profit\n      revenue\n      cost\n    \n  \n  \n    \n      2017-03-08\n      10584.689356\n      0.000000\n      0.000000\n      10583.600318\n    \n    \n      2017-03-29\n      5225.468769\n      -93.792090\n      11510.549118\n      5250.575701\n    \n    \n      2017-02-16\n      13205.019794\n      0.000000\n      0.000000\n      13187.821518\n    \n    \n      2017-05-04\n      12577.980970\n      -17.387456\n      3561.027174\n      12536.763156\n    \n    \n      2017-03-30\n      3812.650344\n      -42.581923\n      16811.036857\n      3830.179057\n    \n    \n      2017-05-16\n      13842.232885\n      114.058575\n      4165.332503\n      13868.190505\n    \n    \n      2017-05-01\n      6789.531992\n      154.664868\n      5354.190170\n      6754.560876\n    \n    \n      2017-06-15\n      0.000000\n      203.997769\n      10639.412827\n      0.000000\n    \n  \n\n\n\n\nNow we can easily look at when we had the most committed. We are subtracting revenue because once we get money back we can reinvest rather than using new money.\n\ncapital_needed = (df.committed.cumsum()-df.revenue.cumsum()).max()\nget_dollars(capital_needed)\n\n'$274744.63'\n\n\nAnd of course our profit is still the same as we had before because we are just aggregating the data differently.\n\n\n\n\n\n\nTip\n\n\n\nThis is the first time we are using the fastcore’s testing framework. It has several handy and easy to use testing functions, such as testing if 2 numbers are arbitrarily close (useful for floats).\n\n\n\ntest_close(transactions.profit.sum(),df.profit.sum())\nget_dollars(df.profit.sum())\n\n'$7648.35'\n\n\nNow that we see our capital needed and our profit, let’s calculate a percent return\n\nf\"Percent Return: {(df.profit.sum() / capital_needed) * 100:.2f}%\"\n\n'Percent Return: 2.78%'\n\n\n\n\n\nMore commonly rather than using the percent return we want to use the log return. There are a lot of reasons they are advantageous to use, but for now we will cover one that is immediately useful to us.\nSymmetry / Additive\n\nPercent Return\n\nInvest 100 dollars\nGet 50% Return on investment 1 and reinvest\nGet -50% Return on investment 2\nEnd with 75 dollars\n\nLog Return\n\nInvest 100 dollars\nGet 50% Return on investment 1 and reinvest\nGet -50% Return on investment 2\nEnd with 100 dollars\n\n\nThis property where a positive return + an equal-sized negative return = no return makes it much easier to look at returns and figure out if you are winning or losing. Just add up all your log returns to get your overall log return. You have to be much more careful with percent returns.\n\npt = capital_needed + df.profit.sum()\npt_1 = capital_needed\nlog_return = math.log(pt/pt_1)\nf\"{math.log(pt/pt_1)*100:.2f}%\"\n\n'2.75%'\n\n\nAs we calculate the log return we see we get a very similar value to our percent return, but it’s not exactly the same. The advantage of using the log return however is we can accurate get an estimated annualized return.\nThis is great because we can easily have everything thought of as an annualized return so that we have a common time frame to compare investment strategies more easily.\n\ntime_period = (df.index.max().date() - df.index.min().date()).days\n\n\nf\"{(log_return/time_period * 365)*100:.2f}%\"\n\n'7.37%'\n\n\nNow we can just convert to normal return to compare very simply to S&P 500 annual return\n\nf\"{(np.exp(log_return/time_period * 365)-1)*100:.2f}%\"\n\n'7.65%'\n\n\nGo ahead an look up S&P 500 annual returns for each year online and compare. How does this fare?\n\n\n\n\n\n\nIf you bought fifty 1.50 dollar lottery tickets and won 10 million dollars in the lottery, what could you say about your chances to win? Well let’s calculate our rate of return.\n\\[\\frac{10,000,000 - 75}{75} = 133,332.33\\]\nSo based on our calculations, the rate of return for playing the lottery is fantastic. But we know that this doesn’t really reflect reality or mean that it’s a good safe investment strategy. We know that you would’ve just gotten lucky.\nSo how do we determine if our trading strategy is a good strategy, or we just got lucky this time? This is where statistical testing comes in.\nI will cover the basics that I think are key, but if you’d like more detail and practice I reading Statistics: Unlocking the Power of Data. Unlike most statistics books it is extremely applied and focused on data, with lots of real world examples.\n\n\n\n\n\nStatistical testing can be done be done using 2 general approaches. The first is the classical approach with is the most widely known. The second is through bootstrapping. This post will focus on bootstrapping because in my opinion it is the first that should be learned.\nBootstrapping is less commonly accepted but is the more powerful and flexible of the two. In bootstrapping you create a sampling distribution by by taking many samples and performing an experiment. In traditional testing you use clever algebra to approximate a sampling distribution. With todays computer you can almost always use bootstrapping and I believe it it the more powerful and flexible approach.\nWith Bootstrapping you start with just the data and make no other assumptions about the data. With classical methods you start with the data and some assumptions about the data in order to arrive at an answer. If you make incorrect assumptions then you get an incorrect answer - and determining which assumptions are safe to make is not always trivial.\nThe idea of bootstrapping and the power of it has also been spoken of by many of the statistical greats for almost 100 years, much longer than it was feasible to do, as pointed out in this article.\nFor example, that article points out that in 1936 Sir R.A. Fisher spoke about using this bootstrapping approach:\n\nActually, the statistician does not carry out this very simple and very tedious process, but his conclusions have no justification beyond the fact that they agree with those which could have been arrived at by this elementary method.\n\nHowever while these methods were tedious in 1936, they are trivial thanks to modern computers. We no longer have to do clever algebraic tricks to approximate a sampling distribution - we can just create a sampling distribution.\nThat said, I do believe it’s good and useful to have the ability to use both approaches. Luckily, traditional testing is just algebraic tricks to approximate what we will do here and so it will not be too hard to pick that up on top of bootstrapping. In this section we will focus on Bootstrapping because in my opinion it is the one that should be learned first.\n\n\n\nWith any type of testing we need to understand what we are testing. We are going to have 2 competing hypothesis. The first in the null hypothesis. The null hypothesis is what we assume to be true, until we have evidence otherwise.\nIn the context of trading strategies we might make the hypothesis that our strategy will yield returns equal to that of randomly picking stocks. If we randomly pick stocks we know that it will roughly match the market so that seems like a reasonable thing to want to beat. The null hypothesis is important because our goal of testing is to determine whether we have sufficient evident to reject that null hypothesis.\nThe alternate hypothesis is what we are testing for. For example, if the null hypothesis is that our strategy will yield returns equal that that of investing in random stocks the alternate hypothesis could be that our strategy will yield returns greater than if we were to invest in random stocks.\nFirst we need a few functions:\n\nA function to run our null hypothesis model (random)\nA function to run the model we created and have been exploring throughout the post\nA function to measure the statistic of each run (log return of the models)\n\n\n\nLet’s design a Null Hypothesis. There are many Null Hypothesis’ you can choose, and one of the beauty of bootstrapping is that you can think about it very intuitively and design the experiment exactly how you want easier.\n\ndef run_random(input_df,buy_chance,short_chance,hold_time):\n    input_df_l = input_df.melt(var_name='ticker',value_name='adj_close',\n                                  ignore_index=False).reset_index()\n    \n    # Buy/Short stocks using random chance, probabilities to be passed as arguements\n    buys = input_df_l.loc[np.random.rand(input_df_l.shape[0])<buy_chance].copy()\n    shorts = input_df_l.loc[np.random.rand(input_df_l.shape[0])<short_chance].copy()\n\n    # Concatenate into 1 dataframe\n    buys['action'] = 'Buy'\n    shorts['action'] = 'Short'\n    trans = pd.concat([buys[['date','ticker','action']],\n                     shorts[['date','ticker','action']]])\n    \n    # Close the positions\n    trans['close_date'] = trans.date + timedelta(hold_time)\n    trans = trans.loc[trans.date < (trans.date.max() - timedelta(hold_time))]\n    \n    # Adjust dates for when market is open\n    f = bind(get_next_trading_day,unique_dates=L(*input_df.index.date))\n    \n    trans['date'] = pd.to_datetime(trans.date.apply(f))\n    trans['close_date'] = pd.to_datetime(trans.close_date.apply(f))\n    return trans\n\n\n\n\nWe also need a model that we are hoping performs well that we are trying to evaluate whether it is better than our random null hypothesis or not. For this, we will just use the model we have been working on throughout the post.\n\ndef run_model(input_df,n_periods,threshold,hold_time):\n    valid_mom = get_momentum_actions(input_df,n_periods,threshold).reset_index()\n    trans = pd.melt(valid_mom,id_vars='date',value_name='action')\n    trans = trans.loc[trans.action != '']\n    trans['close_date'] = trans.date + timedelta(hold_time)\n    trans = trans.loc[trans.date < (trans.date.max() - timedelta(hold_time))]\n\n    f = bind(get_next_trading_day,unique_dates=L(*input_df.index.date))\n    trans['date'] = pd.to_datetime(trans.date.apply(f))\n    trans['close_date'] = pd.to_datetime(trans.close_date.apply(f))\n    return trans\n\n\n\n\nNext, we need a function that can take our 2 different model outputs (for null and alternate hypothesis) and calculate the statistic we care about (log return). We do this by drawing on our work earlier in the post.\n\ndef calculate_returns(trans,input_df):\n    input_df_l = input_df.melt(var_name='ticker',value_name='adj_close',\n                                  ignore_index=False).reset_index()\n    # Merge in stock prices\n    trans = pd.merge(left=trans,left_on=['date','ticker'],\n                             right=input_df_l,right_on=['date','ticker'],\n                              how='left')\n\n    trans = pd.merge(left=trans,left_on=['close_date','ticker'],\n                             right=input_df_l,right_on=['date','ticker'],\n                              how='left',\n                              suffixes=('_atOpen','_atClose'))\n    trans = trans[['ticker','action',\n                   'date_atOpen' ,'adj_close_atOpen',\n                   'date_atClose','adj_close_atClose']]\n\n    # Calculate profit for each transaction\n    trans['committed'] = trans.apply(f_committed,axis=1)\n    trans['revenue'] = trans.apply(f_revenue,axis=1)\n    trans['cost'] = trans.apply(f_cost,axis=1)\n    trans['profit'] = trans.revenue - trans.cost\n\n    # Create Daily dataframe\n    df_daily = pd.DataFrame()\n    for cols in [['date_atOpen','committed'],['date_atClose','profit'],\n                 ['date_atClose','revenue'],['date_atOpen','cost']]:\n        _tmp = trans[cols].groupby(cols[0]).sum()\n        df_daily = pd.merge(df_daily,_tmp,how='outer',\n                            left_index=True,right_index=True)\n    df_daily.fillna(0,inplace=True)\n    df_daily.sort_index(inplace=True)\n    \n    # Calculate Log Return\n    pt_1 = (df_daily.committed.cumsum()-df_daily.revenue.cumsum()).max()\n    pt = pt_1 + df_daily.profit.sum()\n    return math.log(pt/pt_1)\n\ncalculate_returns(run_model(valid.iloc[:,:5],28,0.08,28),valid.iloc[:,:5]),\\\ncalculate_returns(run_random(valid.iloc[:,:5],.1,0.05,28),valid.iloc[:,:5]),\n\n(0.01999690938292933, 0.018885915383428743)\n\n\n\n\n\nNow that we have the setup let’s start our test. First for our random model we need to give it a buy and short chance probability. Let’s assume roughly even volume of trades to keep things simple and get the probabilities from our model. In this way our random model trades at the same volume and frequency, just using random selection instead of momentum selection.\n\ndf_long = valid.melt(var_name='ticker',value_name='adj_close',\n                     ignore_index=False).reset_index()\nt = run_model(valid,28,0.08,28)\nbuy_chance = sum(t.action=='Buy')/len(df_long)\nshort_chance = sum(t.action=='Short')/len(df_long)\nprint(f\"Model buy percentage={buy_chance}\")\nprint(f\"Model buy percentage={short_chance}\")\n\nModel buy percentage=0.08691717171717171\nModel buy percentage=0.04311919191919192\n\n\nGreat - Let’s bind these to a function. This is a functional programming principle that is extremely common in functional languages in the form of currying. Python isn’t quite as convenient so we use fastcore’s bind for this (which is like the more common functool partial, with a few added conveniences).\nThis allows us to pass our model functions as parameters which will be convenient when we want to try different parameters. We could also just have all the parameters for either function as arguments in the main run_bootstrap_test function, but that can get rather clunky and confusing.\n\nmodel_h0 = bind(run_random,\n                buy_chance=buy_chance,\n                short_chance=short_chance,\n                hold_time=28)\n\nNow we get to the meat of bootstrapping. Really all we are doing is tacking different samples and running both models many times on different samples. In classical statistics the normal distribution or some other distribution is assumed so that we can approximate a sampling distribution. In bootstrapping we just create the sampling distribution directly.\n\ndef run_bootstrap_test(input_df,model_h0,runs,sample_size):\n    r_h0 = L()\n    for i in range(runs):\n        ticker_sample = np.random.choice(input_df.columns,sample_size) \n        valid_sample = input_df[ticker_sample]\n        r_h0.append(calculate_returns(model_h0(valid_sample),input_df))\n    return r_h0\n\n\n\n\nIt’s always helpful to visualize whatever you can. Visualizing the data will help you build intuition, generate additional ideas, spot outliers, and identify possible errors in your code. Let’s start with plotting our initial parameters we are using.\n\nfig,ax = plt.subplots(figsize=(6,6))\n\nmodel_h0 = bind(run_random,\n                buy_chance=buy_chance,short_chance=short_chance,hold_time=28)\n\nmodel_h1 = bind(run_model,\n                n_periods=28,threshold=0.08,hold_time=28)\n    \nruns, ss = 200,100\nh0 = run_bootstrap_test(valid,model_h0,runs=runs,sample_size=ss)\nax.set_xlim(-.15,.15)\n\nax.hist(h0,alpha=0.5,bins=60,label = 'h0',range=(-.15,.15))\nx = calculate_returns(model_h1(valid),valid)\nax.scatter(x=x,y=0,marker=\"P\",color='red',s=300)\n\ntitle_r1 = f\"h0 mean={np.array(h0).mean():.4f}\\n\"\ntitle_r2 = f\"h1 value={x:.4f}\"\n\nax.set_title(title_r1 + title_r2)\nax.legend(loc ='upper left') \nplt.show()\n\n\n\n\nThe blue histogram is the distribution of our random sampling. The Red is our models return. Visually the red point seems perfectly in line with the random distribution, but let’s be a bit more scientific about it. Let’s calculate the p-value, and talk about what that means.\n\np_value = (np.array(h0)>x).sum()/runs\nprint(p_value)\n\n0.425\n\n\nThe p-value tells us the percentage of samples from our randomized null hypothesis trials that are more extreme than our model’s return. Roughly 40% of the time we get a more unusual result just using our random selection - which tells us that from this look our model is pretty indistinguishable from our random model.\nSaid in statistical jargon, we failed to reject the null hypothesis.\nLet’s try something else\n\n\n\n\nfig,ax = plt.subplots(figsize=(6,6))\n\nmodel_h0 = bind(run_random,\n                buy_chance=buy_chance,short_chance=short_chance,hold_time=14)\nmodel_h1 = bind(run_model,\n                n_periods=28,threshold=0.08,hold_time=14)\n    \nruns, ss = 200,100\nh0 = run_bootstrap_test(valid,model_h0,runs=runs,sample_size=ss)\nax.set_xlim(-.15,.15)\n\nax.hist(h0,alpha=0.5,bins=60,label = 'h0',range=(-.15,.15))\nx = calculate_returns(model_h1(valid),valid)\nax.scatter(x=x,y=0,marker=\"P\",color='red',s=300)\n\ntitle_r1 = f\"h0 mean={np.array(h0).mean():.4f}\\n\"\ntitle_r2 = f\"h1 value={x:.4f}\"\n\nax.set_title(title_r1 + title_r2)\nax.legend(loc ='upper left') \nplt.show()\n\n\n\n\n\np_value = (np.array(h0)>x).sum()/runs\nprint(p_value)\n\n0.33\n\n\nThis isn’t any better. But that’s ok, because that’s how the process works. Most of your experiments will fail, and this was only our first model. Let’s try one more idea.\n\n\n\n\nfig,ax = plt.subplots(figsize=(6,6))\n\nmodel_h0 = bind(run_random,\n                buy_chance=buy_chance,short_chance=short_chance,hold_time=28)\nmodel_h1 = bind(run_model,\n                n_periods=56,threshold=0.08,hold_time=28)\n    \nruns, ss = 200,100\nh0 = run_bootstrap_test(valid,model_h0,runs=runs,sample_size=ss)\nax.set_xlim(-.15,.15)\n\nax.hist(h0,alpha=0.5,bins=60,label = 'h0',range=(-.15,.15))\nx = calculate_returns(model_h1(valid),valid)\nax.scatter(x=x,y=0,marker=\"P\",color='red',s=300)\n\ntitle_r1 = f\"h0 mean={np.array(h0).mean():.4f}\\n\"\ntitle_r2 = f\"h1 value={x:.4f}\"\n\nax.set_title(title_r1 + title_r2)\nax.legend(loc ='upper left') \nplt.show()\n\n\n\n\n\np_value = (np.array(h0)>x).sum()/runs\nprint(p_value)\n\n0.015\n\n\nNow we’re talking! A very small percentage of the values from our random model are more extreme than the one we got with our model.\nBut now this invites a new question - how small does the p-value need to be to reject the null hypothesis? 0.05 is the most common and many people use this by default, but what your p-value threshold is is a design choice and trede-off that has real implications on your testing. This is something to carefully consider. Remember, testing is our only objective signal for whether we are doing something right or not so it deserves the most care and consideration.\nLet’s consider the extremes so that we can easily thing about the trade-off.\nWhat if the p-value threshold we pick is large (ie 0.5)?*\nWith a p-value threshold of 0.5, we accept anything where less than 50% of the values from the randomized null hypothesis tests are more extreme. The risk of this means that we will have a lot of false positives. Said another way, LOTS of stuff will pass our test many of them just because of luck.\nSaid simply, we will think a lot of bad ideas are good ideas.\nWhat if the p-value threshold we pick is small (ie 0.000001)?*\nWith a p-value this small almost nothing will pass the test, but if it does we can be pretty confident in it! The problem with this is many good ideas will fail the test just because of bad luck.\nSaid simply, we will have a lot of good ideas that we won’t realize are good ideas. These are missed opportunities.\nThe Tradeoff\nSo really the P value is a trade-off. A lower P value is more cautious and safe but you will miss out on some opportunities. A high P value is riskier and more of your approaches will turn out to fail.\nIt is important to understand that this is a tradeoff. For a more detailed guide to testing read the hypothesis testing material in Statistics: Unlocking the Power of Data\n\n\n\n\n\n\nNow that we have our best model, we will test it on the test set. This is the only time we can look at it - so while we got to try lots of things out on our validation set we only get 1 shot at our test set. Let’s see how it performs\n\nfig,ax = plt.subplots(figsize=(6,6))\n\ndf_test_small = df_test[df_test.index < '2021-05-31']\n\nmodel_h0 = bind(run_random,\n                buy_chance=buy_chance,short_chance=short_chance,hold_time=28)\nmodel_h1 = bind(run_model,\n                n_periods=56,threshold=0.08,hold_time=28)\n    \nruns, ss = 200,100\nh0 = run_bootstrap_test(df_test_small,model_h0,runs=runs,sample_size=ss)\nax.set_xlim(-.15,.15)\n\nax.hist(h0,alpha=0.5,bins=60,label = 'h0',range=(-.15,.15))\nx = calculate_returns(model_h1(df_test_small),df_test_small)\nax.scatter(x=x,y=0,marker=\"P\",color='red',s=300)\n\ntitle_r1 = f\"h0 mean={np.array(h0).mean():.4f}\\n\"\ntitle_r2 = f\"h1 value={x:.4f}\"\n\nax.set_title(title_r1 + title_r2)\nax.legend(loc ='upper left') \nplt.show()\n\n\n\n\n\np_value = (np.array(h0)>x).sum()/runs\nprint(p_value)\n\n0.685\n\n\nOh man, suddenly our model failed the test on the test set after it passed on the validation set. I think it’s helpful to keep in mind this is not a “bad” result. Testing is designed to catch errors and it did - it prevented us from deploying a bad model into production and betting real dollars on it.\nIf a test never caught errors, there’d be no point in performing the test. We do this step BECAUSE it catches things like this.\n\n\n\nEntry-Level:\n\nTest different parameters of the model in the post. What is your best model on the validation set? Ho does it perform on our test set?\nModify model to only place “Buy” order and not “Short” orders. How does this effect return?\n\nMid-Level:\n\nTest the other models in post 1 and create a blog post or write up of your findings\nFind something that could have been done differently with one of the models. Change and test it and create a blog post of your findings.\nCreate an ensemble of multiple momentum models and measure the performance. Create a blog post with findings.\n\nSenior-Level:\n\nFind another type of data that momentum can be applied to, and create a model to use that as a signal. For example, maybe momentum of job postings listed by the company could be a signal of stock performance. You should:\n\nCreate train/valid/test split\nVerify data cleanliness and highlight any potential concerns for incorrectness or biases\nExplain why you are choosing the model you did and why you think it’s a good thing to test\nCreate the model and test various configurations on the validation set\nTest the best one on the test set\nWrite a blog post on what you did, why, choices you made along the way, future research that could be done on this data, and whether you think this could be a productive area for a trading firm to look at more."
  },
  {
    "objectID": "posts/APL/TabularData.html",
    "href": "posts/APL/TabularData.html",
    "title": "Tabular Data Intro",
    "section": "",
    "text": "This article will get us started reading and working with tabular data. The goal is to read in some csv data, do a few basic operations (filtering, aggregation, etc.), and create some basic plots.\n\n]box on -style=max\n\n┌→────────────────┐\n│Was ON -style=max│\n└─────────────────┘"
  },
  {
    "objectID": "posts/APL/TabularData.html#load-data-from-csv",
    "href": "posts/APL/TabularData.html#load-data-from-csv",
    "title": "Tabular Data Intro",
    "section": "2.1 Load Data from CSV",
    "text": "2.1 Load Data from CSV\nAPL has a CSV function to read csv data in that we can use. I am using simple stock ticker data for a moving average.\n\neod ← 'eod-quotemedia.csv'\neod_ar ← ⎕CSV eod '' 4\n⍴ eod_ar ⍝ Check Size\n\n┌→───────┐\n│490738 3│\n└~───────┘\n\n\n\nWe can get the head of the dataframe by selecting the first 5 rows and 3 columns. It’s handy to have a small sized piece of data as you develop so we store in it’s own array also. ↑ Lets us specify the head slice.\n\n⎕←eod_ar_s ← 5 3 ↑ eod_ar\n\n┌→──────────────────────────────────┐\n↓ ┌→───┐       ┌→─────┐ ┌→────────┐ │\n│ │date│       │ticker│ │adj_close│ │\n│ └────┘       └──────┘ └─────────┘ │\n│ ┌→─────────┐ ┌→┐                  │\n│ │2013-07-01│ │A│      29.9942     │\n│ └──────────┘ └─┘                  │\n│ ┌→─────────┐ ┌→┐                  │\n│ │2013-07-02│ │A│      29.6501     │\n│ └──────────┘ └─┘                  │\n│ ┌→─────────┐ ┌→┐                  │\n│ │2013-07-03│ │A│      29.7052     │\n│ └──────────┘ └─┘                  │\n│ ┌→─────────┐ ┌→┐                  │\n│ │2013-07-05│ │A│      30.4346     │\n│ └──────────┘ └─┘                  │\n└∊──────────────────────────────────┘"
  },
  {
    "objectID": "posts/APL/TabularData.html#filter-for-1-ticker",
    "href": "posts/APL/TabularData.html#filter-for-1-ticker",
    "title": "Tabular Data Intro",
    "section": "2.2 Filter for 1 ticker",
    "text": "2.2 Filter for 1 ticker\nNext, we need to filter for a particular ticker. The first thing we need to know is where the column we want to sort on is located. We can of course see that it’s the second column, but let’s pretend we need to use APL for this to get some practice.\n\n⎕ ← ticker_loc ← ,/ ⍸ 1⌷eod_ar ∊ ⊂'ticker'\n\n \n2\n \n\n\n\nLet’s break this down a bit:\n\n1⌷eod_ar Gets the first row - this is our header row\n∊ ⊂'ticker' Determines where ‘ticker’ is and returns a mask ([0 1 0]\n⍸ is applied to get the location of the true values (2)\n,/ Applies a concatenation to flatten from an array to a value\n\nThe next step is getting a truth mask (ie [0 1 1 0 0]) that tells us which rows contain the value to filter for, in this case AAPL\n\ntickers ← eod_ar[;ticker_loc] ∊ ⊂ 'AAPL'\n\nSimilar to our first filtering section, ∊ ⊂ 'AAPL' is checking membership of eod_ar[;ticker_loc] to return the mask.\nWe can see we found 1009 rows out of 490,738 that had this ticker.\n\n⍝ We have a truth array (0/1) of whether row contains 'AAPL' or not\n('size',⍴ tickers) , 'Found',+/tickers \n'min',(⌈/ tickers) , 'max',⌊/tickers\n\n┌→─────────────────────┐\n│size 490738 Found 1009│\n└+─────────────────────┘\n\n\n\n┌→──────────┐\n│min 1 max 0│\n└+──────────┘\n\n\n\n+/ distributes the + sign between each item in the list giving us a sum of the vector. The same approach can be used for ⌊ and ⌈ to get minimum and maximum values (0 and 1). ⍴ gives us our shape.\nNext we need to use this mask to actually filter out data. This is quite easy and we can pass our mask with our full array to do that filtering using ⌿\n\nAAPL ← tickers ⌿ eod_ar\n\nSo when we put that together our full solution is:\n\ncol_loc ← ,/ ⍸ 1⌷eod_ar ∊ ⊂'ticker'           ⍝ Column Location\nAAPL ← eod_ar ⌿⍨ eod_ar[;col_loc] ∊ ⊂ 'AAPL'  ⍝ Filter Array\n5 3 ↑ AAPL                                    ⍝ Head of array\n\n┌→────────────────────────────┐\n↓ ┌→─────────┐ ┌→───┐         │\n│ │2013-07-01│ │AAPL│ 53.1092 │\n│ └──────────┘ └────┘         │\n│ ┌→─────────┐ ┌→───┐         │\n│ │2013-07-02│ │AAPL│ 54.3122 │\n│ └──────────┘ └────┘         │\n│ ┌→─────────┐ ┌→───┐         │\n│ │2013-07-03│ │AAPL│ 54.612  │\n│ └──────────┘ └────┘         │\n│ ┌→─────────┐ ┌→───┐         │\n│ │2013-07-05│ │AAPL│ 54.1734 │\n│ └──────────┘ └────┘         │\n│ ┌→─────────┐ ┌→───┐         │\n│ │2013-07-08│ │AAPL│ 53.8658 │\n│ └──────────┘ └────┘         │\n└∊────────────────────────────┘"
  },
  {
    "objectID": "posts/APL/TabularData.html#calculate-moving-average",
    "href": "posts/APL/TabularData.html#calculate-moving-average",
    "title": "Tabular Data Intro",
    "section": "2.3 Calculate Moving Average",
    "text": "2.3 Calculate Moving Average\nWe will calculate a 90 period moving average over the stock prices for the AAPL ticker we just filtered for.\n\nws ← 90\n\nLet’s start with calculating a moving sum. Instead of using +/ to sum over the full list, we can use +⌿ to get a moving sum based on the ws we give it.\n\nmovsum ← ws +⌿ AAPL[;3]\n\nWhen we have a moving sum, a moving average is easy. Simply divide the moving sum by the window size to get an average.\n\nmovavg ← movsum ÷ ws\n'movavg length:',⍴ movavg\n'AAPL length:', ⍴ AAPL[;3]\n\n┌→─────────────────┐\n│movavg length: 920│\n└+─────────────────┘\n\n\n\n┌→────────────────┐\n│AAPL length: 1009│\n└+────────────────┘\n\n\n\nOur moving average is shorter than our original data because we are not calculating when we don’t have enough data at the beginning of our time period. Let’s pad the beginning with the raw values.\n\npadded ← ((ws-1) ↑ AAPL[;3]) , movavg\n'padded length:',⍴ padded\n'AAPL length:', ⍴ AAPL[;3]\n\n┌→──────────────────┐\n│padded length: 1009│\n└+──────────────────┘\n\n\n\n┌→────────────────┐\n│AAPL length: 1009│\n└+────────────────┘\n\n\n\nSo the full moving average with padding back to original size looks like this\n\nmovavg ← (ws ↑ AAPL[;3]) , ws ÷⍨ ws +⌿ AAPL[;3]"
  },
  {
    "objectID": "posts/APL/TabularData.html#plot",
    "href": "posts/APL/TabularData.html#plot",
    "title": "Tabular Data Intro",
    "section": "2.4 Plot",
    "text": "2.4 Plot\nWe can not plot our original data with our moving average. It’s a bit annoying I have to pass in movavg twice to get it to work. It is probably something I am not understanding about how plotting works in APL.\n\n]Plot AAPL[;3] movavg movavg\n\n\nCreated by Causeway SVG engine - SharpPlot v3.71.0\n\nPaint the paper =====\n \n  \n \nBorder =====\nRegion =====\nX-Axis Ticks =====\nX-Axis tickmarks\n \nY-Axis Ticks =====\nY-Axis tickmarks\n \nAxes =====\n \nY-axis labels\n \n  50\n  60\n  70\n  80\n  90\n  100\n  110\n  120\n  130\n  140\n  150\n  160\n \nfor X-axis labels\n \n  0\n  100\n  200\n  300\n  400\n  500\n  600\n  700\n  800\n  900\n  1000\n  1100\n \nHeading, subheading and footnotes =====\nStart of Line Chart ===========\nPoints follow ...\nLine\n \n  \n \nPoints follow ...\nLine\n \n  \n \nPoints follow ...\nLine\n \n  \n \nReset to original origin\n\n\n\n\n┌⊖┐\n⌽0│\n└~┘"
  },
  {
    "objectID": "posts/APL/TabularData.html#full-solution",
    "href": "posts/APL/TabularData.html#full-solution",
    "title": "Tabular Data Intro",
    "section": "2.5 Full Solution",
    "text": "2.5 Full Solution\n\neod_ar ← ⎕CSV 'eod-quotemedia.csv' '' 4\ncol_loc ← ,/ ⍸ 1⌷eod_ar ∊ ⊂'ticker'       \nAAPL ← eod_ar ⌿⍨ eod_ar[;col_loc] ∊ ⊂ 'AAPL'  \nmovavg ← (ws ↑ AAPL[;3]) , ws ÷⍨ (ws←90) +⌿ AAPL[;3]\n]Plot AAPL[;3] movavg movavg\n\n\nCreated by Causeway SVG engine - SharpPlot v3.71.0\n\nPaint the paper =====\n \n  \n \nBorder =====\nRegion =====\nX-Axis Ticks =====\nX-Axis tickmarks\n \nY-Axis Ticks =====\nY-Axis tickmarks\n \nAxes =====\n \nY-axis labels\n \n  50\n  60\n  70\n  80\n  90\n  100\n  110\n  120\n  130\n  140\n  150\n  160\n \nfor X-axis labels\n \n  0\n  100\n  200\n  300\n  400\n  500\n  600\n  700\n  800\n  900\n  1000\n  1100\n \nHeading, subheading and footnotes =====\nStart of Line Chart ===========\nPoints follow ...\nLine\n \n  \n \nPoints follow ...\nLine\n \n  \n \nPoints follow ...\nLine\n \n  \n \nReset to original origin\n\n\n\n\n┌⊖┐\n⌽0│\n└~┘"
  },
  {
    "objectID": "posts/APL/MatrixMultiplication.html",
    "href": "posts/APL/MatrixMultiplication.html",
    "title": "Matrix Multiplication",
    "section": "",
    "text": "I am working through a couple of linear algebra books/courses as I write this. All content in here will be heavily inspired by those resources:\n\nGilbert Strang. 18.06 Linear Algebra. Spring 2010. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA.\nApplied Linear Algebra with A. P. L. by Garry Helzer (Author)\n\nI have 2 main goals: + Learn Dyalog APL: APL works very differently than other languages I have done. By learning it I will learn another way of thinking and approaching problems. By having more ways to think and approach problems I become smarter. I want to be smarter. + Improve my mathematical foundation\nTo do this I plan to go through math material and courses in Dyalog APL. In the beginning I will be reviewing basic math while learning APL, but eventually I will get to content where I am both learning APL and math at the same time. This is where I will document what I do.\nWhere to learn APL\nCheck out the fastai apl study group, accompanying videos, and anki decks if you want to learn APL ."
  },
  {
    "objectID": "posts/APL/MatrixMultiplication.html#manual-calculation",
    "href": "posts/APL/MatrixMultiplication.html#manual-calculation",
    "title": "Matrix Multiplication",
    "section": "3.1 Manual Calculation",
    "text": "3.1 Manual Calculation\nWe take the rows of N times the columns of M to do a linear combination to do matrix multiplication.\n\\(1\\begin{bmatrix}2\\\\1\\end{bmatrix} + 2\\begin{bmatrix}5\\\\3\\end{bmatrix}\\)\nWe can do this exactly in APL and see our answer.\n\n⎕ ← (col1 ← M[;1] × N[1;]) + (col2 ← M[;2] × N[2;])\n\n┌→───┐\n│12 7│\n└~───┘"
  },
  {
    "objectID": "posts/APL/MatrixMultiplication.html#apl-calculation",
    "href": "posts/APL/MatrixMultiplication.html#apl-calculation",
    "title": "Matrix Multiplication",
    "section": "3.2 APL Calculation",
    "text": "3.2 APL Calculation\n\n3.2.1 Dot\nin APL we would normally not write is all out but would write it using the dot (.) function. Let’s figure out what that is and how it works.\nThe . applies the operators the surround it (⍺⍺ and ⍵⍵) in a specific way and order called an inner product.\nIn our matrix multiplication problem it looks like this. \\(\\begin{bmatrix}(1⍵⍵2)⍺⍺(2⍵⍵5)\\\\(1⍵⍵1)⍺⍺(2⍵⍵3)\\end{bmatrix}\\)\nLet’s walk through this in our matrix multiplication example above one operator at a time to understand it a bit better\n\n\n3.2.2 Apply the ⍵⍵ argument\nI like to visualize the first step like this:\n\\(\\begin{bmatrix}⍵⍵&&1\\\\&⍵⍵&2\\\\2&5&\\end{bmatrix}\\)\n\\(\\begin{bmatrix}⍵⍵&&1\\\\&⍵⍵&2\\\\1&3&\\end{bmatrix}\\)\nWe first apply whatever the ⍵⍵ parameter is and combine elements. Just as we did above manually we need to do multiplication here so we know the ⍵⍵ parameter must be ×. These calculation are:\nMatrix 1:\n\n1 × 2 = 2\n2 × 5 = 10\n\nMatrix 2:\n\n1 × 1 = 1\n2 × 3 = 6\n\nSo far we have ⍺⍺.×. We can show the result of our calculations above in matrices.\n\\(\\begin{bmatrix}2&⍺⍺\\\\&10\\end{bmatrix}\\)\n\\(\\begin{bmatrix}1&⍺⍺\\\\&6\\end{bmatrix}\\)\n\n\n3.2.3 Apply the ⍺⍺ argument\nThe next thing the . operator does is combine all the numbers in each of step 1 resulting matrices using ⍺⍺. To get the linear combination we did above we need to add the numbers in each matrix, so the ⍺⍺ operator must be +.\nIf we do that addition:\n\nMatrix 1: 2 + 10 = 12\nMatrix 2: 1 + 6 = 7\n\nLeaving us with our answer of \\(\\begin{bmatrix}12\\\\7\\end{bmatrix}\\)\nSo to do matrix multiplication we simply need to use:\n\nM +.× N\n\n┌→────┐\n↓17 24│\n│10 14│\n└~────┘\n\n\n\n\n\n3.2.4 Dot is flexible\nThis was just 1 example of using the . operator. We used + as ⍺⍺ and × as ⍵⍵ to fit what we needed for this problem.\nNow that we understand that, we can flip our operators and look at ×.+ instead of +.×. We can also do any number of other operators to do lots of different matrix operations. Take a look at the examples below and try calculating them by hand to see what you get!\n\n⍝ using addition.multiplication (normal matrix multiplication) \nM+.×N \n\n┌→─┐\n↓12│\n│ 7│\n└~─┘\n\n\n\n\n⍝ using multiplication.addition\nM×.+N \n\n┌→─┐\n↓21│\n│10│\n└~─┘\n\n\n\n\n⍝ using max.min\nM⌈.⌊N\n\n┌→┐\n↓2│\n│2│\n└~┘\n\n\n\n\n⍝ using addition.subtraction\nM-.+N\n\n┌→─┐\n↓¯4│\n│¯3│\n└~─┘\n\n\n\n\n⍝ using exponent.division\nM*.÷N\n\n┌→──────────┐\n↓5.656854249│\n│1          │\n└~──────────┘\n\n\n\n\n⍝ using factorial.natural_log\nM!.⍟N\n\n┌→───────────┐\n↓1           │\n│0.6309297536│\n└~───────────┘"
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html",
    "title": "Numerical Linear Algebra Part 1",
    "section": "",
    "text": "This notebook is a copy of a notebook from the fast.ai fast.ai Numerical linear algebra course. I have modified it to use APL as a learning experience for myself.\nCheck out the original notebook here\nI do not have any business affiliation with fast.ai, and this notebook is not an official fast.ai notebook.\nYou can read an overview of this Numerical Linear Algebra course in this blog post. The course was originally taught in the University of San Francisco MS in Analytics graduate program. Course lecture videos are available on YouTube (note that the notebook numbers and video numbers do not line up, since some notebooks took longer than 1 video to cover).\nYou can ask questions about the course on our fast.ai forums."
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#why-study-numerical-linear-algebra",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#why-study-numerical-linear-algebra",
    "title": "Numerical Linear Algebra Part 1",
    "section": "1.1 Why study Numerical Linear Algebra?",
    "text": "1.1 Why study Numerical Linear Algebra?\nKey Question of this course: How can we do matrix computations with acceptable speed and acceptable accuracy?\nA list of the Top 10 Algorithms of science and engineering during the 20th century includes: the matrix decompositions approach to linear algebra. It also includes the QR algorithm, which we’ll cover, and Krylov iterative methods which we’ll see an example of. (See here for another take)\n (source: Top 10 Algorithms)\nThere are 4 things to keep in mind when choosing or designing an algorithm for matrix computations: - Memory Use - Speed - Accuracy - Scalability/Parallelization\nOften there will be trade-offs between these categories.\n\n1.1.1 Motivation\nMatrices are everywhere– anything that can be put in an Excel spreadsheet is a matrix, and language and pictures can be represented as matrices as well. Knowing what options there are for matrix algorithms, and how to navigate compromises, can make enormous differences to your solutions. For instance, an approximate matrix computation can often be thousands of times faster than an exact one.\nIt’s not just about knowing the contents of existing libraries, but knowing how they work too. That’s because often you can make variations to an algorithm that aren’t supported by your library, giving you the performance or accuracy that you need. In addition, this field is moving very quickly at the moment, particularly in areas related to deep learning, recommendation systems, approximate algorithms, and graph analytics, so you’ll often find there’s recent results that could make big differences in your project, but aren’t in your library.\nKnowing how the algorithms really work helps to both debug and accelerate your solution."
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#matrix-computations",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#matrix-computations",
    "title": "Numerical Linear Algebra Part 1",
    "section": "1.2 Matrix Computations",
    "text": "1.2 Matrix Computations\nThere are two key types of matrix computation, which get combined in many different ways. These are: - Matrix and tensor products - Matrix decompositions\nSo basically we’re going to be combining matrices, and pulling them apart again!\n\n1.2.1 Matrix and Tensor Products\n\n1.2.1.1 Matrix-Vector Products:\nThe matrix below gives the probabilities of moving from 1 health state to another in 1 year. If the current health states for a group are: - 85% asymptomatic - 10% symptomatic - 5% AIDS - 0% death\nwhat will be the % in each health state in 1 year?\n(Source: Concepts of Markov Chains)\n\n\n1.2.1.2 Answer\n\n1.2.1.2.1 Set up Data\nCreate category names vector for display purposes\n\n]box on -style=max\n\n┌→────────────────┐\n│Was ON -style=max│\n└─────────────────┘\n\n\n\n\nNames←'Asymptomatic' 'Symptomatic' 'Aids'  'Death'\n\nCreate current health states array\n\nNames⍪Start←1 4⍴.85 .1 .05 0\n\n┌→────────────────────────────────────────────┐\n↓ ┌→───────────┐ ┌→──────────┐ ┌→───┐ ┌→────┐ │\n│ │Asymptomatic│ │Symptomatic│ │Aids│ │Death│ │\n│ └────────────┘ └───────────┘ └────┘ └─────┘ │\n│                                             │\n│ 0.85           0.1           0.05   0       │\n│                                             │\n└∊────────────────────────────────────────────┘\n\n\n\nCreate transition table/stochastic matrix\n\nTransitions←4 4⍴.9 .07 .02 .01 0 .93 .05 .02 0 0 .85 .15 0 0 0 1\n((⊂'States')⍪Names)⍪(Names,Transitions)\n\n┌→───────────────────────────────────────────────────────────┐\n↓ ┌→─────┐       ┌→───────────┐ ┌→──────────┐ ┌→───┐ ┌→────┐ │\n│ │States│       │Asymptomatic│ │Symptomatic│ │Aids│ │Death│ │\n│ └──────┘       └────────────┘ └───────────┘ └────┘ └─────┘ │\n│ ┌→───────────┐                                             │\n│ │Asymptomatic│ 0.9            0.07          0.02   0.01    │\n│ └────────────┘                                             │\n│ ┌→──────────┐                                              │\n│ │Symptomatic│  0              0.93          0.05   0.02    │\n│ └───────────┘                                              │\n│ ┌→───┐                                                     │\n│ │Aids│         0              0             0.85   0.15    │\n│ └────┘                                                     │\n│ ┌→────┐                                                    │\n│ │Death│        0              0             0      1       │\n│ └─────┘                                                    │\n└∊───────────────────────────────────────────────────────────┘\n\n\n\n\n\n1.2.1.2.2 Answer Calculation\nMultiply together to get ending health states\n\nNames⍪End←Start+.×Transitions\n\n┌→────────────────────────────────────────────┐\n↓ ┌→───────────┐ ┌→──────────┐ ┌→───┐ ┌→────┐ │\n│ │Asymptomatic│ │Symptomatic│ │Aids│ │Death│ │\n│ └────────────┘ └───────────┘ └────┘ └─────┘ │\n│                                             │\n│ 0.765          0.1525        0.0645 0.018   │\n│                                             │\n└∊────────────────────────────────────────────┘\n\n\n\n\n\n\n1.2.1.3 Matrix-Matrix Products\n(Source: Several Simple Real-world Applications of Linear Algebra Tools)\n\n\n1.2.1.4 Answer\n\n1.2.1.4.1 Set up Data\nLets define the names of all the stuff we are working with for display purposes\n\nitems←'roll' 'bun' 'cake' 'bread'\nshops←'S1' 'S2'\npeople←'P1' 'P2' 'P3'\n\nDemanded quantity of foodstuff\n\nDemanded←3 4⍴6 5 3 1 3 6 2 2 3 4 3 1\n((⊂'')⍪items)⍪(people,Demanded)\n\n┌→─────────────────────────────────┐\n↓ ┌⊖┐  ┌→───┐ ┌→──┐ ┌→───┐ ┌→────┐ │\n│ │ │  │roll│ │bun│ │cake│ │bread│ │\n│ └─┘  └────┘ └───┘ └────┘ └─────┘ │\n│ ┌→─┐                             │\n│ │P1│ 6      5     3      1       │\n│ └──┘                             │\n│ ┌→─┐                             │\n│ │P2│ 3      6     2      2       │\n│ └──┘                             │\n│ ┌→─┐                             │\n│ │P3│ 3      4     3      1       │\n│ └──┘                             │\n└∊─────────────────────────────────┘\n\n\n\n\nPrices←4 2⍴1.5 1 2 2.5 5 4.5 16 17\n((⊂'')⍪shops)⍪(items,Prices)\n\n┌→──────────────────┐\n↓ ┌⊖┐     ┌→─┐ ┌→─┐ │\n│ │ │     │S1│ │S2│ │\n│ └─┘     └──┘ └──┘ │\n│ ┌→───┐            │\n│ │roll│  1.5  1    │\n│ └────┘            │\n│ ┌→──┐             │\n│ │bun│   2    2.5  │\n│ └───┘             │\n│ ┌→───┐            │\n│ │cake│  5    4.5  │\n│ └────┘            │\n│ ┌→────┐           │\n│ │bread│ 16   17   │\n│ └─────┘           │\n└∊──────────────────┘\n\n\n\n\n\n1.2.1.4.2 Answer Calculation\nWe can calculate the price for each shop for each person with matrix multiplication\n\nAns←Demanded+.×Prices\n((⊂'')⍪shops)⍪(people,Ans) ⍝ display\n\n┌→───────────────┐\n↓ ┌⊖┐  ┌→─┐ ┌→─┐ │\n│ │ │  │S1│ │S2│ │\n│ └─┘  └──┘ └──┘ │\n│ ┌→─┐           │\n│ │P1│ 50   49   │\n│ └──┘           │\n│ ┌→─┐           │\n│ │P2│ 58.5 61   │\n│ └──┘           │\n│ ┌→─┐           │\n│ │P3│ 43.5 43.5 │\n│ └──┘           │\n└∊───────────────┘\n\n\n\nThen find the lowest price each person can pay so these folks can budget!\n\nlowest_per_person ← ⌊/Ans\npeople,3 1⍴lowest_per_person ⍝ display\n\n┌→──────────┐\n↓ ┌→─┐      │\n│ │P1│ 49   │\n│ └──┘      │\n│ ┌→─┐      │\n│ │P2│ 58.5 │\n│ └──┘      │\n│ ┌→─┐      │\n│ │P3│ 43.5 │\n│ └──┘      │\n└∊──────────┘\n\n\n\nWe can also identify which store each person should go to based on the min price.\n\n3 2 ⍴ people ⍪ shops ⌷⍨ ⊂ {⍵⍳⌊/⍵}¨↓Ans\n\n┌→──────────┐\n↓ ┌→─┐ ┌→─┐ │\n│ │P1│ │P2│ │\n│ └──┘ └──┘ │\n│ ┌→─┐ ┌→─┐ │\n│ │P3│ │S2│ │\n│ └──┘ └──┘ │\n│ ┌→─┐ ┌→─┐ │\n│ │S1│ │S1│ │\n│ └──┘ └──┘ │\n└∊──────────┘\n\n\n\n\n\n\n1.2.1.5 Image Data\nImages can be represented by matrices.\n (Source: Adam Geitgey)\n\n\n1.2.1.6 Convolution\nConvolutions are the heart of convolutional neural networks (CNNs), a type of deep learning, responsible for the huge advances in image recognitionin the last few years. They are now increasingly being used for speech as well, such as Facebook AI’s results for speech translation which are 9x faster than RNNs (the current most popular approach for speech translation).\nComputers are now more accurate than people at classifying images.\n (Source: Andrej Karpathy)\n (Source: Nvidia)\nYou can think of a convolution as a special kind of matrix product\nThe 3 images below are all from an excellent blog post written by a fast.ai student on CNNs from Different Viewpoints:\nA convolution applies a filter to each section of an image: \nNeural Network Viewpoint: \nMatrix Multiplication Viewpoint: \nLet’s see how convolutions can be used for edge detection in this notebook(originally from the fast.ai Deep Learning Course)\n\n\n\n1.2.2 Matrix Decompositions\nWe will be talking about Matrix Decompositions every day of this course, and will cover the below examples in future lessons:\n\nTopic Modeling (NMF and SVD. SVD uses QR) A group of documents can be represented by a term-document matrix  (source: Introduction to Information Retrieval)  (source: NMF Tutorial)\nBackground removal (robust PCA, which uses truncated SVD) \nGoogle’s PageRank Algorithm (eigen decomposition)\n\n (source: What is in PageRank?)\n\nList of other decompositions and some applications matrix factorization jungle"
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#accuracy",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#accuracy",
    "title": "Numerical Linear Algebra Part 1",
    "section": "1.3 Accuracy",
    "text": "1.3 Accuracy\n\n1.3.1 Floating Point Arithmetic\nTo understand accuracy, we first need to look at how computers (which are finite and discrete) store numbers (which are infinite and continuous)\n\n1.3.1.1 Exercise\nTake a moment to look at the function \\(f\\) below. Before you try running it, write on paper what the output would be of \\(x_1 = f(\\frac{1}{10})\\). Now, (still on paper) plug that back into \\(f\\) and calculate \\(x_2 = f(x_1)\\). Keep going for 10 iterations.\nThis example is taken from page 107 of Numerical Methods, by Greenbaum and Chartier.\ndef f(x):\n    if x <= 1/2:\n        return 2 * x\n    if x > 1/2:\n        return 2*x - 1\n\n⍝ Translate function to APL\nf←{(2×⍵)-⍵>.5}\n\n\n⍝ Create generator\ngen←{(f⍣⍵)⍺}\n\n\n⍝ First 10 generations\n.1 gen¨ ⍳10\n\n┌→──────────────────────────────────────┐\n│0.2 0.4 0.8 0.6 0.2 0.4 0.8 0.6 0.2 0.4│\n└~──────────────────────────────────────┘\n\n\n\n\n⍝ First 80 generations (left to right top to bottom)\n16 5⍴.1 gen¨ ⍳80\n\n┌→───────────────────────────────────────────────────────────────┐\n↓0.2          0.4          0.8          0.6          0.2         │\n│0.4          0.8          0.6          0.2          0.4         │\n│0.8          0.6          0.2          0.4          0.8         │\n│0.6          0.2          0.4          0.8          0.6         │\n│0.2          0.4          0.8          0.6000000001 0.2000000002│\n│0.4000000004 0.8000000007 0.6000000015 0.200000003  0.400000006 │\n│0.8000000119 0.6000000238 0.2000000477 0.4000000954 0.8000001907│\n│0.6000003815 0.2000007629 0.4000015259 0.8000030518 0.6000061035│\n│0.200012207  0.4000244141 0.8000488281 0.6000976563 0.2001953125│\n│0.400390625  0.80078125   0.6015625    0.203125     0.40625     │\n│0.8125       0.625        0.25         0.5          1           │\n│1            1            1            1            1           │\n│1            1            1            1            1           │\n│1            1            1            1            1           │\n│1            1            1            1            1           │\n│1            1            1            1            1           │\n└~───────────────────────────────────────────────────────────────┘\n\n\n\n\n⍝ Answer after 80 generations\n(f⍣80).1\n\n \n1\n \n\n\n\nWhat went wrong?\n\n\n1.3.1.2 Problem: math is continuous & infinite, but computers are discrete & finite\nTwo Limitations of computer representations of numbers: 1. they can’t be arbitrarily large or small 2. there must be gaps between them\nThe reason we need to care about accuracy, is because computers can’t store infinitely accurate numbers. It’s possible to create calculations that give very wrong answers (particularly when repeating an operation many times, since each operation could multiply the error).\nHow computers store numbers:\n\nThe mantissa can also be referred to as the significand.\nIEEE Double precision arithmetic: - Numbers can be as large as \\(1.79 \\times 10^{308}\\) and as small as \\(2.23 \\times 10^{-308}\\). - The interval \\([1,2]\\) is represented by discrete subset: \\[1, \\: 1+2^{-52}, \\: 1+2 \\times 2^{-52},\\: 1+3 \\times 2^{-52},\\: \\ldots, 2\\]\n\nThe interval \\([2,4]\\) is represented: \\[2, \\: 2+2^{-51}, \\: 2+2 \\times 2^{-51},\\: 2+3 \\times 2^{-51},\\: \\ldots, 4\\]\n\nFloats and doubles are not equidistant:\n Source: What you never wanted to know about floating point but will be forced to find out\nMachine Epsilon\nHalf the distance between 1 and the next larger number. This can vary by computer. IEEE standards for double precision specify \\[ \\varepsilon_{machine} = 2^{-53} \\approx 1.11 \\times 10^{-16}\\]\nTwo important properties of Floating Point Arithmetic:\n\nThe difference between a real number \\(x\\) and its closest floating point approximation \\(fl(x)\\) is always smaller than \\(\\varepsilon_{machine}\\) in relative terms. For some \\(\\varepsilon\\), where \\(\\lvert \\varepsilon \\rvert \\leq \\varepsilon_{machine}\\), \\[fl(x)=x \\cdot (1 + \\varepsilon)\\]\nWhere * is any operation (\\(+, -, \\times, \\div\\)), and \\(\\circledast\\) is its floating point analogue, \\[ x \\circledast y = (x * y)(1 + \\varepsilon)\\] for some \\(\\varepsilon\\), where \\(\\lvert \\varepsilon \\rvert \\leq \\varepsilon_{machine}\\) That is, every operation of floating point arithmetic is exact up to a relative error of size at most \\(\\varepsilon_{machine}\\)\n\n\n\n1.3.1.3 History\nFloating point arithmetic may seem like a clear choice in hindsight, but there have been many, many ways of storing numbers: - fixed-point arithmetic - logarithmic and semilogarithmic number systems - continued-fractions - rational numbers - possibly infinite strings of rational numbers - level-index number systems - fixed-slash and floating-slash number systems - 2-adic numbers\nFor references, see Chapter 1 (which is free) of the Handbook of Floating-Point Arithmetic. Yes, there is an entire 16 chapter book on floating point!\nTimeline History of Floating Point Arithmetic: - ~1600 BC: Babylonian radix-60 system was earliest floating-point system (Donald Knuth). Represented the significand of a radix-60 floating-point representation (if ratio of two numbers is a power of 60, represented the same) - 1630 Slide rule. Manipulate only significands (radix-10) - 1914 Leonardo Torres y Quevedo described an electromechanical implementation of Babbage’s Analytical Engine with Floating Point Arithmetic. - 1941 First real, modern implementation. Konrad Zuse’s Z3 computer. Used radix-2, with 14 bit significand, 7 bit exponents, and 1 sign bit. - 1985 IEEE 754-1985 Standard for Binary Floating-Point Arithmetic released. Has increased accuracy, reliability, and portability. William Kahan played leading role.\n“Many different ways of approximating real numbers on computers have been introduced.. And yet, floating-point arithmetic is by far the most widely used way of representing real numbers in modern computers. Simulating an infinite, continuous set (the real numbers) with a finite set (the “machine numbers”) is not a straightforward task: clever compromises must be found between, speed, accuracy, dynamic range, ease of use and implementation, and memory. It appears that floating-point arithmetic, with adequately chosen parameters (radix, precision, extremal exponents, etc.), is a very good compromise for most numerical applications.”\nAlthough a radix value of 2 (binary) seems like the pretty clear winner now for computers, a variety of other radix values have been used at various point:\n\nradix-8 used by early machines PDP-10, Burroughs 570 and 6700\nradix-16 IBM 360\nradix-10 financial calculations, pocket calculators, Maple\nradix-3 Russian SETUN computer (1958). Benefits: minimizes beta x p (symbols x digits), for a fixed largest representable number beta^p - 1. Rounding = truncation\nradix-2 most common. Reasons: easy to implement. Studies have shown (with implicit leading bit) this gives better worst-case or average accuracy than all other radices.\n\n\n\n\n1.3.2 Conditioning and Stability\nSince we can not represent numbers exactly on a computer (due to the finiteness of our storage, and the gaps between numbers in floating point architecture), it becomes important to know how small perturbations in the input to a problem impact the output.\n“A stable algorithm gives nearly the right answer to nearly the right question.” –Trefethen\nConditioning: perturbation behavior of a mathematical problem (e.g. least squares)\nStability: perturbation behavior of an algorithm used to solve that problem on a computer (e.g. least squares algorithms, householder, back substitution, gaussian elimination)\nExample: Eigenvalues of a Matrix\n\n1.3.2.0.1 Create matrices\n\n⎕←A←2 2⍴1 1000 0 1\n⎕←B←2 2⍴1 1000 .001 1\n\n┌→─────┐\n↓1 1000│\n│0    1│\n└~─────┘\n\n\n\n┌→─────────┐\n↓1     1000│\n│0.001    1│\n└~─────────┘\n\n\n\n\n\n1.3.2.0.2 Calculate Eigenvalues (quadratic)\n\nI ← {⍵ ⍵ ⍴ 1, ⍵⍴0} ⍝ calculate identity\ntf← {+/+/(⍵×I 2)} ⍝ Calculate Trace\ndf ← {(×/×/(⍵×I 2)+0=I 2)-×/×/(I 2)+⍵×0=I 2} ⍝ Calculate Determinant\n\n⍝ Eigenvalue calculation\ne1←{2÷⍨⍵-.5*⍨(⍵*2)-4×⍺} ⍝ Eigenvalue 1\ne2←{2÷⍨⍵+.5*⍨(⍵*2)-4×⍺} ⍝ Eigenvalue 2\n\n⍝ Get both eigenvalues\neigenvalues←{(d e2 t),(d←df ⍵)e1(t←tf ⍵)} ⍝ Get both eigenvalues\n\n\neigenvalues A\neigenvalues B\n\n┌→──┐\n│1 1│\n└~──┘\n\n\n\n┌→──┐\n│2 0│\n└~──┘\n\n\n\nReminder: Two properties of Floating Point Arithmetic\n\nThe difference between a real number \\(x\\) and its closest floating point approximation \\(fl(x)\\) is always smaller than \\(\\varepsilon_{machine}\\) in relative terms.\nEvery operation \\(+, -, \\times, \\div\\) of floating point arithmetic is exact up to a relative error of size at most \\(\\varepsilon_{machine}\\)\n\nExamples we’ll see: - Classical vs Modified Gram-Schmidt accuracy - Gram-Schmidt vs. Householder (2 different ways of computing QR factorization), how orthogonal the answer is - Condition of a system of equations\n\n\n\n1.3.3 Approximation accuracy\nIt’s rare that we need to do highly accurate matrix computations at scale. In fact, often we’re doing some kind of machine learning, and less accurate approaches can prevent overfitting.\nIf we accept some decrease in accuracy, then we can often increase speed by orders of magnitude (and/or decrease memory use) by using approximate algorithms. These algorithms typically give a correct answer with some probability. By rerunning the algorithm multiple times you can generally increase that probability multiplicatively!\nExample: A bloom filter allows searching for set membership with 1% false positives, using <10 bits per element. This often represents reductions in memory use of thousands of times.\n\nThe false positives can be easily handled by having a second (exact) stage check all returned items - for rare items this can be very effective. For instance, many web browsers use a bloom filter to create a set of blocked pages (e.g. pages with viruses), since blocked web pages are only a small fraction of the whole web. A false positive can be handled here by taking anything returned by the bloom filter and checking against a web service with the full exact list. (See this bloom filter tutorial for more details).\n\n\n1.3.4 Expensive Errors\nThe below examples are from Greenbaum & Chartier.\nEuropean Space Agency spent 10 years and $7 billion on the Ariane 5 Rocket.\nWhat can happen when you try to fit a 64 bit number into a 16 bit space (integer overflow):\n\n⍝from IPython.display import YouTubeVideo\n⍝YouTubeVideo(\"PK_yguLapgA\")\n\nHere is a floating point error that cost Intel $475 million:\n1994 NYTimes article about Intel Pentium Error \nResources: See Lecture 13 of Trefethen & Bau and Chapter 5 of Greenbaum & Chartier for more on Floating Point Arithmetic"
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#memory-use",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#memory-use",
    "title": "Numerical Linear Algebra Part 1",
    "section": "1.4 Memory Use",
    "text": "1.4 Memory Use\n\n1.4.1 Sparse vs Dense\nAbove we covered how numbers are stored, now let’s talk about how matrices are stored. A key way to save memory (and computation) is not to store all of your matrix. Instead, just store the non-zero elements. This is called sparse storage, and it is well suited to sparse matrices, that is, matrices where most elements are zero.\n\nHere is an example of the matrix from a finite element problem, which shows up in engineering (for instance, when modeling the air-flow around a plane). In this example, the non-zero elements are black and the zero elements are white:  Source\nThere are also special types of structured matrix, such as diagonal, tri-diagonal, hessenberg, and triangular, which each display particular patterns of sparsity, which can be leveraged to reduce memory and computation.\nThe opposite of a sparse matrix is a dense matrix, along with dense storage, which simply refers to a matrix containing mostly non-zeros, in which every element is stored explicitly. Since sparse matrices are helpful and common, numerical linear algebra focuses on maintaining sparsity through as many operations in a computation as possible."
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#speed",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#speed",
    "title": "Numerical Linear Algebra Part 1",
    "section": "1.5 Speed",
    "text": "1.5 Speed\nSpeed differences come from a number of areas, particularly: - Computational complexity - Vectorization - Scaling to multiple cores and nodes - Locality\n\n1.5.1 Computational complexity\nIf you are unfamiliar with computational complexity and \\(\\mathcal{O}\\) notation, you can read about it on Interview Cake and practice on Codecademy. Algorithms are generally expressed in terms of computation complexity with respect to the number of rows and number of columns in the matrix. E.g. you may find an algorithm described as \\(\\mathcal{O(n^2m)}\\).\n\n\n1.5.2 Vectorization\nModern CPUs and GPUs can apply an operation to multiple elements at once on a single core. For instance, take the exponent of 4 floats in a vector in a single step. This is called SIMD. You will not be explicitly writing SIMD code (which tends to require assembly language or special C “intrinsics”), but instead will use vectorized operations in libraries like numpy, which in turn rely on specially tuned vectorized low level linear algebra APIs (in particular, BLAS, and LAPACK).\n\n1.5.2.1 Matrix Computation Packages: BLAS and LAPACK\nBLAS (Basic Linear Algebra Subprograms): specification for low-level matrix and vector arithmetic operations. These are the standard building blocks for performing basic vector and matrix operations. BLAS originated as a Fortran library in 1979. Examples of BLAS libraries include: AMD Core Math Library (ACML), ATLAS, Intel Math Kernel Library (MKL), and OpenBLAS.\nLAPACK is written in Fortran, provides routines for solving systems of linear equations, eigenvalue problems, and singular value problems. Matrix factorizations (LU, Cholesky, QR, SVD, Schur). Dense and banded matrices are handled, but not general sparse matrices. Real and complex, single and double precision.\n1970s and 1980s: EISPACK (eigenvalue routines) and LINPACK (linear equations and linear least-squares routines) libraries\nLAPACK original goal: make LINAPCK and EISPACK run efficiently on shared-memory vector and parallel processors and exploit cache on modern cache-based architectures (initially released in 1992). EISPACK and LINPACK ignore multi-layered memory hierarchies and spend too much time moving data around.\nLAPACK uses highly optimized block operations implementations (which much be implemented on each machine) LAPACK written so as much of the computation as possible is performed by BLAS.\n\n\n\n1.5.3 Locality\nUsing slower ways to access data (e.g. over the internet) can be up to a billion times slower than faster ways (e.g. from a register). But there’s much less fast storage than slow storage. So once we have data in fast storage, we want to do any computation required at that time, rather than having to load it multiple times each time we need it. In addition, for most types of storage its much faster to access data items that are stored next to each other, so we should try to always use any data stored nearby that we know we’ll need soon. These two issues are known as locality.\n\n1.5.3.1 Speed of different types of memory\nHere are some numbers everyone should know (from the legendary Jeff Dean): - L1 cache reference 0.5 ns - L2 cache reference 7 ns - Main memory reference/RAM 100 ns - Send 2K bytes over 1 Gbps network 20,000 ns - Read 1 MB sequentially from memory 250,000 ns - Round trip within same datacenter 500,000 ns - Disk seek 10,000,000 ns - Read 1 MB sequentially from network 10,000,000 ns - Read 1 MB sequentially from disk 30,000,000 ns - Send packet CA->Netherlands->CA 150,000,000 ns\nAnd here is an updated, interactive version, which includes a timeline of how these numbers have changed.\nKey take-away: Each successive memory type is (at least) an order of magnitude worse than the one before it. Disk seeks are very slow.\nThis video has a great example of showing several ways you could compute the blur of a photo, with various trade-offs. Don’t worry about the C code that appears, just focus on the red and green moving pictures of matrix computation.\nAlthough the video is about a new language called Halide, it is a good illustration the issues it raises are universal. Watch minutes 1-13:\n\n⍝ from IPython.display import YouTubeVideo\n⍝ YouTubeVideo(\"3uiEyEKji0M\")\n\nLocality is hard. Potential trade-offs: - redundant computation to save memory bandwidth - sacrificing parallelism to get better reuse\n\n\n1.5.3.2 Temporaries\nThe issue of “temporaries” occurs when the result of a calculation is stored in a temporary variable in RAM, and then that variable is loaded to do another calculation on it. This is many orders of magnitude slower than simply keeping the data in cache or registers and doing all necessary computations before storing the final result in RAM. This is particularly an issue for us since numpy generally creates temporaries for every single operation or function it does. E.g. \\(a=b\\cdot c^2+ln(d)\\) will create four temporaries (since there are four operations and functions).\n\n\n\n1.5.4 Scaling to multiple cores and nodes\nWe have a separate section for scalability, but it’s worth noting that this is also important for speed - if we can’t scale across all the computing resources we have, we’ll be stuck with slower computation."
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#scalability-parallelization",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#scalability-parallelization",
    "title": "Numerical Linear Algebra Part 1",
    "section": "1.6 Scalability / parallelization",
    "text": "1.6 Scalability / parallelization\nOften we’ll find that we have more data than we have memory to handle, or time to compute. In such a case we would like to be able to scale our algorithm across multiple cores (within one computer) or nodes (i.e. multiple computers on a network). We will not be tackling multi-node scaling in this course, although we will look at scaling across multiple cores (called parallelization). In general, scalable algorithms are those where the input can be broken up into smaller pieces, each of which are handled by a different core/computer, and then are put back together at the end."
  },
  {
    "objectID": "posts/APL/Basic Stats.html",
    "href": "posts/APL/Basic Stats.html",
    "title": "Bootstrapping",
    "section": "",
    "text": "This post shows how to calculate statistics in the way I believe should be the default for data scientists, bootstrapping. If you are not familiar with this approach and think it sounds intriguing, check out this page to find a great book to get a fantastic start on bootstrapping and practical statistics.\n\n\n\n\n\n\nNote\n\n\n\nThe quality and simplicity of the APL code in this post was improved thanks to the kind feedback provided by rak1507. It’s awesome to have experienced community members like rak1507 that are willing to read through material written by people newer to array programming and offer feedback in a supportive way.\n\n\n\n\nBecause this opinions seems to put me in the minority of data scientists I am writing a short piece on why bootstrapping here.\nIn classical statistics, very clever algebraic formulas are used to approximate a sampling distribution, and that approximation can be used to calculate a p-value or a confidence interval or other statistics. These formulas rely on assumptions about the data and do not work if those baked in assumptions are not true. In other words they are really shortcuts to calculating an answer that work in specific situations.\nIn modern days, we do not need to approximate a sampling distribution using algebra. We can do something much more elementary, more powerful, and more flexible. Thanks to modern computers, we can just sample our data repeatedly to create an actual sampling distribution and calculate based off of that. You get the same answer. So why do I advocate for a bootstrapping first approach?\n\nIt is simpler and more intuitive. This means it is far easier to craft custom tests and statistics based on whatever you want and reason about what things are.\nBootstrapping assumes nothing other than you have data. Classical statistical formulas are shortcuts that are enabled with baked in assumptions about the data. This means the same boostrapping approach works in basically all situations, where classical statistical formulas only apply in the particular situations they were designed for.\n\nFor this reason I believe it should be the default and you can change to computational shortcuts in the situations where it makes sense (ie you are very confident you understand assumptions, confident they are true in your problem, and the amount of data makes it non-trivial to bootstrap).\n\n\n\n\n\n\nNote\n\n\n\nMuch of this next bit is heavily inspired by Overview of Statistics: UnLocking the Power of Data By Lock, Lock, Lock, Lock, and Lock Published by Wiley (2012). I have summarized key points that I think are relevant to what I want to communicate. For example, the quotes I am using are quotes I originally saw in their article.\n\n\nMany of the top statisticians have known bootstrapping is a more elementary but more flexible approach for longer than the approach was computationally feasible. For example, in 1936 Sir R.A. Fisher (who created the foundations of statistical inference) spoke about using this bootstrapping approach:\n\nActually, the statistician does not carry out this very simple and very tedious process, but his conclusions have no justification beyond the fact that they agree with those which could have been arrived at by this elementary method.\n\nWhile these methods were tedious in 1936, they are trivial thanks to modern computers. We no longer have to do clever algebraic tricks to approximate a sampling distribution - we can just create a sampling distribution, as George Cobb pointed out in the journal Technology Innovations in Statistical Education.\n\nWhat we teach is largely the technical machinery of numerical approximations based on the normal distribution and its many subsidiary cogs. This machinery was once necessary, because the conceptually simpler alternative based on permutations was computationally beyond our reach. Before computers statisticians had no choice. These days we have no excuse.\n\n\n\n\n\nBecause it’s cool\nBecause most modeling now-a-days is done via array programming and learning, and APL is a fantastic way to get better at that\nBecause it’s a more consistent math noting"
  },
  {
    "objectID": "posts/APL/Basic Stats.html#create-1-sample",
    "href": "posts/APL/Basic Stats.html#create-1-sample",
    "title": "Bootstrapping",
    "section": "2.1 Create 1 Sample",
    "text": "2.1 Create 1 Sample\nLet’s start with creating a sample\n\nCreate some data for us to sample\nGet 5 random numbers between 1 and 10 (no duplicate numbers)\n\n\n⎕←V ← 5?10\n\n┌→─────────┐\n│4 3 10 1 5│\n└~─────────┘\n\n\n\nNext we need to get a random sample of indexes from our data V. We can do that in 3 steps: 1. Get the total number of elements in our data array with ≢V (tally the Vector) 1. Create an array of the size of the sample we want and fill it with ≢V using 10⍴≢V. Create an array of dimension 10 with containing the tally of the vector. APL will broadcase to make all elements equal to ≢V automatically. 1. ? will roll a die for each element between 1 and the value of the element. This gives us random index locations for each sample we want.\nPut that all together and we have code that:\n\nGet random sample of indexes\nGet 10 random numbers between 1 and ≢V (duplicates allowed)\n\n\n⎕←S←?10⍴≢V\n\n┌→──────────────────┐\n│3 2 2 2 2 5 1 2 4 2│\n└~──────────────────┘\n\n\n\nSince that created random index locations, we can look those indexes up in our original vector V to get our random sample.\n\nV[S]\n\n┌→───────────────────┐\n│10 3 3 3 3 5 4 3 1 3│\n└~───────────────────┘\n\n\n\nIf we put that together we get a nice compact way of drawing a sample.\n\nV[?10 ⍴ ≢V]\n\n┌→───────────────────┐\n│3 10 3 3 1 4 3 4 3 3│\n└~───────────────────┘"
  },
  {
    "objectID": "posts/APL/Basic Stats.html#create-sampling-distribution",
    "href": "posts/APL/Basic Stats.html#create-sampling-distribution",
    "title": "Bootstrapping",
    "section": "2.2 Create sampling distribution",
    "text": "2.2 Create sampling distribution\nWe drew a sample, but really what we want to do is draw a whole bunch of samples. All we have to do is create a matrix of indices instead of a vector and the exact same approach works.\nThis is the same as above, except instead of 10 ⍴ to create an array of shape 10, we use 5 10 ⍴ to create an array of shape 5 by 10.\nFor convenience I store the shapes in a variable for later use.\n\nV[?(n←5) (ss←10) ⍴ ≢V]\n\n┌→──────────────────────────┐\n↓ 4 4  3  4  1  5  4 3  3 10│\n│ 1 3 10  5  3  4 10 1  4 10│\n│ 5 1 10 10  5  3  3 4  1  4│\n│10 5 10  1 10  3  1 4 10  1│\n│10 5  5  5  1 10  3 3  1 10│\n└~──────────────────────────┘"
  },
  {
    "objectID": "posts/APL/Basic Stats.html#confidence-intervals",
    "href": "posts/APL/Basic Stats.html#confidence-intervals",
    "title": "Bootstrapping",
    "section": "3.1 Confidence Intervals",
    "text": "3.1 Confidence Intervals\nLets do a bigger sample and calculate our confidence interval using 10000 random numbers between 1 and 100.\n\ndata ← ?10000/100 \n\n10↑data ⍝ look at first 10 values\n\n┌→───────────────────────────┐\n│50 3 15 43 93 60 96 29 71 58│\n└~───────────────────────────┘\n\n\n\nNext we can calculate a sampling distribution and look a a few of them. We use the code from before but with 1000 pulls.\n\nsampling_distribution←data[? (n←1000) (ss←10) ⍴ ≢ data]\n5↑sampling_distribution\n\n┌→─────────────────────────────┐\n↓20 56 92 100 34 89 28 92 10 21│\n│34 95 89  69 35 81 25 25 80 87│\n│77 68 32  77 57 20 10 20 21 95│\n│37 73 19  79 11 88 13  1 90 68│\n│70 42 10  74 62 34 82 17  3 19│\n└~─────────────────────────────┘\n\n\n\nWe want to do a confidence interval on the mean so we need to calculate the mean of each of these samples.\n\n+/ Row-wise sum (trailing axis)\nss÷⍨ divides each element by ss (ss defined when creating sampling distribution)\n\n\nsample_means ← ss÷⍨+/ sampling_distribution\n8↑sample_means\n\n┌→────────────────────────────────────┐\n│54.2 62 47.7 47.9 41.3 63.5 50.7 44.3│\n└~────────────────────────────────────┘\n\n\n\nNow we calculate at 90% confidence interval on our sample mean. That means we are 90% confident our mean will land in the given interval range. This is easy to do because we have calculated the mean of a good sampling distribution so we just need to cut off the top and bottom 5% of values and 90% of the values landed in that range.\n\n⍋ sorts ascending, then cut off first 50 and take the first 900 of that\n\n\norder90 ← 900↑50↓⍋sample_means\n\nGet min and max of middle 90% of sample means, which is our 90% confidence interval. Because our data is sorted we can just get the first and last value.\n\nsample_means[⊣/order90]\nsample_means[⊢/order90]\n\n    \n36.2\n    \n\n\n\n    \n65.5\n    \n\n\n\nWe know we are 90 percent confident that a mean based on a sample size of 10 will land in that range because we did that and found that to be true."
  },
  {
    "objectID": "posts/APL/Basic Stats.html#p-values",
    "href": "posts/APL/Basic Stats.html#p-values",
    "title": "Bootstrapping",
    "section": "3.2 P values",
    "text": "3.2 P values\nLet’s say we have 2 sets of data and we want to know whether some statistics are different between them. We have 10,000 samples of our original data, and we ran an experiment and got 100 datapoints with our new process. We want to calculate a p value to see if it supports our hypothesis that it had a statistically significant impact.\n\n\n\n\n\n\nNote\n\n\n\nStatistically significant impact does not necessarily mean practically significant. This test is doing the basic (are these 2 means different), but often that isn’t really that helpful of a question. Often we want to ask “are these 2 means different by at least X”. After reviewing the simple examples think through how you might be able to design that test via bootstrapping!\n\n\n\n3.2.1 P value on equal means\n\nbaseline ← 1-⍨2×⍨?10000/0\nexperiment ← 0.5-⍨?100/0\n\nThese should have roughly the same means so we should get a large p value and show the difference is not statistically significant\nLet’s run the test and see what we get. First let’s get our statistic from our experiment (mean).\n\n⎕←experiment_mean ← (+/experiment) ÷ ≢experiment\n\n             \n0.02131968282\n             \n\n\n\nNow let’s create our sampling distribution on our baseline.\n\nsampling_distribution←baseline[? (n←1000) (ss←10) ⍴ ≢ baseline]\n\nCalculate the means of each.\n\nsampling_means ← ss ÷⍨ +/sampling_distribution\n\nWe then calculate a p value by seeing what percentage of sample means our experiment mean is more extreme than. We can check this on both ends of the distribution and we would take the smaller one normally.\n\nn ÷⍨ +/ experiment_mean>sampling_means\nn ÷⍨ +/ experiment_mean<sampling_means\n\n     \n0.545\n     \n\n\n\n     \n0.455\n     \n\n\n\n\n\n3.2.2 P value on different means\n\nbaseline ← ?10000/0\nexperiment ← 0.2-⍨?100/0\n\nThese should have different means so we should get a large p value and show the different is not practically significant\nLet’s run the test and see what we get. First let’s get our statistic from our experiment (mean).\n\n⎕←experiment_mean ← (+/experiment) ÷ ≢experiment\n\n           \n0.302065664\n           \n\n\n\nNow let’s create our sampling distribution on our baseline.\n\nsampling_distribution← baseline[? (n←1000) (ss←10) ⍴ ≢ baseline]\n\n\nsampling_means ← ss ÷⍨ +/sampling_distribution\n\nWe then calculate a p value by seeing what percentage of sample means our experiment mean is more extreme than. We can check this on both ends of the distribution, but we would take the smaller one. We can see our p value is quite small - it successfully detected that we likely have a different mean.\n\nn ÷⍨ +/ sampling_means > experiment_mean\nn ÷⍨ +/ sampling_means < experiment_mean\n\n     \n0.993\n     \n\n\n\n     \n0.007"
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#belief",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#belief",
    "title": "Simple Time Series (Stock Prices)",
    "section": "4.1 Belief",
    "text": "4.1 Belief\nI believe how a company is doing over the last month can be used to predict how how well it will do in the future. This isn’t much of a leap, but let’s think about a few reasons as to why this may be true.\n\nAvailable Capital: If a company is doing well, it typically means they have more profit. More profit means more that can be reinvested. More reinvestment can mean faster growth.\nEconomies of Scale: Often the more successful a company is the more they can drive down cost in some areas. For example, General Mills can buy sugar at a lower price than a small business due to their buying power. As buying power increase, they can leverage that to drive down costs.\nBrand Recognition: The more successful a business is and the larger it grows, the more brand recognition it has. The more brand recognition it has the more it can leverage it’s brand to grow."
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#hypothesis",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#hypothesis",
    "title": "Simple Time Series (Stock Prices)",
    "section": "4.2 Hypothesis",
    "text": "4.2 Hypothesis\nThe hypothesis for this post is that recent stock performance can be used on its own to predict future stock performance. Regardless of how much we believe that to be true, we should not trade based on this belief until we have evidence. This post will explore several options for using this hypothesis to make trades.\nThe next post will give a foundation in testing and show how we can test and evaluate how well these approaches perform and determine if these are ideas worth keeping."
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#load-data",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#load-data",
    "title": "Simple Time Series (Stock Prices)",
    "section": "5.1 Load data",
    "text": "5.1 Load data\nFirst let’s take a look at the data we will be using and talk a bit about it.\n\nraw = pd.read_csv(path/'eod-quotemedia.csv',parse_dates=['date'])\nraw.head(3)\n\n\n\n\n\n  \n    \n      \n      date\n      ticker\n      adj_close\n    \n  \n  \n    \n      0\n      2013-07-01\n      A\n      29.994186\n    \n    \n      1\n      2013-07-02\n      A\n      29.650137\n    \n    \n      2\n      2013-07-03\n      A\n      29.705185\n    \n  \n\n\n\n\n\nL(*raw.ticker.unique())\n\n(#495) ['A','AAL','AAP','AAPL','ABBV','ABC','ABT','ACN','ADBE','ADI'...]\n\n\nWe can see that for each day we have a ticker.\n\n\n\n\n\n\nTip\n\n\n\nA ticker is a symbol associated with a company. For example Apple has the ticker AAPL. To buy shares in Apple you would buy AAPL.\n\n\nFor each of these day|ticker combinations we have an adj_close, or adjusted close price. After every transaction, the price of a stock changes slightly. The adjusted close price is the last stock price of the day. While this is not as detailed as having the price at a more granular level (second, minute, hour, etc.), called tick data, we can use daily close price to test many types of strategies.\n::{note} This is the stock price for the ticker. adjusted means that the prices have been adjusted to account for various actions, such as stock splits (more discussion on this later). close means that it is the price at close of market.\nA good first step after getting tabular data is to use pandas’ describe method. As we do this we see a few good pieces of information to keep in mind: + Overall size of dataset - 409K rows + Very big range in values (~1 - ~1K), which most of them before $100\n\nraw.describe().transpose()\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n  \n  \n    \n      adj_close\n      490737.0\n      75.100472\n      75.438804\n      1.59\n      36.782424\n      57.499593\n      87.4\n      1011.34"
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#null-values",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#null-values",
    "title": "Simple Time Series (Stock Prices)",
    "section": "5.2 Null Values",
    "text": "5.2 Null Values\nLet’s take a look and make sure we don’t have any null values to handle. This is one of those things you need to do with every dataset. This is also a great opportunity to show how you can add in simple tests into your code as you go using an assert statement, which will help you catch issues as you iterate and change things.\n\nassert np.array([o==0 for o in raw.isnull().sum()]).all() == True"
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#survivorship-bias",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#survivorship-bias",
    "title": "Simple Time Series (Stock Prices)",
    "section": "5.3 Survivorship bias",
    "text": "5.3 Survivorship bias\nWe also want to take a quick look at the non-numeric columns to get an idea of what time frame we have and how many tickers. This is often known from the dataset, but it is good practice to look at the dataset and ensure that your understanding of the dataset aligns with what you see in the data.\n\nprint(f\"Date column contains dates from {raw.date.min().date()} to {raw.date.max().date()}\")\n\nDate column contains dates from 2013-07-01 to 2017-06-30\n\n\nLet’s see if all tickers in the dataset have the same start and end date.\n\nticker_cnt = len(raw.ticker.unique())\n\n_min = raw[['ticker','date']].groupby('ticker').min()\n_min = _min[_min.date != '2013-07-01'].count().date\n\n_max = raw[['ticker','date']].groupby('ticker').max()\n_max = _max[_max.date != '2017-06-30'].count().date\n\nprint(f'''Out of {ticker_cnt} tickers:\n  + 20 do do not start on 2013-07-01\n  + 0 do not have an entry for 2017-06-30''')\n\nOut of 495 tickers:\n  + 20 do do not start on 2013-07-01\n  + 0 do not have an entry for 2017-06-30\n\n\nGood thing we checked! Let’s think through what these two data points mean:\n\n20 do do not start on 2013-07-01: This makes sense because some of the companies may not have been founded or fit our criteria until after the start date of the dataset. Maybe they were private companies that went public, or maybe they grew to a large enough size to be part of our universe.\n0 do not have an entry for 2017-06-30: While it’s not definitive proof of an issue, it is cause for concern. This dataset may have a survivorship bias built in. Let’s talk about what survivorship bias is and why this could be a problem.\n\nIn our dataset we see that every ticker has a close price on the last day of the dataset. This means that all of the companies are active at the end of our dataset. What this means is that either:\n\nNo company went out of business or failed in our universe between our July 2013 and June 2017 dates\nSome companies did fail during our universe time period and our dataset does not reflect that.\n\nWhile either are possible, the second option is a common problem. Let’s talk about why failed companies not being a part of our universe is a problem.\nLet’s say I look at the S&P 500 companies in 2022 and build a dataset of their stock prices for 2015 - 2022. We want to tests how well a strategy would have performed in that time range. Any trade has many outcomes for example:\n\nYou could buy a stock and then the company goes out of business and you lose lots of money\nYou could buy a stock and then the price goes up and you profit\nYou could buy a stock and then the price goes down and you lose money\n\nThe problem is that option #1 is impossible in this dataset. We know none of the business went out of business between 2015 and 2022 because they were all in the S&P 500 in 2022. Option #1 is more likely than it should be because we already know the companies on the list are among the largest companies in 2022, so regardless of what company we pick we know it ends up a large company. Option #3 is less likely than it should be because if a company shrunk in size to the point it’s not in the S&P 500 in 2022 it’s not even in the dataset. Our strategies may perform extremely well on our dataset, but when we run it on real data with real money we could be in for a shock!\nBottom line is we need point in time data, or rather data for a given date should only be as accurate as you could have known on that date.\nWhen we see all tickers in the universe have a stock price on the last day, it’s important to verify that this did not happen in your dataset. When we talk about testing later, we will talk about how we can test to ensure we have accurate results."
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#reformat",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#reformat",
    "title": "Simple Time Series (Stock Prices)",
    "section": "5.4 Reformat",
    "text": "5.4 Reformat\nNow that we have an basic idea of what’s in our data we can reformat it to a format that will be easier to use for analysis. For what we are doing we will be applying things based on ticker, so let’s give each ticker it’s own column.\n\ndf = raw.pivot(index='date', columns='ticker',values='adj_close')\ndf.iloc[:,:5].head(3)\n\n\n\n\n\n  \n    \n      ticker\n      A\n      AAL\n      AAP\n      AAPL\n      ABBV\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2013-07-01\n      29.994186\n      16.176093\n      81.138217\n      53.109173\n      34.924478\n    \n    \n      2013-07-02\n      29.650137\n      15.819834\n      80.722073\n      54.312247\n      35.428076\n    \n    \n      2013-07-03\n      29.705185\n      16.127950\n      81.237299\n      54.612043\n      35.444862\n    \n  \n\n\n\n\nWe can use the same describe as above to see statistics about each ticker.\n\ndf.iloc[:,:5].describe()\n\n\n\n\n\n  \n    \n      ticker\n      A\n      AAL\n      AAP\n      AAPL\n      ABBV\n    \n  \n  \n    \n      count\n      1009.000000\n      1009.000000\n      1009.000000\n      1009.000000\n      1009.000000\n    \n    \n      mean\n      40.983757\n      37.811501\n      141.576280\n      100.360320\n      52.977953\n    \n    \n      std\n      5.850163\n      8.816410\n      26.260390\n      22.660593\n      7.897264\n    \n    \n      min\n      29.650137\n      14.770314\n      78.393647\n      53.109173\n      34.924478\n    \n    \n      25%\n      37.656517\n      34.383874\n      125.561609\n      87.186576\n      46.981317\n    \n    \n      50%\n      39.700762\n      39.218491\n      147.450711\n      102.884811\n      53.983957\n    \n    \n      75%\n      43.944859\n      43.681272\n      159.019446\n      114.257784\n      59.212432\n    \n    \n      max\n      60.131015\n      54.071539\n      199.374304\n      153.694280\n      70.780784"
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#validation-set",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#validation-set",
    "title": "Simple Time Series (Stock Prices)",
    "section": "5.5 Validation Set",
    "text": "5.5 Validation Set\nWe are going to split our data into 2 groups. This way we have a secondary unseen dataset to test my strategies. I want to develop models on one set of data but evaluate them on different data. This may not seem important but it is absolutely crucial and perhaps the most important concept. The more complex your model and analysis the more important this becomes. This will be discussed in much greater detail in [the next post]((../Statistics/BasicTesting.ipynb), but for now just take my word for it.\nWith time series you generally want your validation set to be the most recent data. This reflects reality best; creating a model on data with the intent to use it on future data.\n\ntrain = df.loc[:pd.Timestamp('2017-1-1 01:00:00')]\nvalid = df.loc[pd.Timestamp('2017-1-1 01:00:00'):]\n\n\nprint(f\"Train Dates: {train.index.min().date()} thru {train.index.max().date()}\")\nprint(f\"Valid Dates: {valid.index.min().date()} thru {valid.index.max().date()}\")\n\nTrain Dates: 2013-07-01 thru 2016-12-30\nValid Dates: 2017-01-03 thru 2017-06-30"
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#basic-momentum",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#basic-momentum",
    "title": "Simple Time Series (Stock Prices)",
    "section": "6.1 Basic Momentum",
    "text": "6.1 Basic Momentum\nAs our first model let’s use percent change over recent stock price history. We will take the percent difference between the current stock price and the stock price from a set time in the past. As we think about this approach there are several levers we can pull time find tune our approach:\n\nTime Range: We could use the past 5 days, or past 30 days, or the past year. How far back should we be comparing?\nThreshold: What threshold do we need to cross before we consider it momentous enough to take an action? Is a 1%, 5%, 10%?\nWhat action? Is it just buy and sell? Could we use this to short?\nWhen do we close our position Is it a set time period? Or based on another threshold?\n\nWe will use 28 days for the time range and 8% for our threshold in this example to demonstrate the concept, but in the testing section we will show how to test different parameters.\n\nfrom SimpleTimeSeries import get_momentum_actions\nview_source_code(get_momentum_actions)\n\n\n\n\n\n  \n  \n  \n\n\n\n\ndef get_momentum_actions(df, n_periods,threshold):\n    _x = df.shift(n_periods)\n    \n    # Calculate percent change\n    momentum_rate = df.apply(lambda x: (x-x.shift(n_periods))/x.shift(n_periods))[n_periods:]\n\n    # Select Action Based on Threshold\n    actions = pd.DataFrame(np.where(momentum_rate < -threshold, 'Short',\n                           np.where(momentum_rate > threshold,  'Buy',\n                                                                 '')),\n                   columns=momentum_rate.columns,index=momentum_rate.index)\n    \n    # Because we use close price, we can't make the trade action until the following day\n    actions.index = actions.index + timedelta(1)\n    \n    return actions\n\n\n\n\n\n\nactions = get_momentum_actions(train.iloc[:,:5],n_periods=28,threshold=0.08)\nactions.head(10)\n\n\n\n\n\n  \n    \n      ticker\n      A\n      AAL\n      AAP\n      AAPL\n      ABBV\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2013-08-10\n      \n      Buy\n      \n      Buy\n      Buy\n    \n    \n      2013-08-13\n      \n      Buy\n      \n      Buy\n      \n    \n    \n      2013-08-14\n      Buy\n      \n      \n      Buy\n      \n    \n    \n      2013-08-15\n      \n      \n      \n      Buy\n      \n    \n    \n      2013-08-16\n      \n      \n      \n      Buy\n      \n    \n    \n      2013-08-17\n      \n      \n      \n      Buy\n      \n    \n    \n      2013-08-20\n      \n      Short\n      \n      Buy\n      \n    \n    \n      2013-08-21\n      \n      Short\n      \n      Buy\n      \n    \n    \n      2013-08-22\n      \n      Short\n      \n      Buy\n      \n    \n    \n      2013-08-23\n      \n      Short\n      \n      Buy\n      \n    \n  \n\n\n\n\nThis leaves us with a table of what actions we are going to execute each day for each stock. Let’s look at some other options for using this momentum-esque concept, and then we can test them all and compare how they perform at the end."
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#regression-momentum",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#regression-momentum",
    "title": "Simple Time Series (Stock Prices)",
    "section": "6.2 Regression Momentum",
    "text": "6.2 Regression Momentum\nOur previous approach was just the percent change between 2 dates. But what if one of those days is an outlier? Should we really make a decision based on just 2 data points? To address this concerns we will define momentum slightly differently with the slope of a fit regression.\nFirst, let’s understand the general concept better. Creating these minimal examples and visuals is not just something educational for a post - you should do this in your own projects as well. It will help you think more deeply about your problem.\nBelow I took the stock price for Apple in a 10 day period and plotted it as a scatter plot. Every 4 data points I fit a regression trend line fit to them. We can see that in some groups the trend a very aggressive upward slope, others it’s more neutral, and in others it is a strong negative slope. By using that slope we can determine how much momentum the group of points has. In this way we use all the recent data points to influence momentum and not just the first and last in a period.\n\n# Create Figure\nfig,ax = plt.subplots(figsize=(15,8))\nfig.suptitle(\"Window Regression Lines\",fontsize=50)\n\n# Get 10 data points\n_ = train['AAPL'].iloc[:10] \n\nx1 = _.index.day.values\ny1 = _.values\n\nsz = 4 # Window Size\n\n# Get Windows for x and y\nregr_y = [y1[i:i+sz] for i in range(0,len(y1)-(sz-1))]\nregr_x = [x1[i:i+sz].reshape(-1, 1) for i in range(0,len(x1)-(sz-1))]\n\n# Create Regression lines\nregr = [LinearRegression().fit(x,y).predict(x) for x,y in zip(regr_x,regr_y)]\n\n# Pad Regression Lines for Plotting\nregr_padded = [[None]*i+list(r)+[None]*(len(x1)-sz-i) for i,r in enumerate(regr)]\n\n# Plot\nax.scatter(_.index,y1)\nfor i in range(len(regr)): ax.plot(_.index,regr_padded[i])\n\nplt.show()\n\n\n\n\nSo we take the slope of the trend line to be our momentum. There are very similar levers as before we can change to fine tune out approach:\n\nTime Range: We could use the past 5 days, or past 30 days, or the past year. How far back should we be comparing?\nThreshold: What threshold do we need to cross before we consider it momentous enough to take an action? Is a slope of 1? Slope of 10? Should we do something other than a set value?\nWhat action? Is it just buy and sell? Could we use this to short?\nWhen do we close our position Is it a set time period? Or based on another threshold?\n\nWe will use 28 days for the time range and $5 for our threshold in this example to demonstrate the concept, but in the testing section we will show how to test different parameters.\n\n\n\n\n\n\nNote\n\n\n\nWe are assuming that every day is equally spaced in this regression. I reality, the stock market is not open daily. We are ignoring this for this analysis to keep things simple, but it may be something for us to revisit in a future post!\n\n\nIn this section we are using a straight dollar threshold instead of a percentage. This can cause difficulties because a ticker with a share price at 20 dollars increasing to 25 dollars is a HUGE increase. If a stock goes from 500 to 505 dollars that is not nearly as big of a deal. The point of this post is to show variety of options and get you thinking so we will keep this one as is and see how it shakes out when we test it the approach.\nLet’s codify our approach so we have a function we can use to test with later.\n\nfrom SimpleTimeSeries import get_momentum_regr_actions\nview_source_code(get_momentum_regr_actions)\n\n\n\n\n\n  \n  \n  \n\n\n\n\ndef get_momentum_regr_actions(df, n_periods,threshold):\n    _x = df.shift(n_periods)\n    \n    # Calculate Momentum\n    mom_window = df.rolling(n_periods)\n    mom_rate = mom_window.apply(lambda y: LinearRegression().\n                                    fit(np.array(range(n_periods)).\n                                    reshape(-1, 1),y).coef_)\n    mom_rate = mom_rate[n_periods:]\n    \n    # Select Action Based on Threshold\n    actions = pd.DataFrame(np.where(mom_rate < -threshold, 'Short',\n                           np.where(mom_rate > threshold,  'Buy',\n                                                                 '')),\n                   columns=mom_rate.columns,index=mom_rate.index)\n    \n    # Because we use close price, we can't make the trade action until the following day\n    actions.index = actions.index + timedelta(1)\n    \n    return actions\n\n\n\n\n\n\nactions = get_momentum_regr_actions(train.iloc[:,[3,-3,-8,1,30]],n_periods=28,threshold=.24)\nactions.head(10)\n\n\n\n\n\n  \n    \n      ticker\n      AAPL\n      ZBH\n      XOM\n      AAL\n      ALXN\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2013-08-10\n      Buy\n      Buy\n      \n      \n      Buy\n    \n    \n      2013-08-13\n      Buy\n      Buy\n      \n      \n      Buy\n    \n    \n      2013-08-14\n      Buy\n      \n      \n      \n      Buy\n    \n    \n      2013-08-15\n      Buy\n      \n      \n      \n      Buy\n    \n    \n      2013-08-16\n      Buy\n      \n      \n      \n      Buy\n    \n    \n      2013-08-17\n      Buy\n      \n      \n      \n      \n    \n    \n      2013-08-20\n      Buy\n      \n      \n      \n      \n    \n    \n      2013-08-21\n      Buy\n      \n      \n      \n      \n    \n    \n      2013-08-22\n      Buy\n      \n      \n      \n      \n    \n    \n      2013-08-23\n      Buy\n      \n      Short"
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#bollinger-bands",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#bollinger-bands",
    "title": "Simple Time Series (Stock Prices)",
    "section": "6.3 Bollinger Bands",
    "text": "6.3 Bollinger Bands\nA bollinger band uses a rolling standard deviation to determine when the stock price is unusually high or low. In theory if the price is doing something unexpected we can capitalize on that. So rather than a percent change, or a regression line, we are now picking it based on whether it’s unusually high or low per the standard deviation.\nLet’s walk through graphing a bolldinger band this on a couple tickers so we understand what’s going on. Then we can figure out how to use this to create a trading strategy.\n\nfrom SimpleTimeSeries import calculate_bollinger\nview_source_code(calculate_bollinger)\n\n\n\n\n\n  \n  \n  \n\n\n\n\ndef calculate_bollinger(df, tickers=['AAPL','MSFT'],window_sz=28,band_sz=2):\n    out = {}\n\n    for ticker in tickers:\n        raw = df.loc[:,ticker] \n        \n        # Calculate window statistics\n        _mean = raw.rolling(window_sz).mean()\n        _std = raw.rolling(window_sz).std()\n\n        # Calculate bands based on window statistics\n        upper_band = _mean + (band_sz*_std)\n        lower_band = _mean - (band_sz*_std)\n        \n        # Combine in a dataframe\n        _out = pd.concat([lower_band, raw, upper_band, ],axis=1)\n        _out.columns = ['lower_band','raw','upper_band']\n        _out['lower_limit'] = _out.raw < _out.lower_band\n        _out['upper_limit'] = _out.raw > _out.upper_band\n\n        out[ticker] = _out\n    return out\n\n\n\n\n\n\ncalculate_bollinger(train,['AAPL','MSFT','GOOG','AMZN'])['AAPL'].sample(3)\n\n\n\n\n\n  \n    \n      \n      lower_band\n      raw\n      upper_band\n      lower_limit\n      upper_limit\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2016-07-08\n      89.498384\n      93.405487\n      97.142827\n      False\n      False\n    \n    \n      2014-10-02\n      91.353234\n      93.418008\n      96.999605\n      False\n      False\n    \n    \n      2016-12-06\n      104.282310\n      107.352218\n      111.253007\n      False\n      False\n    \n  \n\n\n\n\n\nfrom SimpleTimeSeries import plot_bollinger\nview_source_code(plot_bollinger)\n\n\n\n\n\n  \n  \n  \n\n\n\n\ndef plot_bollinger(data,min_date,plt_cols=2):\n    # Create Plot    \n    rows = int(len(data.keys())/plt_cols)\n    fig, ax = plt.subplots(rows,plt_cols,figsize=(20,8*rows))\n    fig.suptitle(\"Bollinger Bands\",fontsize=50)\n\n    \n    for i,(ticker,df) in enumerate(data.items()):\n        # Determind plot location\n        row_num = int(i / plt_cols) if len(data.keys()) > 2 else i\n        col_num = i - (row_num * plt_cols)\n        \n        # Filter for dates\n        _d = data[ticker].loc[df.index>=min_date]\n        \n        # Draw Plots\n        if plt_cols >2: _tmp = ax[row_num,col_num]\n        else: _tmp = ax[row_num]\n        _tmp.set_title(ticker,fontsize=18)\n        _tmp.plot(_d[['lower_band','raw','upper_band']])\n        _tmp.scatter(_d[_d.lower_limit].index,_d[_d.lower_limit].raw,c='red')\n        _tmp.scatter(_d[_d.upper_limit].index,_d[_d.upper_limit].raw,c='red')\n\n\n\n\n\n\nplot_bollinger(calculate_bollinger(train),min_date='2016-01-01')\n\n\n\n\nThese charts show stock prices for Apple and Microsoft over time in orange. The green is our upper band, which in this case is 2 standard deviations above the mean when looking at the last 28 days of the adjusted close price. The blue is 2 standard deviations below the mean.\nWe’ve plotted red dots anywhere the stock price crosses these bounds. This can only happen if there is a significant enough shift for it to be 2 standard deviations from the mean. Let’s code up this third momentum-esque approach as well so we have a 3rd method to test. Where the price crosses a bollinger band we will take an action!\n\nfrom SimpleTimeSeries import get_bollinger_actions\nview_source_code(get_bollinger_actions)\n\n\n\n\n\n  \n  \n  \n\n\n\n\ndef get_bollinger_actions(df,window_sz=28,band_sz=2):\n    \n    # Calculate Statistics\n    bollinger_data = calculate_bollinger(df, tickers=df.columns,window_sz=window_sz,band_sz=band_sz)\n    \n    # Calculate Actions\n    _d = L()\n    for ticker,dataframe in bollinger_data.items():\n        _d.append(pd.DataFrame(np.where(dataframe['lower_limit'] == True, 'Short',\n                           np.where(dataframe['upper_limit'] == True,  'Buy',\n                                                                 '')),\n                   columns=[ticker],index=dataframe.index))\n    bollinger_actions = pd.concat(_d,axis=1)\n    \n    # Because we use close price, we can't make the trade action until the following day\n    bollinger_actions.index = bollinger_actions.index + timedelta(1)\n    \n    return bollinger_actions[window_sz:]\n\n\n\n\n\n\nactions = get_bollinger_actions(train.iloc[:,:5],window_sz=28,band_sz=2)\nactions.head(10)\n\n\n\n\n\n  \n    \n      \n      A\n      AAL\n      AAP\n      AAPL\n      ABBV\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2013-08-10\n      \n      \n      \n      \n      \n    \n    \n      2013-08-13\n      \n      \n      \n      \n      \n    \n    \n      2013-08-14\n      \n      Short\n      Buy\n      Buy\n      \n    \n    \n      2013-08-15\n      \n      Short\n      \n      Buy\n      \n    \n    \n      2013-08-16\n      \n      Short\n      \n      Buy\n      \n    \n    \n      2013-08-17\n      \n      Short\n      Short\n      Buy\n      Short\n    \n    \n      2013-08-20\n      \n      Short\n      Short\n      Buy\n      \n    \n    \n      2013-08-21\n      \n      \n      \n      \n      Short\n    \n    \n      2013-08-22\n      \n      \n      \n      \n      Short\n    \n    \n      2013-08-23\n      Buy"
  },
  {
    "objectID": "posts/Python/PythonConcurrency.html",
    "href": "posts/Python/PythonConcurrency.html",
    "title": "Python Concurrency",
    "section": "",
    "text": "Data Scientists that want to learn what they need to know to effectively use parallel programming quickly and easily in their projects.\n\n\n\n\nWhen I need to run something in parallel, should I use threads or processes?\n\nHow much of a speed up will I actually get?\n\nWhat’s the easiest way run something in parallel in python without lots of complicated/annoying boilerplate?\n\n\n\n\n\nThe Fastcore library that was created by the fastai(https://www.fast.ai/) team led by Jeremy Howard. It’s a really amazing library and has lots of really powerful features that I use on a daily basis. There’s a parallel module which I will be using for this post.\nSuper awesome article on Python Concurrency that was written by Hamel Husain. This is a great post that explains in detail exactly how it all works.\n\n\n\n\n\nThis will be application focused. Rather than examples such as a calculating fibonacci numbers, I will use real world examples for comparisons.\nRather than showing parallel processing from the foundations, we will be showing applications using the fastcore library.\n\nIf you want low level and deep understanding you should study Hamel’s post. His article also covers topics I am not covering in this post. If you want a shortcut to get the information needed to apply and use parallel processing for data science this post is ideal as it covers the most critical pieces of information. I recommend reading this post then follow it up with Hamel’s post for a top-down approach."
  },
  {
    "objectID": "posts/Python/PythonConcurrency.html#cpu-bound-task",
    "href": "posts/Python/PythonConcurrency.html#cpu-bound-task",
    "title": "Python Concurrency",
    "section": "2.1 CPU Bound Task",
    "text": "2.1 CPU Bound Task\nLet’s imagine you are cooking. You are working on cutting your ingredients for a side-dish and also need to cook a separate dish that needs to be stirred constantly/non-stop. How can you speed that up? Well you cannot divide your time because the dish needs to be stirred non-stop. And while you are doing that, the cutting isn’t getting done. You simply can’t just jump back and forth. You could however speed it up by adding a second chef.\nThis is similar to the trade-off for parallelizing using processes in python. Each chef needs their own tools for their task. This translates to each process needing it’s own data in memory. This means data is often duplicated in memory for each process. But, if your constraint is the CPU this is the way to go!\nFor our CPU bound task we are going to train a bunch of decision trees. Of course, for basic architectures libraries will generally do the parallelization for you. As you start getting into more custom architectures, ideas, and tweaks that’s when it gets very important to know how to parallelize something.\nThe crude way to check if something is CPU bound you can start it and look at the utilization of the CPU as it runs. If it is maxed out without parallelization then it’s CPU bound.\n\n\n\n\n\n\nTip\n\n\n\nFor CPU bound tasks you will want to parallelize using processes\n\n\n\n2.1.1 The Data\nWe are going to use the iris dataset as an example. It’s too small to do a meaningful example so I duplicate it a bunch to make it large enough to whee we can see what the difference in time is.\n\nfrom sklearn.datasets import load_iris\nfrom datetime import datetime\nfrom sklearn import tree\nimport numpy as np\nfrom fastcore.foundation import *\nfrom fastcore.parallel import *\nimport random\n\n\nX, y = load_iris(return_X_y=True)\nfor i in range(12): X,y = np.concatenate((X,X)),np.concatenate((y,y))\nprint(f'Our big iris dataset has {X.shape[0]} rows and {X.shape[1]} columns')\n\nOur big iris dataset has 614400 rows and 4 columns\n\n\n\n\n2.1.2 The Task\nWe are going to train a tree many times. Of course, we wouldn’t want to train the same tree each time, so we will allow for passing in indexes so that we get a different random subset of the data to train our different trees on\n\nidxs_all = L(L(random.sample(range(0, len(X)), int(len(X)*.8))) for i in range(50))\n\n\ndef train_tree(idxs,X=X,y=y):\n    X_tmp,y_tmp = X[idxs],y[idxs]\n    clf = tree.DecisionTreeClassifier()\n    clf = clf.fit(X_tmp, y_tmp)\n    return clf\n\n\n\n2.1.3 Comparison\nGreat so let’s train all our decision trees. I told you above that we should use process for this but I will do all three options so we can compare times and validate that.\n\nOption 1 - No Parallel: We will just use a standard for loop to do them sequentially.\nOption 2 - Threads: Parallelize using threads\nOption 3 - Processes: Parallelize using processes\n\nAs you can see below the right approach to parallel processing for this task (processes) meant less than 1/4 the time to complete the task. But the wrong approach to parallel processing in this task (threads) helped some but not nearly as much. This is a great example of why it’s important to understand how parallel processing works and what the constraint is so you can make smart choices quickly.\n\n\n\n\n\n\nTip\n\n\n\nThe parallel function I am using is from the fastcore library. Set your number of workers, and apply the function in the first argument to all in the second and you are done. Threadpool boolean determines whether it uses threads or processes for parallelization.\n\n\n\n# No Parallel\nst = datetime.now()\nout = L()\nfor idxs in idxs_all: out.append(train_tree(idxs))\nprint(f'Completion time without parallel processing: {datetime.now()-st}')\n\nCompletion time without parallel processing: 0:00:11.251791\n\n\n\n# Parallel using Threads\nst = datetime.now()\nparallel(train_tree, idxs_all, threadpool=True, n_workers=8, progress=False)\nprint(f'Completion time with thread parallelization: {datetime.now()-st}')\n\nCompletion time with thread parallelization: 0:00:06.415588\n\n\n\n# Parallel using Processes\nst = datetime.now()\nparallel(train_tree, idxs_all, threadpool=False, n_workers=5, progress=False)\nprint(datetime.now()-st)\nprint(f'Completion time with process parallelization: {datetime.now()-st}')\n\n0:00:03.352727\nCompletion time with process parallelization: 0:00:03.353085"
  },
  {
    "objectID": "posts/Python/PythonConcurrency.html#non-cpu-bound-task",
    "href": "posts/Python/PythonConcurrency.html#non-cpu-bound-task",
    "title": "Python Concurrency",
    "section": "2.2 Non CPU Bound Task",
    "text": "2.2 Non CPU Bound Task\nIn another cooking analogy, imagine you are making hard boiled eggs and rice. If you are doing 1 after the other how could you speed that up? Well you could:\n\nStart boiling the water for the egg\nWhile the water for the egg is heating put the rice and water in the rice cooker and start that\nChange tasks and add the egg to the hot water\nChange tasks and take the rice out when done\nChange tasks and take the egg out when done\n\nThis is how parallelizing using threads works in python. Rather than adding a chef you just have 1 chef divide there time and switch between tasks quickly. This isn’t possible if the CPU core is maxed out, but if the constraint is not the CPU it can help a ton. If the CPU is maxed out, multi-tasking won’t actually speed anything up.\nThe two main examples of non CPU bound tasks are network related tasks (ie Upload/Download) or disk related takes (ie reading or writing files). This is fairly common in data science. If you are doing deep learning on images for example you probably have tens of thousands of images you need to read in in batches every epoch. That will be our example.\n\n\n\n\n\n\nTip\n\n\n\nFor CPU bound tasks you will want to parallelize using threads\n\n\n\n2.2.1 The Data\nWe are going to use IMAGENETTE_320 and download using fastai’s external data module. We have a list of image paths!\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.IMAGENETTE_320)\n\nimg_paths = get_image_files(path)\nimg_paths = [str(o) for o in img_paths]\n\nprint(f'Number of images: {len(img_paths)}')\nprint(\"Thee examples of what's in this list:\")\nimg_paths[:3]\n\n\n\n\n\n\n    \n      \n      100.00% [341663744/341663724 00:18<00:00]\n    \n    \n\n\nNumber of images: 13394\nThee examples of what's in this list:\n\n\n['/Users/isaacflath/.fastai/data/imagenette2-320/train/n03394916/ILSVRC2012_val_00046669.JPEG',\n '/Users/isaacflath/.fastai/data/imagenette2-320/train/n03394916/n03394916_58454.JPEG',\n '/Users/isaacflath/.fastai/data/imagenette2-320/train/n03394916/n03394916_32588.JPEG']\n\n\n\n\n2.2.2 The Task\nWe are going to load in our images for training. We will do it in batches of 128. We aren’t doing a full pipeline with augmentation and all that. All we are doing is reading the images in to see the speed up in that piece. Let’s read in one image and display it to familiarize ourselves with the task.\n\nimport cv2\nbatch_size=128\nimg = cv2.imread(img_paths[0])\ncv2.imshow('image',img)\nplt.imshow(img); plt.axis('off'); plt.show()\n\n\n\n\n\n\n2.2.3 Comparisons\nNow we read the images in for the full dataset in batches of 128. I told you above that we should use threads for this but I will do all three options so we can compare times.\n\nOption 1 - No Parallel: We will just use a standard for loop to do them sequentially.\nOption 2 - Threads: Parallelize using threads\nOption 3 - Processes: Parallelize using processes\n\nAs you can see below the right approach to parallel processing for this task (threads) meant less than 1/3 the time to complete the task. But the wrong approach to parallel processing for this task (processes) means over 3x more time to complete the task. This is a great example of why it’s important to understand how parallel processing works and what the constraint is so you can make smart choices quickly.\n\n# No Parallel\nst = datetime.now()\nfor i in range(int(len(img_paths)/batch_size)): \n  batch_paths = img_paths[i*batch_size:i*batch_size+batch_size]\n  batch = L()\n  for img_path in batch_paths:\n    batch.append(cv2.imread(img_path))\nprint(datetime.now()-st)\nprint(f'Completion time without parallel processing: {datetime.now()-st}')\n\n0:00:24.941654\nCompletion time without parallel processing: 0:00:24.941788\n\n\n\n# Parallel using Threads\nst = datetime.now()\nfor i in range(int(len(img_paths)/128)):\n  batch_paths = img_paths[i*128:i*128+128]\n  parallel(cv2.imread, batch_paths, threadpool=True, n_workers=8, progress=False)\nprint(datetime.now()-st)\nprint(f'Completion time with thread parallelization: {datetime.now()-st}')\n\n0:00:08.041621\nCompletion time with thread parallelization: 0:00:08.042516\n\n\n\n# Parallel using Processes\nst = datetime.now()\nfor i in range(int(len(img_paths)/128)):\n  batch_paths = img_paths[i*128:i*128+128]\n  parallel(cv2.imread, batch_paths, threadpool=False, n_workers=8, progress=False)\nprint(datetime.now()-st)\nprint(f'Completion time with process parallelization: {datetime.now()-st}')\n\n0:01:13.452496\nCompletion time with process parallelization: 0:01:13.452966"
  },
  {
    "objectID": "posts/Python/CodingStyle.html",
    "href": "posts/Python/CodingStyle.html",
    "title": "My Coding Style",
    "section": "",
    "text": "This document outlines the general principles I try to follow while coding.\nBear in mind these are my preferences and I code in different styles depending on what project I am working on. If you’re on a big project and everyone is using pep8, you should follow pep8. Many in the python community feel strong emotions when they see non-pep8 code, so be aware of that too.\nBut if left to my own vices, this is what I do."
  },
  {
    "objectID": "posts/Python/CodingStyle.html#code-standards",
    "href": "posts/Python/CodingStyle.html#code-standards",
    "title": "My Coding Style",
    "section": "0.1 Code Standards",
    "text": "0.1 Code Standards\nPep8 says “A Foolish Consistency is the Hobgoblin of Little Minds”. I believe this strongly. While I don’t believe I should break from a standard for no reason, I also don’t believe I should follow a standard if I have reason not to."
  },
  {
    "objectID": "posts/Python/CodingStyle.html#automation",
    "href": "posts/Python/CodingStyle.html#automation",
    "title": "My Coding Style",
    "section": "0.2 Automation",
    "text": "0.2 Automation\nMany parts of code formatting can be easily automated. For items that can be automated, I do not worry about writing my code in that format as the computer can do that for me.\nFor example, if a few extra spaces makes things more legible to me but doesn’t conform to whatever the standards of the project are I will write it with the extra spaces and use a script to remove extra spaces later."
  },
  {
    "objectID": "posts/Python/CodingStyle.html#duplicate-code",
    "href": "posts/Python/CodingStyle.html#duplicate-code",
    "title": "My Coding Style",
    "section": "0.3 Duplicate Code",
    "text": "0.3 Duplicate Code\nDuplicate code as little as possible. Duplicate code means:\n\nChanges must be implemented in multiple places\nMultiple of the same thing means multiple opportunities for a bug\nMore to analyze when doing pull reviews - easier to miss things\nSlower development because small changes must be applied more\nI have to break my chain of thought when making a change to look for other places the same code exists"
  },
  {
    "objectID": "posts/Python/CodingStyle.html#assume-a-modern-monitor",
    "href": "posts/Python/CodingStyle.html#assume-a-modern-monitor",
    "title": "My Coding Style",
    "section": "2.1 Assume a modern monitor",
    "text": "2.1 Assume a modern monitor\nI assume readers have a modern monitor and text can span most of the screen horizontally, within reason."
  },
  {
    "objectID": "posts/Python/CodingStyle.html#make-differences-apparent",
    "href": "posts/Python/CodingStyle.html#make-differences-apparent",
    "title": "My Coding Style",
    "section": "2.2 Make differences apparent",
    "text": "2.2 Make differences apparent\n\nGroup similar things together and make differences apparent, for example this makes it very easy to see the differences in these functions.\n\n    def add     (a, b): return a +  b\n    def multiply(a, b): return a *  b\n    def power   (a, b): return a ** b"
  },
  {
    "objectID": "posts/Python/CodingStyle.html#minimize-scrolling",
    "href": "posts/Python/CodingStyle.html#minimize-scrolling",
    "title": "My Coding Style",
    "section": "2.3 Minimize Scrolling",
    "text": "2.3 Minimize Scrolling\nThe more scrolling I have to do, the harder it is to have “flow”. I want to spend my time thinking about code, not searching for code.\nThis means I try to consolidate lines that I think can be consolidate. For example I think it’s perfectly fine to consolidate import statements like this:\n    import logging, string, pandas as pd, sqlparse\nOr to consolidate variable initialization like this:\n    vara, varb, varc = [], 0, {}\nThere are of course limits to this - use common sense. Generally if you use plain english to describe the intent of the line it should be a simple sentence.\nFor example “Import required libraries” is fine. “initialize variables with starting values” is fine. “Predict value, append that to list, then calculate mean squared error by comparing to truth list” is probably too much for 1 line."
  },
  {
    "objectID": "posts/Python/Python.html",
    "href": "posts/Python/Python.html",
    "title": "Python Programming Tips",
    "section": "",
    "text": "Code\nfrom functools import partial\nfrom datetime import datetime\nimport logging, string, pandas as pd, sqlparse\nfrom fastcore.all import *\nfrom fastcore.docments import *\nfrom IPython.display import Markdown,display, HTML\nimport pandas as pd\n\nfrom pygments import highlight\nfrom pygments.lexers import PythonLexer\nfrom pygments.formatters import HtmlFormatter\n\ndef print_function_source(fn):\n    fn = print_decorator\n    formatter = HtmlFormatter()\n    display(HTML('<style type=\"text/css\">{}</style>{}'.format(\n        formatter.get_style_defs('.highlight'),\n        highlight(inspect.getsource(fn), PythonLexer(), formatter))))"
  },
  {
    "objectID": "posts/Python/Python.html#parallel-processing",
    "href": "posts/Python/Python.html#parallel-processing",
    "title": "Python Programming Tips",
    "section": "5.1 Parallel Processing",
    "text": "5.1 Parallel Processing\nSee this blog post\n\n5.1.1 Docments\nNice way of documenting code concisely and being able to access info from code. It’s concise, easy to manipulate to display how you want, and easy to read. I much prefer it over the large numpy style docstrings that are big string blocks\n\nfrom fastcore.docments import *\n\ndef distance(pointa:tuple,  # tuple representing the coordinates of the first point (x,y)\n             pointb:tuple=(0,0) # tuple representing the coordinates of the first point (x,y)\n            )->float: # float representing distance between pointa and pointb\n    '''Calculates the distance between pointa and pointb'''\n    edges = np.abs(np.subtract(pointa,pointa))\n    distance = np.sqrt((edges**2).sum())\n    return distance\n\n\ndocstring(distance)\n\n'Calculates the distance between pointa and pointb'\n\n\n\ndocments(distance)\n\n{ 'pointa': 'tuple representing the coordinates of the first point (x,y)',\n  'pointb': 'tuple representing the coordinates of the first point (x,y)',\n  'return': 'float representing distance between pointa and pointb'}\n\n\n\ndocments(distance,full=True)\n\n{ 'pointa': { 'anno': <class 'tuple'>,\n              'default': <class 'inspect._empty'>,\n              'docment': 'tuple representing the coordinates of the first '\n                         'point (x,y)'},\n  'pointb': { 'anno': <class 'tuple'>,\n              'default': (0, 0),\n              'docment': 'tuple representing the coordinates of the first '\n                         'point (x,y)'},\n  'return': { 'anno': <class 'float'>,\n              'default': <class 'inspect._empty'>,\n              'docment': 'float representing distance between pointa and '\n                         'pointb'}}\n\n\n\n\n5.1.2 Testing\nEveryone agrees testing is important. But not all testing is equal. The needs for unit testing the google code base are not the same as the needs a data scientist needs for building and deploying models, libraries, and most software.\nFastcore is a great tool for most of my testing needs. Fast and simple enough that I can add tests as I build and as I am exploring and building models. I want testing to enhance my development workflow, not be something I have to painstakingly build at the end.\nSometimes simple assert statements are sufficient, but there’s small annoyances. For example, a small change in type can mean a failed test. Sometimes that change in type should cause a failure, sometimes I’m ok if it’s a different type if the values are the same\n\nfrom fastcore.test import *\n\n\ntest_eq([1,2],(1,2))\n\nFor floating points it has handy functionality for that, which is very common in data science. For example, we may want .1 + .1 + .1 == .3 to be true, because they are close enough based on floating point precision\n\n.1 + .1 + .1 == .3\n\nFalse\n\n\n\ntest_close(.1 + .1 + .1, .3)\n\nWe can test that something fails, if there are particular situation we want to ensure raise errors.\n\ndef _fail(): raise Exception(\"foobar\")\ntest_fail(_fail)\n\nWe can test if 2 lists have the same values, just in different orders (convenient for testing some situations with random mini-batches).\n\na = list(range(5))\nb = a.copy()\nb.reverse()\ntest_shuffled(a,b)\n\nThere’s more of course, check out the docs\n\n\n5.1.3 L\nL is a replacement for a list, but with lots of adding functionality. Some of it are functional programming concepts, some is numpy like stuff, and some is just niceities (like cleaner printing).\n\nalist = L(1,2,3,4,3)\n\n\nalist.sort()\nalist.sorted()\n\n(#5) [1,2,3,3,4]\n\n\n\nalist.unique()\n\n(#4) [1,2,3,4]\n\n\n\nalist.filter(lambda x: x < 3)\n\n(#2) [1,2]\n\n\n\nalist.map(lambda x: x * 2)\n\n(#5) [2,4,6,8,6]\n\n\n\n\n5.1.4 AttrDict\nAttrdict is another nice thing from fastcore, that makes dictionaries a bit nicer to use.\n\nregdict = {'a':2,'b':3}\nadict = AttrDict({'a':2,'b':3})\n\n\nadict\n\n{'a': 2, 'b': 3}\n\n\n\nadict.a\n\n2\n\n\n\ndef _fail(): return regdict.a\ntest_fail(_fail)"
  },
  {
    "objectID": "posts/Python/Python.html#filter",
    "href": "posts/Python/Python.html#filter",
    "title": "Python Programming Tips",
    "section": "7.1 Filter",
    "text": "7.1 Filter\nFilter is a common higher order function.\n\nL(1,2,3,4,5).filter(lambda x: x>3)\n\n(#2) [4,5]\n\n\nThis is very flexible because we can put filtering logic of any complexity in a function and use that to filter a list of any type."
  },
  {
    "objectID": "posts/Python/Python.html#map",
    "href": "posts/Python/Python.html#map",
    "title": "Python Programming Tips",
    "section": "7.2 Map",
    "text": "7.2 Map\nMap is another very common higher order function.\n\nL(1,2,3,4,5).map(lambda x: x**2)\n\n(#5) [1,4,9,16,25]\n\n\nIt is again super flexible because we can apply a function of any complexity to have it be applied and modify each element of the list.\n\nL(1,2,3,4,5).map(lambda x: string.ascii_lowercase[x])\n\n(#5) ['b','c','d','e','f']"
  },
  {
    "objectID": "posts/Python/Python.html#simple-logging",
    "href": "posts/Python/Python.html#simple-logging",
    "title": "Python Programming Tips",
    "section": "7.3 Simple Logging",
    "text": "7.3 Simple Logging\nWe could make a function for logging, where we can pass a function in that we want to use for logging (ie info vs warning).\n\ndef log_stuff(msg,fn=logger.info,**kwargs): \n    dt = get_current_time()\n    fn(f\"{dt}|{msg}\")\n    for k,v in kwargs.items(): fn(f\"{dt}|{k}={v}\")\n\n\nlog_stuff('abcd',a=1,b=55)\n\n\n!tail -3 mylog.log\n\nINFO:root:20221106_193211|abcd\nINFO:root:20221106_193211|a=1\nINFO:root:20221106_193211|b=55\n\n\n\nlog_stuff('something might be awry',fn=logger.critical,a=1,b=55)\n\n\n!tail -3 mylog.log\n\nCRITICAL:root:20221106_193211|something might be awry\nCRITICAL:root:20221106_193211|a=1\nCRITICAL:root:20221106_193211|b=55"
  },
  {
    "objectID": "posts/Python/Python.html#file-processor",
    "href": "posts/Python/Python.html#file-processor",
    "title": "Python Programming Tips",
    "section": "7.4 File Processor",
    "text": "7.4 File Processor\nYou can also make a generic file processor that you can pass callbacks to. This file processor can include log statements to log what you’re doing, so you can minimize repeating lots of code. For now, we’ll do a simple processor, and callbacks to clean and format a messy sql file.\n\ndef process_file(fpath,callbacks): \n    with open(fpath, \"r\") as f: contents = f.read()\n    for callback in callbacks: contents = callback(contents)\n    return contents"
  },
  {
    "objectID": "posts/Python/Python.html#format-and-clean-sql-file",
    "href": "posts/Python/Python.html#format-and-clean-sql-file",
    "title": "Python Programming Tips",
    "section": "7.5 Format and clean SQL file",
    "text": "7.5 Format and clean SQL file\n\nsql_formatter_cb = partial(sqlparse.format,\n                strip_comments=True,comma_first=True,\n                keyword_case='upper', identifier_case='lower',\n                reindent=True, indent_width=4,)\n\n\n\nqrys = process_file('test.sql',[sql_formatter_cb,sqlparse.split])\n\n\ndef sql_pprint(sql): display(Markdown(f\"```sql\\n\\n{sql}\\n\\n```\"))\nfor qry in qrys: sql_pprint(qry)\n\n\nSELECT top 25 *\nFROM some_table;\n\n\n\nSELECT count(1)\nFROM another TABLE ;\n\n\n\nSELECT date_time\n     , mbr_id\n     , transactions\n     , count(1)\nFROM table3\nWHERE date_time > '2021-02-02'\nGROUP BY 1\n       , 2\n       , 3;"
  },
  {
    "objectID": "posts/Python/Python.html#silly-simple-example",
    "href": "posts/Python/Python.html#silly-simple-example",
    "title": "Python Programming Tips",
    "section": "8.1 Silly Simple Example",
    "text": "8.1 Silly Simple Example\n\ndef add_another(func):\n    def wrapper(number):\n        print(f\"The decorator took over!\")\n        print(f\"I could log the original number ({number}) here!\")\n        print(f\"Or I could log the original answer ({func(number)}) here!\")\n        return func(number) + 1\n    return wrapper\n    \n@add_another\ndef add_one(number): return number + 1\n\nSo when we use a decorator, the code in the wrapper function is called instead of the original function. Typically the wrapper function calls the original function (otherwise there would be no point in decorating it as you’d just have a new unrelated function)."
  },
  {
    "objectID": "posts/Python/Python.html#useful-example",
    "href": "posts/Python/Python.html#useful-example",
    "title": "Python Programming Tips",
    "section": "8.2 Useful Example",
    "text": "8.2 Useful Example\nFor example, maybe you want to print (or log) particular function call times and the args. See this decorator that does just that (and can be used on methods too)\n\nfrom datetime import datetime\n\n\ndef print_decorator(func):\n    def wrapper(*args, **kwargs):\n        print(f\"{datetime.now()}:{func}:args={args}:kwargs={kwargs}\")\n        return func(*args, **kwargs)\n    return wrapper\n\n\n@print_decorator\ndef simple_add(a,b): return a + b\n\n\nsimple_add(2,4)\n\n2022-11-02 14:18:56.635936:<function simple_add>:args=(2, 4):kwargs={}\n\n\n6\n\n\n\n@print_decorator\ndef complex_add(a,b,*args,**kwargs): \n    out = a + b\n    for arg in args: out = out + arg\n    for kwarg in kwargs.values(): out = out + kwarg\n    return out\n\n\ncomplex_add(5,2,3,foo=6,bar=10)\n\n2022-11-02 14:18:57.716085:<function complex_add>:args=(5, 2, 3):kwargs={'foo': 6, 'bar': 10}\n\n\n26"
  },
  {
    "objectID": "posts/Python/Python.html#use-on-existing-functions",
    "href": "posts/Python/Python.html#use-on-existing-functions",
    "title": "Python Programming Tips",
    "section": "8.3 Use on Existing Functions",
    "text": "8.3 Use on Existing Functions\nWhat we have seen is applying a decorator to functions we fully define but we can also apply them to previously existing functions like ones we import from a library. This is helpful not just in understanding one way you can extend an existing libraries functionality, but also in understanding what decorators are. They aren’t magical.\nLet’s add logging to pd.DataFrame using our existing decorator so we can see when a dataframe is constructed.\n\nLoggingDataFrame = print_decorator(pd.DataFrame)\ndf = LoggingDataFrame([1,2,3])\n\n2022-11-02 14:53:16.323144:<class 'pandas.core.frame.DataFrame'>:args=([1, 2, 3],):kwargs={}\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      1\n    \n    \n      1\n      2\n    \n    \n      2\n      3\n    \n  \n\n\n\n\nThe key thing to notice here is that the @ syntax really isn’t doing anything magical. It’s just passing the function into the decorator and using that as the function definition. It’s just syntactic sugar for a higher order function that takes a function and returns a function.\nTo understand why this works, think through what our decorator is doing. 1. It’s a function that takes a function as an argument 2. It creates a new function called wrapper. This wrapper function called the argument passed into it, but also has other code. 3. It returns that function as the output\n\nprint_function_source(print_decorator)\n\ndef print_decorator(func):\n    def wrapper(*args, **kwargs):\n        print(f\"{datetime.now()}:{func}:args={args}:kwargs={kwargs}\")\n        return func(*args, **kwargs)\n    return wrapper"
  },
  {
    "objectID": "posts/Python/Python.html#silly-simple-example-1",
    "href": "posts/Python/Python.html#silly-simple-example-1",
    "title": "Python Programming Tips",
    "section": "9.1 Silly Simple Example",
    "text": "9.1 Silly Simple Example\n\nclass aClass: a = 2\n    \nclass bClass(aClass): pass\n    \naClass.a == bClass.a\n\nTrue"
  },
  {
    "objectID": "posts/Python/Python.html#useful-examples",
    "href": "posts/Python/Python.html#useful-examples",
    "title": "Python Programming Tips",
    "section": "9.2 Useful Examples",
    "text": "9.2 Useful Examples\nIn many cases there are common things we want to inherit in lots of classes. One example is having access to the date. Often you want this for logging, or printing, or any number of things. By subclassing you don’t have to reformat the date each time in your classes.\n\nclass DateMinuteMixin:\n    date_format='%Y%m%d_%H%M%S'\n    dte = datetime.now()\n\n    @property\n    def date_str(self): return self.dte.strftime(self.date_format)\n\nAnother handy use is to have generic behavior for handling different file types. In this case, we have a mixin where it opens and reads a sql file. Rather than rewriting this code for every class that needs to read a sql file, you can inherit from a class when you need that functionality.\n\n\n\n\n\n\nTip\n\n\n\nYou can define an abstract property like below to let users know that after inheriting this class, they need to define that property. In this case, they define the sql_filepath, and they get the contents of the file for free via the other methods.\n\n\n\nimport abc\n\nclass SqlFileMixin:\n    @abc.abstractproperty\n    def sql_filepath(self):\n        pass\n\n    @property\n    def sql_file(self):\n        return open(self.sql_filepath)\n\n    @property\n    def query(self):\n        return self.sql_file.read()"
  },
  {
    "objectID": "posts/Python/Python.html#silly-simple-example-2",
    "href": "posts/Python/Python.html#silly-simple-example-2",
    "title": "Python Programming Tips",
    "section": "11.1 Silly Simple Example",
    "text": "11.1 Silly Simple Example\n\ndef mapper(items,fn):\n    for item in items: yield item\n\n\nit = mapper([2,4,6,8],square)\nit\n\n<generator object mapper>\n\n\n\nnext(it), next(it), next(it)\n\n(2, 4, 6)\n\n\nYou can also process it sequentially in a loop.\n\nfor item in mapper([2,4,6,8],square): \n    print(item)\n\n2\n4\n6\n8"
  },
  {
    "objectID": "posts/Python/Python.html#useful-example-1",
    "href": "posts/Python/Python.html#useful-example-1",
    "title": "Python Programming Tips",
    "section": "11.2 Useful Example",
    "text": "11.2 Useful Example\n\n11.2.1 File Streaming\n\nprint_plus = partial(print,end='\\n++++++\\n')\n\nwith open('test.txt', 'rb') as f:\n    iterator = iter(partial(f.read, 64), b'')\n    print_plus(type(iterator))\n    for block in iterator: print_plus(block)\n\n<class 'callable_iterator'>\n++++++\nb'one\\ntwo\\nthree\\nfour\\nfive\\nsix\\nseven\\neight\\nnine\\nten\\neleven\\ntwelve\\nt'\n++++++\nb'hirteen\\nninety nine thousand nine hundred ninety\\nninety nine tho'\n++++++\nb'usand nine hundred ninety one\\nninety nine thousand nine hundred '\n++++++\nb'ninety two\\nninety nine thousand nine hundred ninety three\\nninety'\n++++++\nb' nine thousand nine hundred ninety four\\nninety nine thousand nin'\n++++++\nb'e hundred ninety five\\nninety nine thousand nine hundred ninety s'\n++++++\nb'ix\\nninety nine thousand nine hundred ninety seven\\nninety nine th'\n++++++\nb'ousand nine hundred ninety eight\\nninety nine thousand nine hundr'\n++++++\nb'ed ninety nine\\n'\n++++++"
  },
  {
    "objectID": "posts/NeuralNetworks/NeuralNetworksXOR.html",
    "href": "posts/NeuralNetworks/NeuralNetworksXOR.html",
    "title": "Neural Networks and XOR",
    "section": "",
    "text": "This post dives into why Neural Networks are so powerful from a foundational level. The goal is to show an example of a problem that a Neural Network can solve easily that strictly linear models cannot solve. We will do this in the simplest example, the XOR.\nI will cover what an XOR is in this article, so there aren’t any prerequisites for reading this article. Though if this is your first time hearing of the XOR, you may not understand the implications or the importance of solving the XOR.\n\n1 Credit\nI got the idea to write a post on this from reading the deep learning book. The information covered in this post is also in that book, though the book covers more information and goes into more detail. The primary benefit of this post over the book is that this post is in ‘python pytorch’ notation where the book covers this material in ‘math’ notation. If this post is interesting to you, I would recommend checking out the book.\n\n\n\n\n\n\nBook Information\n\n\n\n\nTitle: Deep Learning\nAuthor: Ian Goodfellow and Yoshua Bengio and Aaron Courville\nPublisher: MIT Press\nURL: http://www.deeplearningbook.org,\nYear: 2016\n\n\n\n\n\n2 The Problem\nIn the graph below we see the XOR operator outputs. XOR is similar to OR. If either one of the bits is positive, then the result is positive. The difference is that if both are positive, then the result is negative.\nWe can see the 2 classes as red and blue dots on the visualization. Try to draw a single line that divides the red dots from the blue dots and you will find that it cannot be done. A linear model simply cannot classify these points accurately\n\n#collapse-hide\nfrom torch import Tensor\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n#collapse-hide\nx = Tensor([[0,0],[0,1],[1,0],[1,1]])\ny = [0,1,1,0]\nout = pd.DataFrame(x,columns = ['x1','x2']); out['XOR'] = y\n\nclass1 = out[out.XOR==0].iloc[:,:2]; class2 = out[out.XOR==1].iloc[:,:2]\nfig, ax = plt.subplots(figsize=(4,4))\nax.scatter(class1.x1,class1.x2,c='red'); ax.scatter(class2.x1,class2.x2,c='blue')\nax.set_xlabel('Input1'); ax.set_ylabel('Input2')\nax.set_title('Input Data')\nplt.show()\n\n\n\n\n\n\n3 Neural Network Feature Space\nNow that we see a linear model cannot solve the problem, or said another way it’s not linearly seperable, let’s see how a Neural Network would help.\nWe start by defining the tensors that we need:\n\nx: This shows all the points and are the inputs we are using to predict with. You can verify the points on the graph above.\nw: This is the weight matrix. A linear layer is xw + b.\n\nb: This is the bias. A linear layer is xw + b\ny: This is the dependent variable we are trying to predict (whether the dot is blue or red, or XOR operator output)\n\n\n\n\n\n\n\nNote\n\n\n\nThe text and some libraries do transposes to have wx + b, but it’s the same thing.\n\n\n\nx = Tensor([[0,0],[0,1],[1,0],[1,1]])\ny = [0,1,1,0]\nw = torch.ones(2,2); \nb = Tensor([0,-1])\n\nNow we do out linear layer with activation function and store that in h.\n\nx@w + b : This is the linear function\ntorch.clip : This is replacing any negative values with 0. The fancy term for this is a ReLU or Rectified Linear Unit\n\n\nh = torch.clip(x@w + b,0)\n\nInstead of plotting our inputs like we did above (when we saw this problem couldn’t be solved linearly), let’s plot the outputs of layer we just calculated.\nAs you can see when we plot the output of the first layer it’s trivial to separate the blue and red points with a line. We have created a representation of the data that makes it very easy to classify the points to solve the XOR problem!\n\n#collapse-hide\nout = pd.DataFrame(h,columns = ['x1','x2']); out['XOR'] = y\nclass1 = out[out.XOR==0].iloc[:,:2]; class2 = out[out.XOR==1].iloc[:,:2]\nfig, ax = plt.subplots(figsize=(4,4))\nax.scatter(class1.x1,class1.x2,c='red'); ax.scatter(class2.x1,class2.x2,c='blue')\nax.set_xlabel('Feature1'); ax.set_ylabel('Feature2')\nax.set_title('Hidden Features')\nplt.show()\n\n\n\n\n\n\n4 Model Predictions\nNow that it’s linearly separable, we can easily add an output layer to form out predictions. All we do for this is multiply my another Tensor so that we get the correct number of outputs. In this case we have 4 points we want to classify, so we have 4 outputs (1 per point).\nWe see that the model was able to solve the XOR problem\n\nh = torch.clip(x@w + b,0) @ Tensor([1,-2])\nres = pd.DataFrame(x.int().numpy(),columns=['x1','x2']); res['preds']=h.int(); res['targets']=y\nres\n\n\n\n\n\n  \n    \n      \n      x1\n      x2\n      preds\n      targets\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      1\n      1\n      1\n    \n    \n      2\n      1\n      0\n      1\n      1\n    \n    \n      3\n      1\n      1\n      0\n      0"
  },
  {
    "objectID": "posts/NeuralNetworks/PlantPathologyKaggle.html",
    "href": "posts/NeuralNetworks/PlantPathologyKaggle.html",
    "title": "Plant Pathology Kaggle",
    "section": "",
    "text": "This post is looking at the plant pathology kaggle competition and showing how you could create a top 10 kaggle submission with fastai. This is the first of a blog series where I will do this with historical kaggle competitions. Many techniques will come from the winning solution code base by the “Alipay Tian Suan Security Lab Kaggle” team, but I will make a modifications as I see fit. I will explain those as they come up.\nThe original code was in pytorch lightning. I will be using Fastai, which will allow me to simplify the solution considerably without sacrificing results."
  },
  {
    "objectID": "posts/NeuralNetworks/PlantPathologyKaggle.html#getting-distilled-soft-labels",
    "href": "posts/NeuralNetworks/PlantPathologyKaggle.html#getting-distilled-soft-labels",
    "title": "Plant Pathology Kaggle",
    "section": "8.1 Getting Distilled (soft) labels",
    "text": "8.1 Getting Distilled (soft) labels\nHere’s the 5 fold to get predictions on our entire training set.\n\n# #hide_output\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nsplits, preds, targs, preds_c,  = [],[],[],[]\n\ntrue = pd.DataFrame(columns = L(o for o in train.columns))\npred = pd.DataFrame(columns = L(o for o in train.columns))\ni=0\nfor _, val_idx in skf.split(train.image_id,train.label):\n    splitter = IndexSplitter(val_idx)\n\n    # Create dataloaders splittin on indexes defined by StratifiedKFold\n    db = DataBlock(blocks=(ImageBlock,CategoryBlock),\n                    get_x=ColReader(0),get_y=ColReader(5),\n                    item_tfms=item_tfms,batch_tfms=batch_tfms,\n                    splitter=splitter)\n    dls = db.dataloaders(train,bs=24)\n\n    #train model with fastai dataloaders, pytorch model, pytorch loss function, fastai gradient clipping, custom callback, on fp16 precision \n    learn = Learner(dls,se_resnext50_32x4d(),loss_func=CrossEntropyLossOneHot(),cbs=[GradientClip,SoftLabelCB()]).to_fp16()\n    learn.fine_tune(80,reset_opt=True) # Train freeze epoch then unfreeze for 80 epochs   \n\n    p, _ = learn.tta() # test time augmentation\n    p=p.softmax(axis=1) # Convert to probabilities\n\n    # Format dataframe to save\n    items_pred=pd.DataFrame(p,columns=dls.vocab)\n    items_pred['label'] = [dls.vocab[int(o)] for o in p.argmax(dim=1)]\n    items_pred['image_id'] = dls.valid.items.image_id.values\n    items_pred = items_pred[train.columns]\n    true = pd.concat([true,dls.valid.items])\n    pred = pd.concat([pred,items_pred])\n\n    # predict and submit to kaggle\n    test_predict(i,f'distilling labels fold count {i}') \n    i+=1\n\n\npred.to_csv('distilled_labels.csv',index=False)\n\nGreat! So that got us to third place with the best model already! Here’s the fold scores. Our second place model as well as an average of all 5 fold predictions were both also in the top 10. We successfully got into the top 10 already with 3 submissions. It is also helpful to note that the most reasonable 2 models to pick would be the average of all 5 fold predictions as well as the 3rd place submission based on the public leaderboard (as when choosing which to use in an active competition you wouldn’t be able to see the private leaderboard score). Mission accomplished!\nFirst place in the competition had a Private Leaderboard score of 0.98445 with 10th place had 0.97883 for reference.\nAs you can see, fastai is plenty capable to compete on the cutting edge at the top of kaggle competitions."
  },
  {
    "objectID": "posts/NeuralNetworks/PlantPathologyKaggle.html#training-with-soft-labeling",
    "href": "posts/NeuralNetworks/PlantPathologyKaggle.html#training-with-soft-labeling",
    "title": "Plant Pathology Kaggle",
    "section": "8.2 Training with Soft Labeling",
    "text": "8.2 Training with Soft Labeling\nI am not going to go through the rest of the implementation, but the key technique in the rest is soft labeling. Since this is not a super commonly known technique I am going to explain how to do it in fastai.\nSoft labeling is taking a weighted average of a predicted label and a truth label and training your model on that. This tends to be a good thing to try when some of your labels are wrong. Generally labels that the model get wrong are more likely to be the mislabeled ones, and so when you take a weighted average those get label smoothing applies which makes your model react less to those outliers.\nSo how to do it within fastai? Well we already had the callback set up that allows our model to take one hot encoded labels, so all we need to do is set the weights in our dataframe and re-run what we did before. At this point the fastai implementation is done (the callback), and all I need to do is pass it the soft labeled values as the targets instead of the one hot encoded I was using before.\nTo demonstrate:\n\nLoad training labels and datat\n\n\ntrain = process_df('data/train.csv')\ntrain = train.sort_values('image_id')\n\n\nLoad predictions from previous k-fold\n\n\ndistilled_labels = pd.read_csv('distilled_labels.csv')\ndistilled_labels = distilled_labels.sort_values('image_id');\n\n# Get one hot encoded labels (zeros and ones)\ndistilled_labels.iloc[:,1:-1] = pd.get_dummies(distilled_labels.label)\n\n\nWeight as you see fit (winner did 7:3 ratio)\n\n\n# check to make sure the data matches\nassert (train.image_id.values==distilled_labels.image_id.values).all()\ndistilled_labels.reset_index(drop=True,inplace=True); train.reset_index(drop=True,inplace=True); \n\n# get soft labels\ntrain.iloc[:,1:-1] = distilled_labels.iloc[:,1:-1] * .3 + train.iloc[:,1:-1] * .7\ntrain.loc[train.healthy == 0.3][:5]\n\n\n\n\n\n  \n    \n      \n      image_id\n      healthy\n      multiple_diseases\n      rust\n      scab\n      label\n    \n  \n  \n    \n      33\n      data/images/Train_1027.jpg\n      0.3\n      0.0\n      0.0\n      0.7\n      scab\n    \n    \n      47\n      data/images/Train_104.jpg\n      0.3\n      0.7\n      0.0\n      0.0\n      multiple_diseases\n    \n    \n      146\n      data/images/Train_1129.jpg\n      0.3\n      0.7\n      0.0\n      0.0\n      multiple_diseases\n    \n    \n      392\n      data/images/Train_1350.jpg\n      0.3\n      0.0\n      0.0\n      0.7\n      scab\n    \n    \n      574\n      data/images/Train_1514.jpg\n      0.3\n      0.7\n      0.0\n      0.0\n      multiple_diseases\n    \n  \n\n\n\n\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      image_id\n      healthy\n      multiple_diseases\n      rust\n      scab\n      label\n    \n  \n  \n    \n      0\n      data/images/Train_0.jpg\n      0.0\n      0.0\n      0.0\n      1.0\n      scab\n    \n    \n      1\n      data/images/Train_1.jpg\n      0.0\n      1.0\n      0.0\n      0.0\n      multiple_diseases\n    \n    \n      2\n      data/images/Train_10.jpg\n      0.0\n      0.0\n      1.0\n      0.0\n      rust\n    \n    \n      3\n      data/images/Train_100.jpg\n      1.0\n      0.0\n      0.0\n      0.0\n      healthy\n    \n    \n      4\n      data/images/Train_1000.jpg\n      0.0\n      0.0\n      1.0\n      0.0\n      rust\n    \n  \n\n\n\n\nGreat - that’s exactly what we want. We were already using the SoftLabelCB above - so that’s it. You can rerun the same kfold code above or train a single model. Feel free to check out the repository linked above to see the exact implementation and details if you want to do a recreation of their results and the rest of their solution!\nJust remember If you got the results you are looking for in a significantly easier way than you have thought - This is a good thing and a testament to the power of the library (fastai). This not an indication that you are using a beginner tool. Fastai takes care of all the normal SoTA best practices for you so you can focus on bleeding edge implementations or problem specific challenges. Sometimes that means when you go to implement something or work a problem it “just works” with lot less effort than you were expecting and you are left thinking “That’s it?”. How could that possibly be a reasonable reason not to use the library or to think it’s not an effective library?\n\ntwitter: https://twitter.com/BBrainkite/status/1359192051837984778"
  },
  {
    "objectID": "posts/NeuralNetworks/ChatBots.html",
    "href": "posts/NeuralNetworks/ChatBots.html",
    "title": "Practical Chatbots",
    "section": "",
    "text": "I developed a process and app for creating and deploying chatbots for support and question answering purposes for Novetta. This solution focused on practicality including minimizing the dataset needed, minimizing training/retraining needed, eliminating the need to retrain if policies change, and empowering domain experts to expand and improve the responses of without requiring a ML engineer for every tweak.\nThis approach was built on the sentence transformer (siamese BERT) architecture and the core machine learning technology was semantic similarity. I built it using dash, but any deployment method could be used (slack bot, email bot, web app, etc.)\nTo read the blog post click here"
  },
  {
    "objectID": "posts/NeuralNetworks/NLPTokenizationFoundations.html",
    "href": "posts/NeuralNetworks/NLPTokenizationFoundations.html",
    "title": "NLP Tokenization Foundations",
    "section": "",
    "text": "1 Intro\nIn this post we are going to dive into NLP, specifically Tokenization. Tokenization are the foundation of all NLP.\nSo what is a language model? In short, it is a model that uses the preceding words to predict the next word. We do not need separate labels, because they are in the text. This is training the model on the nuances of the language you will be working on. If you want to know if a tweet is toxic or not, you will need to be able to read and understand the tweet in order to do that. The language model helps with understanding the tweet - then you can use that model with those weights to tune it for the final task (determining whether the tweet is toxic or not).\nFor this post, I will be using news articles to show how to tokenize a news article and numericalize it to get ready for deep learning.\n\n\n2 Credit Where Credit is Due\nThe concept and techniques covered in this post are covered in much greater detail in Jeremy Howard and Sylvain Gugger’s book. If you like this post, you should buy the book as you’ll probably like it even more!\n\n\n3 The Data\nI will be using the “All-the-news” dataset from this site. https://components.one/datasets/all-the-news-2-news-articles-dataset/\nI downloaded then put the csv into a sqlite database for convenience\n\nimport pandas as pd\nimport sqlite3\ncon = sqlite3.connect('../../../data/news/all-the-news.db')\n\n\npd.read_sql_query('SELECT publication, min(date),max(date), count(*) from \"all-the-news-2-1\" group by publication order by max(date) desc limit 5', con)\n\n\n\n\n\n  \n    \n      \n      publication\n      min(date)\n      max(date)\n      count(*)\n    \n  \n  \n    \n      0\n      Buzzfeed News\n      2016-02-19 00:00:00\n      2020-04-02 00:00:00\n      32819\n    \n    \n      1\n      The New York Times\n      2016-01-01 00:00:00\n      2020-04-01 13:42:08\n      252259\n    \n    \n      2\n      Business Insider\n      2016-01-01 03:08:00\n      2020-04-01 01:48:46\n      57953\n    \n    \n      3\n      Washington Post\n      2016-06-10 00:00:00\n      2020-04-01 00:00:00\n      40882\n    \n    \n      4\n      TMZ\n      2016-01-01 00:00:00\n      2020-04-01 00:00:00\n      49595\n    \n  \n\n\n\n\nI am going to pick the 5 most recent New York times Articles. For the final model I will use all of the data, but for simplicity of demonstrating tokenization we will use just 5 articles. Here is an example of the start of one of the articles\n\ndf = pd.read_sql_query('SELECT article from \"all-the-news-2-1\" where publication = \"The New York Times\" order by date desc limit 5', con)\nex = df.iloc[1,0]; ex[:162]\n\n'President Trump told of “hard days that lie ahead” as his top scientific advisers released models predicting that the U.S. death toll would be 100,000 to 240,000.'\n\n\n\n\n4 Tokenization\nSo how do I turn what I see above (text) into something a neural network can use? The first layer in a neural network is going to do matrix multiplication and addition. How do I multiply “President Trump told of “hard days that lie ahead” as his top scientific advisers released models” by any number? This is the core question we will answer with tokenization.\n\n\n\n\n\n\nTip\n\n\n\nTokenization is the method in which we take text and turn them into numbers we can feed into a model\n\n\n\n4.0.1 A simple Approach\nLet’s start with a simple idea. Let’s treat each word as separate inputs in the same way that separate pixels in an image are separate inputs. We can do this in the english language by splitting our text by spaces/\n\nex[:162]\n\n'President Trump told of “hard days that lie ahead” as his top scientific advisers released models predicting that the U.S. death toll would be 100,000 to 240,000.'\n\n\n\nimport numpy as np\ntokens = ex.split(sep = ' ')\ntokens[:10]\n\n['President',\n 'Trump',\n 'told',\n 'of',\n '“hard',\n 'days',\n 'that',\n 'lie',\n 'ahead”',\n 'as']\n\n\nThat’s better, now we have distinct data points. But we need them to be numbers in order to multiply and add them. So let’s replace each work with a number.\nTo do this we will get a unique list of all of the words, then assign a number to each word.\n\nfrom fastai2.text.all import *\nvocab = L(tokens).unique()\n\n\nword2idx = {w:i for i,w in enumerate(vocab)}\n\nWe have 20165 words, but only 1545 unique words. Each of those assigned a number in a dictionary.\n\nlen(ex),len(vocab)\n\n(21065, 1545)\n\n\nWe can see that each word gets a number.\n\nlist(word2idx.items())[:5]\n\n[('President', 0), ('Trump', 1), ('told', 2), ('of', 3), ('“hard', 4)]\n\n\nNow all we have to do is replace our tokens with the numbers in our word2idx dictionary. Lets take a look at 10 words near the end of our article and see what it looks like as tokens as well as numbers\n\nnums = L(word2idx[i] for i in tokens)\nnums[3000:3010],L(tokens[3000:3010])\n\n((#10) [1359,24,17,943,1360,1361,388,331,77,1362],\n (#10) ['linked','to','the','coronavirus.','Only','Italy','has','recorded','a','worse'])\n\n\n\n\n4.0.2 Next Steps\nWhile this is the idea behind tokenization, there are many things that were not considered. Here are some other ideas to consider when choosing a tokenization approach.\n\nWhat holds meaning other than words in english that we could make into tokens? What about punctuation or a comma? What about the beginning of a sentence or paragraph?\nShould ‘words’ and ‘word’ be tokenized as 2 separate words? Or could we assign ‘word’ and ‘s’ as the tokens because the base of the word has the same meaning?\nIs there a better way to break up a sentence other than by words? What if it were just based on common sentence strings. Maybe ‘of a’ could be 1 token rather than 2. could ’ dis’ or ‘ing’ be tokens that can be combined with many different words?"
  },
  {
    "objectID": "posts/NeuralNetworks/NeuralNetworkFoundationsP2.html",
    "href": "posts/NeuralNetworks/NeuralNetworkFoundationsP2.html",
    "title": "Neural Network Foundations (Part 2)",
    "section": "",
    "text": "from fastai.vision.all import *\nfrom fastai.data.external import *\nfrom PIL import Image\nimport math\n\n\n1 Intro\nToday we will be working with the MNIST dataset. The goal is going to be to take an image of handwritten digits and automatically predict what number it is. We will be building a Neural Network to do this. This is building off of the previous post where we classified 3s vs 7s. If anything in this post is confusing, I recommend heading over to that post first.\n\n\n\n\n\n\nNote\n\n\n\nIf you get through this and want more detail, I highly recommend checking out Deep Learning for Coders with fastai & Pytorch by Jeremy Howard and Sylvain Gugger. All of the material in this guide and more is covered in much greater detail in that book. They also have some awesome courses on the fast.ai website, such as their deep learning course\n\n\n\n\n2 Load the Data\nThe first step is to get and load the data. We’ll look at it a bit to make sure it was loaded properly as well. We will be using fastai’s built in dataset feature rather than sourcing it ourself. We will skim over this quickly as this was covered in part 1.\n\n# This command downloads the MNIST_TINY dataset and returns the path where it was downloaded\npath = untar_data(URLs.MNIST)\n\n# This takes that path from above, and get the path for training and validation\ntraining = [x.ls() for x in (path/'training').ls().sorted()]\nvalidation = [x.ls() for x in (path/'testing').ls().sorted()]\n\nLet’s take a look at an image. The first thing I recommend doing for any dataset is to view something to verify you loaded it right. The second thing is to look at the size of it. This is not just for memory concerns, but you want to generally know some basics about whatever you are working with.\n\n# Let's view what one of the images looks like\nim3 = Image.open(training[6][1])\nim3\n\n\n\n\n\n# Let's see what shape the underlying matrix is that represents the picture\ntensor(im3).shape\n\ntorch.Size([28, 28])\n\n\n\n\n3 Linear Equation\nWe are looking to do wx + b = y. In a single class classifier, y has 1 column as it is predicting 1 thing (0 or 1). In a multi-class classifier y has “however-many-classes-you-have” columns.\n\n3.0.1 Tensor Setup\nFirst we get our xs and ys in tensors in the right format.\n\ntraining_t = list()\nfor x in range(0,len(training)):\n    # For each class, stack them together.  Divide by 255 so all numbers are between 0 and 1\n    training_t.append(torch.stack([tensor(Image.open(i)) for i in training[x]]).float()/255)\n    \nvalidation_t = list()\nfor x in range(0,len(validation)):\n    # For each class, stack them together.  Divide by 255 so all numbers are between 0 and 1\n    validation_t.append(torch.stack([tensor(Image.open(i)) for i in validation[x]]).float()/255)\n\n\n# Let's make sure images are the same size as before\ntraining_t[1][1].shape\n\ntorch.Size([28, 28])\n\n\nWe can do simple average of one of our images as a sanity check. We can see that after averaging, we get a recognizable number. That’s a good sign.\n\nshow_image(training_t[5].mean(0))\n\n<AxesSubplot:>\n\n\n\n\n\n\n# combine all our different images into 1 matrix.  Convert Rank 3 tensor to rank 2 tensor.\nx = torch.cat([x for x in training_t]).view(-1, 28*28)\nvalid_x = torch.cat([x for x in validation_t]).view(-1, 28*28)\n\n# Defining Y.  I am starting with a tensor of all 0.  \n# This tensor has 1 row per image, and 1 column per class\ny = tensor([[0]*len(training_t)]*len(x))\nvalid_y = tensor([[0]*len(validation_t)]*len(valid_x))\n\n# Column 0 = 1 when the digit is a 0, 0 when the digit is not a 0\n# Column 1 = 1 when the digit is a 1, 0 when the digit is not a 1\n# Column 2 = 1 when the digit is a 2, 0 when the digit is not a 2\n# etc.\nj=0\nfor colnum in range(0,len(training_t)):\n    y[j:j+len(training_t[colnum]):,colnum] = 1\n    j = j + len(training[colnum])\n    \nj=0\nfor colnum in range(0,len(validation_t)):\n    valid_y[j:j+len(validation_t[colnum]):,colnum] = 1\n    j = j + len(validation[colnum])\n\n\n# Combine by xs and ys into 1 dataset for convenience.\ndset = list(zip(x,y))\nvalid_dset = list(zip(valid_x,valid_y))\n\n# Inspect the shape of our tensors\nx.shape,y.shape,valid_x.shape,valid_y.shape\n\n(torch.Size([60000, 784]),\n torch.Size([60000, 10]),\n torch.Size([10000, 784]),\n torch.Size([10000, 10]))\n\n\nPerfect. We have exactly what we need and defined above. 60,000 images x 784 pixels for x and 60,000 images x 10 classes for my predictions.\n10,000 images make up the validation set.\n\n\n3.0.2 Calculate wx + b\nLet’s initialize our weights and biases and then do the matrix multiplication and make sure the output is the expected shape (60,000 images x 10 classes).\n\n# Random number initialization\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\n# Initialize w and b weight tensors\nw = init_params((28*28,10))\nb = init_params(10)\n\n\n# Linear equation to see what shape we get.\n(x@w+b).shape,(valid_x@w+b).shape\n\n(torch.Size([60000, 10]), torch.Size([10000, 10]))\n\n\nWe have the right number of predictions. The predictions are no good because all our weights are random, but we know we’ve got the right shapes.\nThe first thing we need to do is turn our Linear Equation into a Neural Network. To do that we need to do this twice with a ReLu inbetween.\n\n\n\n4 Neural Network\n\n\n\n\n\n\nImportant\n\n\n\nYou can check out previous blog post that does thin in a simpler problem (single class classifier) and assumes less pre-requisite knowledge. I am assuming that the information in Part 1 is understood. If you understand Part 1, you are ready for this post!\n\n\n\n# Here's a simple Neural Network.  \n# This can have more layers by duplicating the patten seen below, this is just the fewest layers for demonstration.\n\ndef simple_net(xb): \n    \n    # Linear Equation from above\n    res = xb@w1 + b1 #Linear\n    \n    # Replace any negative values with 0.  This is called a ReLu.\n    res = res.max(tensor(0.0)) #ReLu\n    \n    # Do another Linear Equation\n    res = res@w2 + b2 #Linear\n    \n    # return the predictions\n    return res\n\n\n# initialize random weights.  \n# The number 30 here can be adjusted for more or less model complexity.\n\nmultipliers = 30\n\nw1 = init_params((28*28,multipliers))\nb1 = init_params(multipliers)\nw2 = init_params((multipliers,10))\nb2 = init_params(10)\n\n\nsimple_net(x).shape # 60,000 images with 10 predictions per class (one per digit)\n\ntorch.Size([60000, 10])\n\n\n\n\n5 Improving Weights and Biases\nWe have predictions with random weights and biases. We need to find the right numbers for the weights and biases rather than random numbers. To do this we need to use gradient descent to improve the weights. Here’s roughly what we need to do:\n\nCreate a loss function to measure how close (or far) off we are\nCalculate the gradient (slope) so we know which direction to step\nAdjust our values in that direction\nRepeat many times\n\nThe first thing we need in order to use gradient descent is a loss function. Let’s use something simple, how far off we were. If the correct answer was 1, and we predicted a 0.5 that would be a loss of 0.5. We will do this for every class\nWe will add something called a sigmoid. A sigmoid ensures that all of our predictions land between 0 and 1. We never want to predict anything outside of these ranges.\n\n\n\n\n\n\nNote\n\n\n\nIf you want more of a background on what is going on here, please take a look at my series on Gradient Descent where I dive deeper on this. We will be calculating a gradient - which are equivalent to the “Path Value”\n\n\n\n5.0.1 Loss Function\n\ndef mnist_loss(predictions, targets):\n    \n    # make all prediction between 0 and 1\n    predictions = predictions.sigmoid()\n    \n    # Difference between predictions and target\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\n\n# Calculate loss on training and validation sets to make sure the function works\nmnist_loss(simple_net(x),y),mnist_loss(simple_net(valid_x),valid_y)\n\n(tensor(0.5195, grad_fn=<MeanBackward0>),\n tensor(0.5191, grad_fn=<MeanBackward0>))\n\n\n\n\n5.0.2 Calculate Gradient\nWE now have a function we need to optimize and a loss function to tell us our error. We are ready for gradient descent. Let’s create a function to change our weights.\nFirst, we will make sure our datasets are in a DataLoader. This is convenience class that helps manage our data and get batches.\n\n# Batch size of 256 - feel free to change that based on your memory\ndl = DataLoader(dset, batch_size=1000, shuffle=True)\nvalid_dl = DataLoader(valid_dset, batch_size=1000)\n\n# Example for how to get the first batch\nxb,yb = first(dl)\nvalid_xb,valid_yb = first(valid_dl)\n\n\ndef calc_grad(xb, yb, model):\n    \n    # calculate predictions\n    preds = model(xb)\n    \n    # calculate loss\n    loss = mnist_loss(preds, yb)\n    \n    # Adjust weights based on gradients\n    loss.backward()\n\n\n\n5.0.3 Train the Model\n\nNote: This is the same from part 1\n\n\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\n\n\n5.0.4 Measure Accuracy on Batch\n\ndef batch_accuracy(xb, yb):    \n    # this is checking for each row, which column has the highest score.\n    # p_inds, y_inds gives the index highest score, which is our prediction.\n    p_out, p_inds = torch.max(xb,dim=1)\n    y_out, y_inds = torch.max(yb,dim=1)\n    \n    # Compre predictions with actual\n    correct = p_inds == y_inds\n    \n    # average how often we are right (accuracy)\n    return correct.float().mean()\n\n\n\n5.0.5 Measure Accuracy on All\n\nNote: This is the same from part 1\n\n\ndef validate_epoch(model):\n    # Calculate accuracy on the entire validation set\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    \n    # Combine accuracy from each batch and round\n    return round(torch.stack(accs).mean().item(), 4)\n\n\n\n5.0.6 Initialize weights and biases\n\n# When classifying 3 vs 7 in part one, we just used 30 weights.  \n# With this problem being much harder, I will give it more weights to work with\n\ncomplexity = 500 \nw1 = init_params((28*28,complexity))\nb1 = init_params(complexity)\nw2 = init_params((complexity,10))\nb2 = init_params(10)\n\nparams = w1,b1,w2,b2\n\n\n\n5.0.7 Train the Model\nBelow we will actually train our model.\n\nlr = 50\n# epoch means # of passes through our data (60,000 images)\nepochs = 30\nloss_old = 9999999\n\nfor i in range(epochs):\n    train_epoch(simple_net, lr, params)\n    \n    # Print Accuracy metric every 10 iterations\n    if (i % 10 == 0) or (i == epochs - 1):\n        print('Accuracy:'+ str(round(validate_epoch(simple_net)*100,2))+'%')\n        \n    loss_new = mnist_loss(simple_net(x),y)\n    \n    loss_old = loss_new\n\nAccuracy:18.71%\nAccuracy:31.39%\nAccuracy:34.11%\nAccuracy:34.81%\n\n\n\n\n5.0.8 Results\nA few key points:\n\nThe Loss is not the same as the metric (Accuracy). Loss is what the models use, Accuracy is more meaningful to us humans.\nWe see that our loss slowly decreases each epoch. Our accuracy is getting better over time as well.\n\n\n\n5.0.9 This Model vs SOTA\nWhat is different about this model than a best practice model?\n\nThis model is only 1 layer. State of the art for image recognition will use more layers. Resnet 34 and Resnet 50 are common (34 and 50 layers). This would just mean we would alternate between the ReLu and linear layers and duplicate what we are doing with more weights and biases.\nMore weights and Biases. The Weights and Biases I used are fairly small - I ran this extremely quickly on a CPU. With the appropriate size weight and biases tensors, it would make way more sense to use a GPU.\nMatrix Multiplication is replaced with Convolutions for image recognition. A Convolution can be thought of as matrix multiplication if you averaged some of the pixels together. This intuitively makes sense as 1 pixel in itself is meaningless without the context of other pixels. So we tie them together some.\nDropout would make our model less likely to overfit and less dependent on specific pixels. It would do this by randomly ignoring different pixels so it cannot rely on them. It’s very similar to how decision trees randomly ignore variables for their splits.\nDiscriminate learning rates means that the learning rates are not the same for all levels of the neural network. With only 1 layer, naturally we don’t worry about this.\nGradient Descent - we can adjust our learning rate based on our loss to speed up the process\nTransfer learning - we can optimize our weights on a similar task so when we start trying to optimize weights on digits we aren’t starting from random variables.\n\nKeep training for as many epochs as we see our validation loss decrease\n\nAs you can see, these are not completely different models. These are small tweaks to what we have done above that make improvements - the combination of these small tweaks and a few other tricks are what elevate these models. There are many ‘advanced’ variations of Neural Networks, but the concepts are typically along the lines of above. If you boil them down to what they are really doing without all the jargon - they are pretty simple concepts."
  },
  {
    "objectID": "posts/NeuralNetworks/MixupDeepDive.html",
    "href": "posts/NeuralNetworks/MixupDeepDive.html",
    "title": "Mixup Deep Dive",
    "section": "",
    "text": "1 Intro\nMixup is a very powerful data augmentation tool that is super helpful tool to have in your toolbox, especially when you don’t have enough data and are over-fitting. In this post we are going to dive into what mixup is.\nThe goal of this post is to communicate an intuitive understanding of what mixup is and why it works. The goal of this post is not to show you the intricacies of training a model using mixup - that will be reserved for a future post. If you don’t know what the tool is, it’s impossible to have good intuition on how and when to use it.\nWe will be using the Pets dataset to demonstrate this.\n\n\n\n\n\n\nBonus Challenge\n\n\n\nAs you go through each step, think about what other kinds of data you may be able to apply these concepts to. Could you apply these transformations to NLP Embeddings? Could you apply these transformations to Tabular Data?\n\n\n\n\n2 Setup\n\n2.0.1 Get Libraries/Data\n\nfrom fastai.data.external import *\nfrom fastai.vision.all import *\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\nfrom functools import partial,update_wrapper\n\n\nseed = 42\n\n# Download and get path for dataseet\npath = untar_data(URLs.PETS) #Sample dataset from fastai2\npath.ls()\n\nDownloading a new version of this dataset...\n\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 00:13<00:00]\n    \n    \n\n\n(#2) [Path('/Users/isaacflath/.fastai/data/oxford-iiit-pet/images'),Path('/Users/isaacflath/.fastai/data/oxford-iiit-pet/annotations')]\n\n\n\n\n2.0.2 Helper Functions\n\ndef plot_images(imgs):\n    rcParams['figure.figsize'] = 10, 20\n    imgs_len = len(imgs)\n    for x in range(0,imgs_len):\n        plt.subplot(1,imgs_len,x+1)\n        plt.imshow(imgs[x])\n\n\n\n2.0.3 Data Setup\nDataBlocks and data loaders are convenient tools that fastai has to help us manage an load data. There is a lot going on in the DataBlock API and I am going to break it down piece by piece in this post so that we can get to talking about mixup.\n\n2.0.3.1 DataBlock\n\npets = DataBlock(\n    blocks = (ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    splitter= RandomSplitter(valid_pct = 0.2, seed=seed),\n    get_y= using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'),'name'),\n    item_tfms=Resize(460),\n    batch_tfms=aug_transforms(min_scale = 0.9,size=224)\n    )\n\n\n\n2.0.3.2 Dataloader\nThe dataloader is what we will actually interact with. In the DataBlock we defined lots of things we need to do to get and transform images, but not where to get them from. We define that in the dataloader.\n\ndls = pets.dataloaders(path/\"images\")\n\n\n\n\n\n3 Mixup Explained\nTo understand what mixup is we need to look at a couple images and see how a Neural Network would see them, and then apply mixup and look at the same images after the augmentation is applied. To say that another way. we want to understand the inputs and the outputs.\n\n3.0.1 x: No Mixup\nLet’s use 2 images as an example. I have plotted them below.\n\nim1 = tensor(Image.open((path/'images').ls()[8]).resize((500,371))).float()/255; \nim2 = tensor(Image.open((path/'images').ls()[6]).resize((500,371))).float()/255; \n\n\nplot_images([im1,im2])\n\n\n\n\nGreat, so the inputs are the pictures. What are the outputs? Well the output is going to be what breed they are. Let’s see what breed they are.\n\n(path/'images').ls()[8],(path/'images').ls()[6]\n\n(Path('/Users/isaacflath/.fastai/data/oxford-iiit-pet/images/Siamese_47.jpg'),\n Path('/Users/isaacflath/.fastai/data/oxford-iiit-pet/images/Birman_167.jpg'))\n\n\nWe can see in the file name that the dog is a leonberger and the cat is a ragdoll. Now we need to translate that into the one-hot encoded matrix for our model to predict. Looking at dls.vocab gives us all the class names.\n\ndls.vocab\n\n['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair', 'Egyptian_Mau', 'Maine_Coon', 'Persian', 'Ragdoll', 'Russian_Blue', 'Siamese', 'Sphynx', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond', 'leonberger', 'miniature_pinscher', 'newfoundland', 'pomeranian', 'pug', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier']\n\n\n\n\n3.0.2 y: No Mixup\nNow we define y for these 2 images. We have 1 column per class. When looking at the vocab above we saw that there were 37 classes. All of them will be 0 except the target.\nLet’s start by figuring out which column is the target (ie leonberger and ragdoll). Then we just need a tensor of length 37 that is all zeros except that position which will be a 1.\n\nlist(dls.vocab).index('leonberger'),list(dls.vocab).index('Ragdoll')\n\n(25, 8)\n\n\n\n# 37 classes long, all 0 except position 25 which represents leonberger and is 1\ny_leonberger = tensor([0]*25+[1]+[0]*(37-26))\n\n# 37 classes long, all 0 except position 8 which represents Ragdoll and is 1\ny_Ragdoll = tensor([0]*8+[1]+[0]*(37-9))\n\nprint(y_leonberger)\nprint(y_Ragdoll)\n\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\nGreat! We have our images that go in, and our output we want to predict. This is what a normal neural network is going to try to predict. Let’s see whats different if we use these 2 images with the Mixup data Augmentation instead.\n\n\n3.0.3 x: With Mixup\nFor the images we are going to apply an augmentation. Mixup mixes to images together.\nLet’s take a mix of the 2 images. We will take 40% of the first image, and 60% of the second image and plot them. We are doing this by multiplying the actual pixel values in a weighted average.\nFor example, if the pixel 1 value from image 1 * .4 + pixel 1 value from image 2 * .6 and that will equal pixel 1 value in my new image. Take a look at the third image and you can see it really does have a bit of each image in there.\n\nim_mixup = im1*.6+im2*.4\n\n\nplot_images([im1,im2,im_mixup])\n\n\n\n\n\n\n3.0.4 y: With Mixup\nWe have our new augmented image with mixup. Clearly it’s not really fair to call it 100% of either class. In fact it’s 60% of one class and 40% of the other. We should make our y represent taht.\nWe already have our ys when they are 100% of either class, so lets just take 60% of one + 40% of the other exactly like we did for our images. That will give us an appropriate label.\n\n# 37 classes long, all 0 except position 25 which represents leonberger and is 1\ny_leonberger = tensor([0]*25+[1]+[0]*(37-26))\n\n# 37 classes long, all 0 except position 8 which represents Ragdoll and is 1\ny_Ragdoll = tensor([0]*8+[1]+[0]*(37-9))\n\ny_mixup = y_leonberger*.6+y_Ragdoll*.4\n\nprint(y_leonberger)\nprint(y_Ragdoll)\nprint(y_mixup)\n\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\ntensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000])\n\n\n\n\n3.0.5 What weights?\nHere I took 60% of one image and 40% of the other. You could to a 90/10 split. Or a 99/1 split. Or a 50/50 split. I picked relatively close weights so it’s easy to see, but you should play around and see what works.\n\n\n\n4 FastAI mixup\nApplying the basic Mixup in fastai is super easy. Here’s how you can create a CNN using Mixup.\n\nlearn = cnn_learner(dls,resnet34,cbs=MixUp)"
  },
  {
    "objectID": "posts/NeuralNetworks/NeuralNetworkFoundationsP1.html",
    "href": "posts/NeuralNetworks/NeuralNetworkFoundationsP1.html",
    "title": "Neural Network Foundations (Part 1)",
    "section": "",
    "text": "from fastai.vision.all import *\nfrom fastai.data.external import *\nfrom PIL import Image\nimport math\ntorch.manual_seed(100)\n\n<torch._C.Generator>\n\n\n\n1 Intro\nToday we will be working with a subset of the MNIST dataset. The goal is going to be to take an image of handwritten digits and automatically predict whether it is a 3 or a 7. We will be building a Neural Network to do this.\n\n\n\n\n\n\nNote\n\n\n\nIf you get through this and want more detail, I highly recommend checking out Deep Learning for Coders with fastai & Pytorch by Jeremy Howard and Sylvain Gugger. All of the material in this guide and more is covered in much greater detail in that book. They also have some awesome courses on the fast.ai website, such as their deep learning course\n\n\n\n\n2 Load the Data\nThe first step is to get and load the data. We’ll look at it a bit along the way to make sure it was loaded properly and we understand it. We will be using fastai’s built in dataset feature rather than sourcing it ourself.\n\n# This command downloads the MNIST_TINY dataset and returns the path where it was downloaded\npath = untar_data(URLs.MNIST_TINY)\n\n# This takes that path from above, and get the path for the threes and the sevens\nthrees = (path/'train'/'3').ls().sorted()\nsevens = (path/'train'/'7').ls().sorted()\n\nLet’s take a look at an image. The first thing I reccomend doing for any dataset is to view something to verify you loaded it right. The second thing is to look at the size of it. This is not just for memory concerns, but you want to generally know some basics about whatever you are working with.\nNow that we’ve loaded our images let’s look at what one looks like.\n\nim3 = Image.open(threes[1])\nim3\n\n\n\n\nUnder the hood it’s just a 28x28 matrix!\n\ntensor(im3).shape\n\ntorch.Size([28, 28])\n\n\nLet’s put the matrix into a pandas dataframe. We will then color each cell based on the value in that cell.\nWhen we do that, we can clearly see that this is a 3 just from the values in the tensor. This should give a good idea for the data we are working with and how an image can be worked with (it’s just a bunch of numbers).\n\npd.DataFrame(tensor(im3)).loc[3:24,6:20].style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n\n\n\n\n  \n    \n       \n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n    \n  \n  \n    \n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      0\n      0\n      0\n      77\n      181\n      254\n      255\n      95\n      88\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      5\n      0\n      3\n      97\n      242\n      253\n      253\n      253\n      253\n      251\n      117\n      15\n      0\n      0\n      0\n      0\n    \n    \n      6\n      0\n      20\n      198\n      253\n      253\n      253\n      253\n      253\n      253\n      253\n      239\n      59\n      0\n      0\n      0\n    \n    \n      7\n      0\n      0\n      108\n      248\n      253\n      244\n      220\n      231\n      253\n      253\n      253\n      138\n      0\n      0\n      0\n    \n    \n      8\n      0\n      0\n      0\n      110\n      129\n      176\n      0\n      83\n      253\n      253\n      253\n      194\n      24\n      0\n      0\n    \n    \n      9\n      0\n      0\n      0\n      0\n      0\n      26\n      0\n      83\n      253\n      253\n      253\n      253\n      48\n      0\n      0\n    \n    \n      10\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      83\n      253\n      253\n      253\n      189\n      22\n      0\n      0\n    \n    \n      11\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      83\n      253\n      253\n      253\n      138\n      0\n      0\n      0\n    \n    \n      12\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      183\n      253\n      253\n      253\n      138\n      0\n      0\n      0\n    \n    \n      13\n      0\n      0\n      0\n      0\n      0\n      65\n      246\n      253\n      253\n      253\n      175\n      4\n      0\n      0\n      0\n    \n    \n      14\n      0\n      0\n      0\n      0\n      0\n      172\n      253\n      253\n      253\n      253\n      70\n      0\n      0\n      0\n      0\n    \n    \n      15\n      0\n      0\n      0\n      0\n      0\n      66\n      253\n      253\n      253\n      253\n      238\n      54\n      0\n      0\n      0\n    \n    \n      16\n      0\n      0\n      0\n      0\n      0\n      17\n      65\n      232\n      253\n      253\n      253\n      149\n      5\n      0\n      0\n    \n    \n      17\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      45\n      141\n      253\n      253\n      253\n      123\n      0\n      0\n    \n    \n      18\n      0\n      0\n      0\n      41\n      205\n      205\n      205\n      33\n      2\n      128\n      253\n      253\n      245\n      99\n      0\n    \n    \n      19\n      0\n      0\n      0\n      50\n      253\n      253\n      253\n      213\n      131\n      179\n      253\n      253\n      231\n      59\n      0\n    \n    \n      20\n      0\n      0\n      0\n      50\n      253\n      253\n      253\n      253\n      253\n      253\n      253\n      253\n      212\n      0\n      0\n    \n    \n      21\n      0\n      0\n      0\n      21\n      187\n      253\n      253\n      253\n      253\n      253\n      253\n      253\n      212\n      0\n      0\n    \n    \n      22\n      0\n      0\n      0\n      0\n      9\n      58\n      179\n      251\n      253\n      253\n      253\n      219\n      44\n      0\n      0\n    \n    \n      23\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      139\n      253\n      253\n      130\n      49\n      0\n      0\n      0\n    \n    \n      24\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n\n\n\n\n3 Defining our Linear Equation\nOne of the foundations of neural networks are linear layers, in this case xw + b = y.\n\n3.0.1 Setup\nWe will need a weight matrix w with 1 weight per pixel, meaning this will be a 784 row by 1 column matrix.\nWe are also going to add b, so let’s initialize that as well. Since we haven’t solved the problem yet, we don’t know what good values for w and b are so we will make them random to start with.\n\n\n\n\n\n\nNote\n\n\n\nWhen we checked the shape above, we saw our images were 28 x 28 pixels, which is 784 total pixels.\n\n\n\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\nw = init_params((28*28,1))\nb = init_params(1)\n\n\nmax(w)\n\ntensor([2.8108], grad_fn=<UnbindBackward0>)\n\n\nNow we just need x and y. A 784x1 matrix times a 1x784 matrix. We want all values to be between 0 and 1 so we divide by the max pixel value (255).\n\n# open each image and convert them to a tensor\nthrees_t = [tensor(Image.open(o)) for o in threes]\nsevens_t = [tensor(Image.open(o)) for o in sevens]\n\n# Get list of tensors and \"stack\" them.  Also dividing by 255 so all values are between 0 and 1\nthrees_s = torch.stack(threes_t).float()/255\nsevens_s = torch.stack(sevens_t).float()/255\n\n# Verify max and min pixel values\ntorch.min(threes_s), torch.max(threes_s), torch.min(sevens_s), torch.max(sevens_s)\n\n(tensor(0.), tensor(1.), tensor(0.), tensor(1.))\n\n\nNext we do a simple average of all our threes together and see what we get. It’s a nice sanity check to see that we did things ok. We can see that after averaging, we pretty much get a three!\n\nshow_image(threes_s.mean(0))\n\n<AxesSubplot:>\n\n\n\n\n\nLets finish defining our x. We want x to have both threes and sevens, but right now they are separated into different variables. We will use torch.cat to concatenate them, and .view to change the format of the matrix to the right shape. Y is being defined as a long matrix with 1 row per image (prediction) and 1 column.\n\n# combine our threes and sevens into 1 matrix.  Convert Rank 3 matrix to rank 2.\nx = torch.cat([threes_s, sevens_s]).view(-1, 28*28)\n\n# Set my y, or dependent variable matrix.  A three will be 1, and seven will be 0.  So we will be prediction 0 or 1.\ny = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\n\n# Combine by xs and ys into 1 dataset for convenience.\ndset = list(zip(x,y))\nx_0,y_0 = dset[0]\nx_0.shape,y_0\n\n(torch.Size([784]), tensor([1]))\n\n\nPerfect. We have exactly what we need and defined above. A 784 x 1 matrix times a 1 x 784 matrix + a constanct = our prediction. Let’s take a look to verify things are the right shape, and if we actually multiply these things together we get 1 prediction per image.\n\nw.shape,x_0.shape,b.shape,y_0.shape\n\n(torch.Size([784, 1]), torch.Size([784]), torch.Size([1]), torch.Size([1]))\n\n\n\nprint((x@w+b).shape)\n(x@w+b)[1:10]\n\ntorch.Size([709, 1])\n\n\ntensor([[ 3.3164],\n        [ 5.2035],\n        [-3.7491],\n        [ 1.2665],\n        [ 2.2916],\n        [ 1.3741],\n        [-7.6092],\n        [ 1.3464],\n        [ 2.7644]], grad_fn=<SliceBackward0>)\n\n\nGreat! We have the right number of predictions. The predictions are not good, because weights and biases are all random. Let’s do something about that.\nWe will need to do everything that we did above on out validation set, so let’s do that now.\n\n\nCode\nvalid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])\nvalid_3_tens = valid_3_tens.float()/255\nvalid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()])\nvalid_7_tens = valid_7_tens.float()/255\nvalid_3_tens.shape,valid_7_tens.shape\nvalid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\nvalid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\nvalid_dset = list(zip(valid_x,valid_y))\n\n\n\n\n\n4 Loss Function\nWe need to improve our weights and biases (w and b) and we do that using gradient descent. I have a few posts on gradient descent, feel free to check those out if you want details on how it works. Here we will use the built-in pytorch functionality.\nThe first thing we need to use gradient descent is we need a loss function. Let’s use something simple; how far off we were. If the correct answer was 1, and we predicted a 0.5 that would be a loss of 0.5.\nThe one addition is that we will add something called a sigmoid. All a sigmoid is doing is ensuring that all of our predictions land between 0 and 1. We never want to predict anything outside of these ranges as those are our 2 categories.\n\ndef mnist_loss(predictions, targets):\n    \n    # make all prediction between 0 and 1\n    predictions = predictions.sigmoid()\n    \n    # Difference between predictions and target\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\n\n\n5 Gradient Descent\n\n5.0.1 Background and Setup\n\npredict\ncalculate loss\ncalculate gradient\nsubtract from weights and bias\n\nNow we have a function we need to optimize and a loss function to tell us our error. We are ready for gradient descent. Let’s create a function to change our weights.\n\n\n\n\n\n\nNote\n\n\n\nIf you want more of a background on what is going on here, please take a look at my series on Gradient Descent where I dive deeper on this. We will be calculating a gradient - which are equivalent to the “Path Value”\n\n\n\n# Here is the function to minimize xw + b\ndef linear1(xb): return xb@weights + bias\n\n# Here is how we will initialize paramaters.  This is just giving me random numbers.\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\nFirst we will make sure our datasets are in a DataLoader. This is convenience class that helps manage our data and get batches.\n\ndl = DataLoader(dset, batch_size=256, shuffle=True)\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\nWe are now going to get the first batch out. We’ll use a batch size of 256, but feel free to change that based on your memory. You can see that we can just simply call first dl, and it creates our shuffled batch for us.\n\nxb,yb = first(dl)\nxb.shape,yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\nLet’s Initialize our paramaters we will need.\n\nweights = init_params((28*28,1))\nbias = init_params(1)\n\n\n\n5.0.2 Calculate the Gradient\nWe now have our batch of x and y, and we have our weights and biases. The next step is to make a prediction. Since our batch size is 256, we see 256x1 tensor.\n\npreds = linear1(xb)\npreds.shape, preds[:5]\n\n(torch.Size([256, 1]),\n tensor([[-14.7916],\n         [  2.7240],\n         [ -6.8821],\n         [ -2.5954],\n         [ -1.3394]], grad_fn=<SliceBackward0>))\n\n\nNow we calculate our loss to see how we did. Probably not well considering all our weights at this point are random.\n\nloss = mnist_loss(preds, yb)\nloss\n\ntensor(0.5753, grad_fn=<MeanBackward0>)\n\n\nLet’s calculate our Gradients\n\nloss.backward()\nweights.grad.shape,weights.grad.mean(),bias.grad\n\n(torch.Size([784, 1]), tensor(-0.0014), tensor([-0.0099]))\n\n\nSince we are going to want to calculate gradients every since step, let’s create a function that we can call that does these three steps above. Let’s put all that in a function. From here on out, we will use this function.\n\n\n\n\n\n\nTip\n\n\n\nIt’s always a good idea to periodically reviewing and trying to simplify re-usable code. I recommend doing following the above approach, make something that works - then simplify. It often wastes a lot of time trying to write things in the most perfect way from the start.\n\n\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\n\ncalc_grad(xb, yb, linear1)\nweights.grad.mean(),bias.grad\n\n(tensor(-0.0028), tensor([-0.0198]))\n\n\n\nweights.grad.zero_()\nbias.grad.zero_();\n\n\n\n5.0.3 Training the Model\nWe are ready to create a function that trains for 1 epoch.\n\n\n\n\n\n\nNote\n\n\n\nEpoch is just a fancy way of saying 1 pass through all our data.\n\n\n\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\nWe want to be able to measure accuracy so we know how well we are doing. It’s hard for us to gauge how well the model is doing from our loss function. We create an accuracy metric to look at accuracy for that batch.\n\n\n\n\n\n\nNote\n\n\n\nA loss function is designed to be good for gradient descent. A Metric is designed to be good for human understanding. This is why they are different sometimes.\n\n\n\ndef batch_accuracy(xb, yb):\n    preds = xb.sigmoid()\n    correct = (preds>0.5) == yb\n    return correct.float().mean()\n\n\nbatch_accuracy(linear1(xb), yb)\n\ntensor(0.4141)\n\n\nLooking at accuracy of our batch is great, but we also want to look at our accuracy for the validation set. This is our way to do that using the accuracy funcion we just defined.\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\n\nvalidate_epoch(linear1)\n\n0.4372\n\n\nAwesome! Let’s throw this all in a loop and see what we get.\n\nparams = weights,bias\nlr = 1.\n\nfor i in range(20):\n    train_epoch(linear1, lr, params)\n    print(f\"Epoch {i} accuracy: {validate_epoch(linear1)}\")\n\nEpoch 0 accuracy: 0.4916\nEpoch 1 accuracy: 0.5477\nEpoch 2 accuracy: 0.5953\nEpoch 3 accuracy: 0.6459\nEpoch 4 accuracy: 0.6781\nEpoch 5 accuracy: 0.7277\nEpoch 6 accuracy: 0.77\nEpoch 7 accuracy: 0.7931\nEpoch 8 accuracy: 0.8118\nEpoch 9 accuracy: 0.8358\nEpoch 10 accuracy: 0.8568\nEpoch 11 accuracy: 0.8711\nEpoch 12 accuracy: 0.8882\nEpoch 13 accuracy: 0.8957\nEpoch 14 accuracy: 0.9001\nEpoch 15 accuracy: 0.9093\nEpoch 16 accuracy: 0.9199\nEpoch 17 accuracy: 0.9286\nEpoch 18 accuracy: 0.9335\nEpoch 19 accuracy: 0.9348\n\n\n\n\n5.0.4 Linear Recap\nLet’s recap really what we did:\n\nMake a prediction\nMeasure how we did\nChange our weights so we do slightly better next time\nPrint out accuracy metrics along the way so we can see how we are doing\n\nThis is a great start, but what we have is a linear model. Now we need to add non-linearities so that we can have a true Neural Network.\n\n\n\n6 ReLu\n\n6.0.1 What is it?\nA ReLu is a common non-linearity in a neural network. A neural network is just alternating linear and nonlinear layers. We defined the Linear layer above, here we will talk about the non-linear ones. So what exactly does the non-linear layer do? It’s actually much simpler than people like to believe. It’s taking a max.\nFor example, I am going to apply a ReLu to a matrix.\n\\(\\begin{bmatrix}-1&1\\\\-1&-1\\\\0&0\\\\0&1\\\\1&-1\\\\1&0\\\\-1&0\\\\-1&-1\\\\1&1\\end{bmatrix}\\) \\(=>\\) \\(\\begin{bmatrix}0&1\\\\0&0\\\\0&0\\\\0&1\\\\1&0\\\\1&0\\\\0&0\\\\0&0\\\\1&1\\end{bmatrix}\\)\nAs you will see I just took max(x,0). Another way of saying that is I replaced any negative values with 0. That’s all a ReLu is.\n\n\n6.0.2 In a Neural Network\nThese ReLus go between our linear layers. Here’s what a simple Neural Net looks like.\n\n# initialize random weights.  \nw1 = init_params((28*28,30))\nb1 = init_params(30)\nw2 = init_params((30,1))\nb2 = init_params(1)\n\n\n# Here's a simple Neural Network.  \n# This can have more layers by duplicating the patten seen below, this is just the simplest model.\ndef simple_net(xb): \n    res = xb@w1 + b1 #Linear\n    res = res.max(tensor(0.0)) #ReLu\n    res = res@w2 + b2 #Linear\n    return res\n\n\n\n\n7 Train the Full Neural Network\nWe have our new model with new weights. It’s more than just the linear model, so how do we use gradient descent? We now have 4 weights (w1,w2,b1,b2).\nTurns our it’s exactly what we already did. Let’s add the new parameters and change out the linear model with the simple_net we just defined. We end up with a pretty decent accuracy!\n\nw1 = init_params((28*28,30))\nb1 = init_params(30)\nw2 = init_params((30,1))\nb2 = init_params(1)\n\n\nparams = w1,b1,w2,b2\nlr = 1\n\nfor i in range(20):\n    train_epoch(simple_net, lr, params)\n    print(f\"Epoch {i} accuracy: {validate_epoch(simple_net)}\")\n\nEpoch 0 accuracy: 0.5979\nEpoch 1 accuracy: 0.7431\nEpoch 2 accuracy: 0.8324\nEpoch 3 accuracy: 0.8753\nEpoch 4 accuracy: 0.9045\nEpoch 5 accuracy: 0.9288\nEpoch 6 accuracy: 0.9322\nEpoch 7 accuracy: 0.9387\nEpoch 8 accuracy: 0.9406\nEpoch 9 accuracy: 0.941\nEpoch 10 accuracy: 0.9436\nEpoch 11 accuracy: 0.948\nEpoch 12 accuracy: 0.9511\nEpoch 13 accuracy: 0.9501\nEpoch 14 accuracy: 0.9532\nEpoch 15 accuracy: 0.9524\nEpoch 16 accuracy: 0.9581\nEpoch 17 accuracy: 0.9576\nEpoch 18 accuracy: 0.9607\nEpoch 19 accuracy: 0.9602\n\n\n\n\n8 Recap of Tensor Shapes\nUnderstanding the shapes of these tensors and how the network works is crucial. Here’s the network we built. You can see how each layer can fit into the next layer.\n\nx.shape,w1.shape,b1.shape,w2.shape,b2.shape\n\n(torch.Size([709, 784]),\n torch.Size([784, 30]),\n torch.Size([30]),\n torch.Size([30, 1]),\n torch.Size([1]))\n\n\n\n\n\n9 What’s Next?\nThis is a Neural Network. Now we can do tweaks to enhance performance. I will talk about those in future posts, but here’s a few concepts.\n\nInstead of a Linear Layer, A ReLu, then a linear layer - Can we add more layers to have a deeper net?\nWhat if we average some of the pixels in our image together before dong matrix multiplication (ie a convolutions)?\nCan we randomly ignore pixels to prevent overfitting (ie dropout)?\n\nThere are many advanced variations of Neural Networks, but the concepts are typically along the lines of above. If you boil them down to what they are really doing - they are pretty simple concepts."
  },
  {
    "objectID": "posts/NeuralNetworks/RNNFoundations.html",
    "href": "posts/NeuralNetworks/RNNFoundations.html",
    "title": "NLP Recurrent NN Foundations",
    "section": "",
    "text": "from fastai.text.all import *\n\n\n1 Credit Where Credit is Due\nThe concept and techniques covered in this post are covered in much greater detail in Jeremy Howard and Sylvain Gugger’s book. If you like this post, you should buy the book as you’ll probably like it even more!\n\n\n2 Data Setup\n\n2.0.1 Get the Data\n\npath = untar_data(URLs.HUMAN_NUMBERS)\nlines = L()\nwith open(path/\"train.txt\") as f: lines += L(*f.readlines())\nwith open(path/\"valid.txt\") as f: lines += L(*f.readlines())\nlines\n\n(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]\n\n\n\n\n2.0.2 Tokenization\nWhat is Tokenization?\nTokenization is about getting ‘tokens’ of language that have meaning. A word could be a token as it has meaning. A piece of punctuation could be a token as it has meaning. If a work is in all capital letters that could be a token. A portion of a word could be a token (ie dis) because a word beginning with dis has meaning. There are many many ways to tokenize, for this post I will use the most simple approach. That is, I will split based on spaces to make each word a token.\n\ntxt = ' . '.join([l.strip() for l in lines])\n\n\ntokens = L(*txt.split(' ')); tokens\n\n(#63095) ['one','.','two','.','three','.','four','.','five','.'...]\n\n\n\n\n2.0.3 Numericalization\nNow that things are split into tokens, we need to start thinking about how to feed it to a Neural Network. Neural Networks rely on multiplication and addition, and we can’t do that with a word. Somehow we need to convert these tokens to numbers. That is what Numericalization is all about. We will do this in a few steps:\n\nGet a unique list of all tokens (v)\nAssign a number to each of token (vocab)\nReplace tokens with numbers (nums)\n\n\n# Get a unique list of all tokens (v)\nv = tokens.unique()\n\n# Assign a number to each of token (vocab)\nvocab = {v:i for i,v in enumerate(v)};\n\n# We can lookup the number associated with a token like this\nvocab['fifty']\n\n23\n\n\n\n# Replace tokens with numbers (nums)\nnums = L(vocab[tok] for tok in tokens); nums\n\n(#63095) [0,1,2,1,3,1,4,1,5,1...]\n\n\n\n\n2.0.4 Sequence Definition\nNow that we have tokens in the form of numbers, we need to create out inputs and outputs to the model. For this we need to organize our data into dependent and independent variables. Let’s use the preceding 3 words to predict the next word. Below, we see the same thing in 2 ways - one with tokens and one with numbers. These are the same thing, just translating the tokens to numbers using the vocab above.\n\n\n\n\n\n\nNote\n\n\n\nSequence Length (sl) will be 3, because we are using a sequence of 3 words to predict the next word.\n\n\n\nsl = 3\n\n# For example, we will use the tokens 'one','.', and 'two' to predict '.'\nL((tokens[i:i+sl], tokens[i+sl]) for i in range(0,len(tokens)-sl-1,sl))\n\n(#21031) [((#3) ['one','.','two'], '.'),((#3) ['.','three','.'], 'four'),((#3) ['four','.','five'], '.'),((#3) ['.','six','.'], 'seven'),((#3) ['seven','.','eight'], '.'),((#3) ['.','nine','.'], 'ten'),((#3) ['ten','.','eleven'], '.'),((#3) ['.','twelve','.'], 'thirteen'),((#3) ['thirteen','.','fourteen'], '.'),((#3) ['.','fifteen','.'], 'sixteen')...]\n\n\n\nseqs = L((tensor(nums[i:i+sl]), nums[i+sl]) for i in range(0,len(nums)-sl-1,sl)); seqs\n\n(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]\n\n\n\n\n2.0.5 Dataloader\nNow we need to create our dataloader. The dataloader is just packaging it into batches, and not doing any transformations or changes to the data. What we saw above is what will be given to the model.\n\nbs = 128\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(seqs[:cut],seqs[cut:],bs=bs, shuffle=False)\n\n\ndls2 = DataLoader(seqs[:cut],bs=bs, shuffle=False)\ndls3 = DataLoader(seqs[cut:],bs=bs, shuffle=False)\n\ndls4 = DataLoaders(dls3,dls3)\n\n\n\n\n3 Language Model\n\n3.0.1 Naive Model\nFirst, let’s figure out a baseline for what is the best ‘non-stupid’ model we can come up with. If a model can’t beat this score, then it’s not worth anything.\nThe approach we will take will be to predict the most common token every time. If we do that we get about a 15% accuracy.\n\nn,counts = 0,torch.zeros(len(vocab))\nfor x,y in dls.valid:\n    n += y.shape[0]\n    for i in range_of(vocab): counts[i] += (y==i).long().sum()\nidx = torch.argmax(counts)\nidx, v[idx.item()], counts[idx].item()/n\n\n(tensor(29), 'thousand', 0.15165200855716662)\n\n\n\n\n3.0.2 RNN Number 1\n\n3.0.2.0.1 Code\nWe are going to make the simplest RNN we can. Here’s a quick explanation of the code below.\nfor i in range(sl): Because we are feeding in a number of tokens based on our sequence length, sl, which was defined as 3. We will have 3 steps, 1 per token.\nh = h + self.i_h(x[:,i]) For each input token we will run our input to hidden function. We are indexing to grab the column in our embedding matrix that corresponds with the token, and adding that. All this is doing is adding the embedding for the particular token.\nh = F.relu(self.h_h(h)) We then run our hidden to hidden function (h_h), which is a linear layer (y = wx + b). We do a ReLu of that, which is just replacing any negative values with 0.\nreturn self.h_o(h) We then run our hidden to output function (h_o), which is another linear layer, but it is outputing the prediction of which word is next. Naturally, this is the size of our vocabulary.\nWrap all that in a class and it looks like the below:\n\nclass LM1(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.h_h = nn.Linear(n_hidden, n_hidden)\n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        \n    def forward(self, x):\n        h = 0\n        for i in range(sl):\n            h = h + self.i_h(x[:,i])\n            h = F.relu(self.h_h(h))\n        return self.h_o(h)\n\nNow we can run it below and see that we get almost 50% accuracy before we overfit, which is great considering the most common token only appears 15% of the time.\n\nlearn = Learner(dls, LM1(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      2.505863\n      2.136583\n      0.458046\n      00:00\n    \n    \n      1\n      1.602575\n      1.847033\n      0.480865\n      00:00\n    \n    \n      2\n      1.503249\n      1.727588\n      0.492275\n      00:00\n    \n    \n      3\n      1.436492\n      1.771485\n      0.410506\n      00:00\n    \n  \n\n\n\n\n\n3.0.2.0.2 Tensors\nSo what is it REALLY doing? To understand that, I find it helpful to think about the matrix/tensor sizes at each step.\nEmbeddings\nLet’s start with our input_hidden. Our Embedding matrix is has 64 weights (n_hidden) for each token in our vocabulary. So that looks like this:\n\\(\\underbrace{ \\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}_{\\displaystyle 64-weights} \\left.\\vphantom{\\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}\\right\\}30-tokens\\)\nNow all the embedding layer does is get the correct columns. So for the first word in the sequence we get the index, then look it up in the embedding matrix. That 1 index location turns into the 64 weights.\n\\(\\underbrace{ \\begin{bmatrix} \\cdots \\\\ \\cdots \\\\ \\cdots \\\\ \\cdots \\\\ \\cdots \\\\ \\cdots \\\\ \\end{bmatrix}}_{\\displaystyle token-idx} \\left.\\vphantom{\\begin{bmatrix} \\cdots \\\\ \\cdots \\\\ \\cdots \\\\ \\cdots \\\\ \\cdots \\\\ \\cdots \\\\ \\end{bmatrix}}\\right\\}128-bs\\) \\(==\\) lookup in embedding matrix \\(==>\\) \\(\\underbrace{ \\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}_{\\displaystyle 64} \\left.\\vphantom{\\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}\\right\\}128\\)\nHidden Linear Layer\nNext, we have out hidden_hidden. We have our 128x64 matrix from our embedding lookup and we need to do a linear layer.\n\\(\\underbrace{ \\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}_{\\displaystyle 64-weights} \\left.\\vphantom{\\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}\\right\\}128-bs\\) \\(\\underbrace{ \\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}_{\\displaystyle 64} \\left.\\vphantom{\\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}\\right\\}64\\) \\(+\\) \\(\\underbrace{ \\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}_{\\displaystyle 64-bias} \\left.\\vphantom{\\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}\\right\\}1\\) \\(=\\) \\(\\underbrace{ \\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}_{\\displaystyle 64-weights} \\left.\\vphantom{\\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}\\right\\}128-bs\\) ===ReLu - Replace all negatives with 0 ===> \\(\\underbrace{ \\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}_{\\displaystyle 64-weights} \\left.\\vphantom{\\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}\\right\\}128-bs\\)\nAnd we do the above for however long our sequence is, in our case 3. So for each token we do the above. We start with 0 on the first loop, and each subsequent loop through we add onto that.\nOuput Linear Layer\nWe ended with a 128x64 matrix, which isn’t exactly what we want. We have 30 words, so we want to know which one of the 30 is most likely. Specifically for each of the 128 items in our batch, we want 30 scores (1 for each word in our vocab). So we do a similar step as our hidden linear layer, but adjust the number of weights so we end up with the matrix of the appropriate size.\n\\(\\underbrace{ \\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}_{\\displaystyle 64-weights} \\left.\\vphantom{\\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}\\right\\}128-bs\\) \\(\\underbrace{ \\begin{bmatrix} \\cdots & \\cdots\\\\ \\cdots & \\cdots\\\\ \\cdots & \\cdots\\\\ \\cdots & \\cdots\\\\ \\end{bmatrix}}_{\\displaystyle 30} \\left.\\vphantom{\\begin{bmatrix} \\cdots & \\cdots\\\\ \\cdots & \\cdots\\\\ \\cdots & \\cdots\\\\ \\cdots & \\cdots\\\\ \\end{bmatrix}}\\right\\}64\\) \\(+\\) \\(\\underbrace{ \\begin{bmatrix} \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}_{\\displaystyle 30-bias} \\left.\\vphantom{\\begin{bmatrix} \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}\\right\\}1\\) \\(=\\) \\(\\underbrace{ \\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}_{\\displaystyle 30-preds} \\left.\\vphantom{\\begin{bmatrix} \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\cdots & \\cdots & \\cdots & \\cdots\\\\ \\end{bmatrix}}\\right\\}128-bs\\)\n\n\n\n3.0.3 RNN Number 2\nNow that we have a simple model, how do we improve it? There are many steps that need to be taken to get to a cutting edge model. We’ll do one improvement, then leave the rest for future blog posts.\nOne thing that was a bit odd is in the training loop we reset back to 0 every time. What I mean by that, is we would loop through each of the 3 tokens, output our predictions for those, update the weights, then reset back for a new set. This isn’t really how language works. Language has a pattern and a sequence to it. The further back you go the less important, but even things said a couple minutes ago could be important. Could you imagine holding a conversation if you could only remember and respond based on the last 3 words?\nSo let’s fix this problem. We will move our h=0 up to the initialization of the class, and never reset back to 0. Instead, we will continuously keep adding to it. We will only update the last batch of weights (as if we updated all of them by the 1000th one we would be updating far to many weights to compute). We call this “detaching” it. Ultimately we are left with the same thing, but if has a memory of previous sequences beyond the one we are processing! Let’s see if it makes things better.\n\nclass LM2(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.h_h = nn.Linear(n_hidden, n_hidden)\n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0\n\n    def forward(self, x):\n        for i in range(3):\n            self.h = self.h + self.i_h(x[:,i])\n            self.h = F.relu(self.h_h(self.h))\n        out = self.h_o(self.h)\n        self.h = self.h.detach()\n        return out\n\nTo do this we need to take care that our data is in the appropriate order, so let’s do a few tranformations to make that work.\n\nm = len(seqs)//bs\nm,bs,len(seqs)\ndef group_chunks(ds, bs):\n    m = len(ds) // bs\n    new_ds = L()\n    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n    return new_ds\n\n\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(\n    group_chunks(seqs[:cut], bs), \n    group_chunks(seqs[cut:], bs), \n    bs=bs, drop_last=True, shuffle=False)\n\n\nlearn = Learner(dls, LM2(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy)\n\n\nlearn.fit_one_cycle(10, 3e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      2.342321\n      1.897249\n      0.481689\n      00:00\n    \n    \n      1\n      1.453624\n      1.713581\n      0.449707\n      00:00\n    \n    \n      2\n      1.154838\n      1.680148\n      0.519775\n      00:00\n    \n    \n      3\n      1.042766\n      1.566625\n      0.517822\n      00:00\n    \n    \n      4\n      0.969852\n      1.633654\n      0.542480\n      00:00\n    \n    \n      5\n      0.937066\n      1.581196\n      0.559570\n      00:00\n    \n    \n      6\n      0.882712\n      1.660810\n      0.588379\n      00:00\n    \n    \n      7\n      0.844926\n      1.595611\n      0.597656\n      00:00\n    \n    \n      8\n      0.808309\n      1.613600\n      0.605225\n      00:00\n    \n    \n      9\n      0.797358\n      1.621867\n      0.605713\n      00:00\n    \n  \n\n\n\nAnd we are up from about 50% accuracy to about 60%!\n\n\n\n4 Conclusion\nHopefully from this post you gained an understanding of the fundamental concepts behind NLP using Neural Networks. While this isn’t cutting edge, the fundamental principles must be understood if you want to gain an intuition about what types of things might work."
  },
  {
    "objectID": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html",
    "href": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html",
    "title": "Pseudo Labeling for Data Cleaning",
    "section": "",
    "text": "Goal: The goal of this article is to provide an understanding of what pseudo labeling is, why you might use it, and how you would go about using it.\nWhat’s Included in this post: The information needed to get started on pseudo labeling on your project."
  },
  {
    "objectID": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html#data-cleaning-noise-reduction",
    "href": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html#data-cleaning-noise-reduction",
    "title": "Pseudo Labeling for Data Cleaning",
    "section": "3.1 Data Cleaning & Noise Reduction",
    "text": "3.1 Data Cleaning & Noise Reduction\nImagine you have a dataset and all the samples have been hand labeled. You know they can’t all be labeled appropriately because it was manual labeling and you want to improve your labels. You have a few options:\n\nGo through every datapoint again and manually verify them all\nSomehow identify the ones that are likely to be wrong and put more focus on those ones.\n\nPseudo labeling can help with option number 2. By creating a prediction on a datapoint, you can see which labels the model disagrees with. Even better, you can look at the confidence that model has in the prediction. So by looking at datapoints that the model is confident are wrong, you can really narrow your focus on your problem areas quickly.\nYou then can fix it in 2 ways:\n\nReplace your labels with the predicted labels following some threshold (ie score of .9 or higher).\nManually re-classify these labels if you have the time and domain expertise to do these."
  },
  {
    "objectID": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html#data-augmentation",
    "href": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html#data-augmentation",
    "title": "Pseudo Labeling for Data Cleaning",
    "section": "3.2 Data Augmentation",
    "text": "3.2 Data Augmentation\nThis approach can also be used on unlabeled data. Rather than trying to replace bad labels, this approach focuses on creating labels for unlabeled data. This can be used on a kaggle test set for example. The reason this can work is because you are teaching the model the structure of the data. Even if not all labels are correct, a lot can still be learning.\nThink about if you were to learn what a new type of object looks like. Maybe a type of furniture you’d never heard of before. Doing a google image search for that name and looking at all the results is really helpful, even if not all of the images that are shown are all correct.\n\n\n\n\n\n\nNote\n\n\n\nI found out about this Approach from Jeremy Howard. He speaks on this in an old version of the fastai course. Here is a summary that includes a link to the portion of the old course he discusses this approach in. I would highly recommend checking out the latest course as well here."
  },
  {
    "objectID": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html#imports",
    "href": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html#imports",
    "title": "Pseudo Labeling for Data Cleaning",
    "section": "4.1 Imports",
    "text": "4.1 Imports\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.MNIST,force_download=True)\nfrom sklearn.model_selection import StratifiedKFold\nfrom numpy.random import default_rng"
  },
  {
    "objectID": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html#introduce-noise-to-data",
    "href": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html#introduce-noise-to-data",
    "title": "Pseudo Labeling for Data Cleaning",
    "section": "4.2 Introduce Noise to Data",
    "text": "4.2 Introduce Noise to Data\n\nx = get_image_files(path)\ny = L(parent_label(o) for o in get_image_files(path))\n\nGet 10% of the indexes to randomly change\n\nn = len(x)\nrng = default_rng()\n\nnoise_idxs = rng.choice(n, size=round(n*0.1), replace=False)\nlen(noise_idxs),noise_idxs[:5]\n\n(7000, array([17419, 48844, 61590, 49810, 26348]))\n\n\nRandomly change these so we have some bad labels\n\nfor i in range(0,len(noise_idxs)):\n    old_path = str(x[noise_idxs[i]])\n                     \n    if 'training' in old_path:\n        new_path = str(x[noise_idxs[i]])[:49]+f'{np.random.randint(0,10)}'+str(x[noise_idxs[i]])[50:]\n    elif 'testing' in old_path:\n        new_path = str(x[noise_idxs[i]])[:48]+f'{np.random.randint(0,10)}'+str(x[noise_idxs[i]])[49:]\n        \n    os.system(f'mv {old_path} {new_path}')"
  },
  {
    "objectID": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html#look-at-data",
    "href": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html#look-at-data",
    "title": "Pseudo Labeling for Data Cleaning",
    "section": "4.3 Look at Data",
    "text": "4.3 Look at Data\nSome of our labels are now labeled, but we don’t know which ones. We could look at every image to find them, but that would take a ton of time. Let’s try to find the mislabeled images and correct them using a pseudo labeling approach.\n\nmnist = DataBlock(blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), \n                  get_items=get_image_files, \n                  splitter=RandomSplitter(),\n                  get_y=parent_label)\ndls = mnist.dataloaders(path,bs=16)\ndls.show_batch(max_n=36,figsize=(6,6))"
  },
  {
    "objectID": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html#create-crossfold-train-and-predict",
    "href": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html#create-crossfold-train-and-predict",
    "title": "Pseudo Labeling for Data Cleaning",
    "section": "4.4 Create Crossfold, Train, and Predict",
    "text": "4.4 Create Crossfold, Train, and Predict\nThis step is much simpler if you are generating labels for the test set, as you would train your model as normal and predict as normal. The reason I am doing cross folds is to get predicted labels on the training set.\n\n\n\n\n\n\nNote\n\n\n\nI am doing this with a 2 fold, but you may want to use a 5-fold or more folds.\n\n\nThis cross-fold code was mostly supplied by Zach Mueller, with minor modifications by me for this dataset and tutorial. There is also a tutorial he wrote with more details here\n\nskf = StratifiedKFold(n_splits=2, shuffle=True, random_state=1)\nsplits, preds, targs, preds_c, items = [],[],[],[], []\n\nfor _, val_idx in skf.split(x,y):\n    splitter = IndexSplitter(val_idx)\n    splits.append(val_idx)\n\n    mnist = DataBlock(blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), \n                      get_items=get_image_files, \n                      splitter=splitter,\n                      get_y=parent_label)\n\n    dls = mnist.dataloaders(path,bs=16)\n    learn = cnn_learner(dls,resnet18,metrics=accuracy)\n    learn.fine_tune(2,reset_opt=True)\n    \n    # store predictions\n    p, t, c = learn.get_preds(ds_idx=1,with_decoded=True)\n    preds.append(p); targs.append(t); preds_c.append(c); items.append(dls.valid.items)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.310182\n      1.102497\n      0.722114\n      01:01\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.696935\n      0.617736\n      0.886114\n      01:20\n    \n    \n      1\n      0.631840\n      0.570121\n      0.895343\n      01:21\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.379348\n      1.098212\n      0.715343\n      01:04\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.725246\n      0.607686\n      0.888714\n      01:21\n    \n    \n      1\n      0.625833\n      0.557313\n      0.897943\n      01:21"
  },
  {
    "objectID": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html#look-at-predictions",
    "href": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html#look-at-predictions",
    "title": "Pseudo Labeling for Data Cleaning",
    "section": "4.5 Look at Predictions",
    "text": "4.5 Look at Predictions\nLets throw it all in a dataframe so we can look at what we have a little easier. First, let’s break out our different pieces of information.\n\nitems_flat = L(itertools.chain.from_iterable(items))\nimgs = L(o for o in items_flat)\ny_true = L(int(parent_label(o)) for o in items_flat) # Labels from dataset\ny_targ = L(int(o) for o in torch.cat(targs)) # Labels from out predictions\ny_pred = L(int(o) for o in torch.cat(preds_c)) # predicted labels or \"pseudo labels\"\np_max = torch.cat(preds).max(dim=1)[0] # max model score for row\n\nWe can double check we are matching things up correctly by checking that the labels line up from the predictions and the original data. Throwing some simple assert statements in is nice because it takes no time and it will let you know if you screw something up later as you are tinkering with things.\n\nassert (y_true == y_targ) # test we matched these up correct\n\nPut it in a dataframe and see what we have.\n\nres = pd.DataFrame({'imgs':imgs,'y_true':y_true,'y_pred':y_pred,'p_max':p_max})\nres.head(5)\n\n\n\n\n\n  \n    \n      \n      imgs\n      y_true\n      y_pred\n      p_max\n    \n  \n  \n    \n      0\n      /home/isaacflath/.fastai/data/mnist_png/testing/1/8418.png\n      1\n      1\n      0.864995\n    \n    \n      1\n      /home/isaacflath/.fastai/data/mnist_png/testing/1/2888.png\n      1\n      7\n      0.900654\n    \n    \n      2\n      /home/isaacflath/.fastai/data/mnist_png/testing/1/6482.png\n      1\n      1\n      0.906335\n    \n    \n      3\n      /home/isaacflath/.fastai/data/mnist_png/testing/1/7582.png\n      1\n      1\n      0.902999\n    \n    \n      4\n      /home/isaacflath/.fastai/data/mnist_png/testing/1/4232.png\n      1\n      1\n      0.925955\n    \n  \n\n\n\n\nPerfect so lets get a list of our images our model got ‘wrong’ and grab some random ones out of the top 5000 the model was most confident about. The theory is that many of these may be mislabeled, and we can reclassify them either using the predicted ‘pseudo’ labels, or with manual classification.\n\nimgs = res[res.y_true != res.y_pred].sort_values('p_max',ascending=False)[:5000].sample(frac=1)\n\nAnd then we plot them and see our predicted labels of these are WAY better than the actual labels. A great way to identify some bad labels.\n\n%matplotlib inline\nfig, ax = plt.subplots(5,5,figsize=(10,10))\n\nfor row in range(0,5):\n    for col in range(0,5):\n        img_path1 = imgs.iloc[row*4+col,0]\n        img_path1 = np.array(Image.open(img_path1))\n        ax[row,col].imshow(img_path1,cmap='Greys')\n        ax[row,col].set_title(f'Label:{parent_label(imgs.iloc[row*4+col,0])} | Pred:{imgs.iloc[row*4+col,2]}')\n        ax[row,col].get_xaxis().set_visible(False)\n        ax[row,col].get_yaxis().set_visible(False)"
  },
  {
    "objectID": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html#what-next",
    "href": "posts/NeuralNetworks/PseudoLabelingDataCleaning.html#what-next",
    "title": "Pseudo Labeling for Data Cleaning",
    "section": "4.6 What Next?",
    "text": "4.6 What Next?\nNow that we have found mislabeled data, we can fix them. We see that in this problem in the top 5000 most confident wrong answers our predicted labels are much better.\nSo the next step would be to replace the labels with our predicted labels, then train our model on the newly cleaned labels!\n\nNote: This same approach can be used on unlabeled data to get data points the model is confident in to expand the training data."
  },
  {
    "objectID": "posts/Tabular/PartialDependencePlotting.html",
    "href": "posts/Tabular/PartialDependencePlotting.html",
    "title": "Partial Dependence Plotting",
    "section": "",
    "text": "1 What is Partial Dependence Plotting?\nTraditional analysis looks at historical data and describes it. The data is fixed and you are looking to make a prediction based on the data. The person is responsible for finding and understanding the interactions. This gets exponentially more difficult in higher dimensional problems.\nPartial Dependence Plotting looks at a prediction, and modifies the data to see how it effects the prediction. The model is fixed and you are looking to understand interactions by modifying the data. The model is responsible for finding the interactions and the person just focuses on understanding the interactions the model found.\nIn this way, you can think of Descriptive Analysis as a method of analysis that focuses on understanding the data, where Partial Dependence Plotting is a method of analysis that focuses on understanding the predictions. They can accomplish similar goals but approach it differently.\n\n\n2 Why use it?\nOne of the biggest challenges of statistics is that is requires a person to make a series of assumptions. Whether you are doing p-values, or correlations, or any other test you are typically making some assumption. Maybe you have to assume the data follows a particular distribution, maybe you have to assume that your variables are independent, or maybe you assume your variables and linearly dependent. Whatever the case may be, you typically have to make some assumption - and if you make the wrong assumptions you can get incorrect findings. It’s for this reason that P-Values are not recommended for use in validating results by the majority of the top statisticians (including the American Statistical Association), despite the fact that most business analysts use them heavily in their analysis and decision making.\nThis leads me to the first advantage of the partial dependence plotting approach, which is a big differentiator between statistics and data science. With partial dependence plotting you are testing through experimentation rather than through descriptive statistics and statistical testing. For example a Neural Network can approximate any function, whether that is linear, exponential, logistic, or any other shape with any number of interactions. So I can use that, measure the accuracy, and understand the uncertainty of my analysis with no assumptions about distribution, co-linearity, or type of function.\nA second advantage is that if you have sufficient data for your analysis, but it’s not all the right type due to some data imbalance you can leverage all the data for the analysis. For example, if you are looking at customer churn you likely have many more accounts that did not churn than accounts that did churn. With descriptive statistics you will look at the few churned accounts and see what they have in common. With a model driven approach, you can look at accounts that did not churn and see what changes you could make to those accounts that makes them more likely to churn.\nA third advantage is that the output you are seeing is the prediction. The prediction of the future is often what is of value, so if the goal is to make some decision based on what is likely to happen in the future keeping that in the forefront is ideal.\n\n\n3 Examples\nEnough jibber-jabber. Let’s look at some examples. We’ll start simple and get a little bit more complicated over time.\n\n3.0.1 Logistic Regression\nLet’s create a model on the classic iris dataset and fit a logistic regression to it. When we do this, we see we have a mean accuracy score of about 97%.\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nX, y = load_iris(return_X_y=True)\nclf = LogisticRegression(max_iter=500).fit(X, y)\nclf.score(X, y)\n\n0.9733333333333334\n\n\nThat’s pretty good, but we don’t really know what the important variables are. Let’s experiment a bit.\nfor our first 2 rows of data we can see the model predicts 0, which is the correct answer. What changes could we make to the data to make it predict something else?\n\nX[0:2, :]\n\narray([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2]])\n\n\n\nclf.predict(X[0:2, :])\n\narray([0, 0])\n\n\nLet’s test adding to each of the columns and see if any change the predictions. Our original prediction is [0,0], so anything different tells us something\n\nfor i in range(0,X_temp.shape[1]):\n  X_temp = X[0:2, :].copy()\n  X_temp[:,i] = X_temp[:,i]+20\n  print('New Values')\n  print(X_temp)\n  print('Prediction')\n  print(clf.predict(X_temp))\n\nNew Values\n[[25.1  3.5  1.4  0.2]\n [24.9  3.   1.4  0.2]]\nPrediction\n[1 1]\nNew Values\n[[ 5.1 23.5  1.4  0.2]\n [ 4.9 23.   1.4  0.2]]\nPrediction\n[0 0]\nNew Values\n[[ 5.1  3.5 21.4  0.2]\n [ 4.9  3.  21.4  0.2]]\nPrediction\n[2 2]\nNew Values\n[[ 5.1  3.5  1.4 20.2]\n [ 4.9  3.   1.4 20.2]]\nPrediction\n[2 2]\n\n\nThat’s interesting. We can see that adding to the 1st, 3rd, and 4th variable made our model make a different prediction. Since we know our model was about 97% accurate, we know this is meaningful. It is picking up on some trend in the data using each of these columns. Lets hone in on column 1 to see if we can understand this more.\n\nfor i in range(-10,10):\n  X_temp = X[0:2, :].copy()\n  X_temp[:,0] = X_temp[:,0]+i\n  if (clf.predict(X_temp) == np.array([0,0])).all():\n    continue\n  print('Prediction for adding ' +str(i))\n  print(clf.predict(X_temp))\n\nPrediction for adding 4\n[0 1]\nPrediction for adding 5\n[1 1]\nPrediction for adding 6\n[1 1]\nPrediction for adding 7\n[1 1]\nPrediction for adding 8\n[1 1]\nPrediction for adding 9\n[1 1]\n\n\nGreat, here we see that subtracting from that value does not change the prediction, but once we start adding 4 - 5 to it changes. Our original value for these rows were 5.1 and 4.9, so it seems that when we get to the 9 - 10 range for that value it becomes more likely that it is a different class all other things equal.\nThis is great insight, and throughout this process we made no assumptions of co-linearity, distribution, or anything else. we just manipulated the data to see the change in predictions of the mode.\n\n3.0.1.0.1 Neural Network\nLet’s try a slightly more difficult problem and use a Neural Net. I could have used an XGBoost or a random forest, or any other model choice.\nThe goal of this dataset is to predict whether the person makes more or less than 50K salary. This could be useful for marketing reasons if you are trying to decide what products to market to whom. Let’s first look at the data.\n\nfrom fastai.tabular.all import *\npath = untar_data('ADULT_SAMPLE')\ndf = pd.read_csv(path/'adult.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      age\n      workclass\n      fnlwgt\n      education\n      education-num\n      marital-status\n      occupation\n      relationship\n      race\n      sex\n      capital-gain\n      capital-loss\n      hours-per-week\n      native-country\n      salary\n    \n  \n  \n    \n      0\n      49\n      Private\n      101320\n      Assoc-acdm\n      12.0\n      Married-civ-spouse\n      NaN\n      Wife\n      White\n      Female\n      0\n      1902\n      40\n      United-States\n      >=50k\n    \n    \n      1\n      44\n      Private\n      236746\n      Masters\n      14.0\n      Divorced\n      Exec-managerial\n      Not-in-family\n      White\n      Male\n      10520\n      0\n      45\n      United-States\n      >=50k\n    \n    \n      2\n      38\n      Private\n      96185\n      HS-grad\n      NaN\n      Divorced\n      NaN\n      Unmarried\n      Black\n      Female\n      0\n      0\n      32\n      United-States\n      <50k\n    \n    \n      3\n      38\n      Self-emp-inc\n      112847\n      Prof-school\n      15.0\n      Married-civ-spouse\n      Prof-specialty\n      Husband\n      Asian-Pac-Islander\n      Male\n      0\n      0\n      40\n      United-States\n      >=50k\n    \n    \n      4\n      42\n      Self-emp-not-inc\n      82297\n      7th-8th\n      NaN\n      Married-civ-spouse\n      Other-service\n      Wife\n      Black\n      Female\n      0\n      0\n      50\n      United-States\n      <50k\n    \n  \n\n\n\n\nWe can see we’ve got a lot of difference variables or both categorical and continuous to look at. Lets format this for a Neural Network and fit the model.\n\nto = TabularPandas(df, procs=[Categorify, FillMissing,Normalize],\n                   cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race'],\n                   cont_names = ['age', 'fnlwgt', 'education-num'],\n                   y_names='salary',\n                   splits=RandomSplitter(valid_pct=0.2)(range_of(df)))\ndls = to.dataloaders(bs=64)\n\n\nlearn = tabular_learner(dls, metrics=accuracy, wd=0.01)\nprint(to.valid.xs.shape)\nlearn.fit_one_cycle(3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.363723\n      0.374584\n      0.823710\n      00:04\n    \n    \n      1\n      0.350536\n      0.357585\n      0.832002\n      00:04\n    \n    \n      2\n      0.342484\n      0.354669\n      0.831081\n      00:04\n    \n  \n\n\n\n\nto.y\n\n21805    0\n14537    0\n1399     0\n8107     0\n16255    0\n        ..\n3603     1\n15429    0\n3551     0\n1880     0\n30442    0\nName: salary, Length: 32561, dtype: int8\n\n\n\ndf.iloc[0]\n\nage                                49\nworkclass                     Private\nfnlwgt                         101320\neducation                  Assoc-acdm\neducation-num                      12\nmarital-status     Married-civ-spouse\noccupation                        NaN\nrelationship                     Wife\nrace                            White\nsex                            Female\ncapital-gain                        0\ncapital-loss                     1902\nhours-per-week                     40\nnative-country          United-States\nsalary                          >=50k\nName: 0, dtype: object\n\n\nPerfect - we have pretty good accuracy and our validation set has over 6000 data points. Let’s look at some features to see if we can understand what impacts our model’s prediction of the individuals salary.\n0 = <50K, so it correctly predicts the first row is someone the makes more than 50k. Let’s see at what point the prediction switches if I reduce hours worked.\n\nrow, clas, probs = learn.predict(df.iloc[0])\nclas\n\n\n\n\ntensor(1)\n\n\n\nfor i in range(-40,0):\n  X_temp = df.iloc[0].copy()\n  X_temp['hours-per-week'] = X_temp['hours-per-week']+i\n  row, clas, probs = learn.predict(X_temp)\n  if clas == tensor(1):\n    continue\n  print('Prediction for adding ' +str(i))\n  print(clas)\n  \nfrom IPython.display import clear_output\nclear_output()\n\nInterestingly, the model isn’t convinced even if I change hours works to 0, maybe it thinks the money is passive income or comes from the husband. Let’s see if we can figure that out.\n\nfor i in df.relationship.unique():\n  X_temp = df.iloc[0].copy()\n  X_temp['relationship'] = i\n  row, clas, probs = learn.predict(X_temp)\n  print('Prediction for adding ' +str(i))\n  print(clas)\n\n\n\n\nPrediction for adding  Wife\ntensor(1)\n\n\n\n\n\nPrediction for adding  Not-in-family\ntensor(0)\n\n\n\n\n\nPrediction for adding  Unmarried\ntensor(0)\n\n\n\n\n\nPrediction for adding  Husband\ntensor(0)\n\n\n\n\n\nPrediction for adding  Own-child\ntensor(0)\n\n\n\n\n\nPrediction for adding  Other-relative\ntensor(0)\n\n\nIf we change the relationship to anything else, the model starts thinking she makes less than 50K instead of more. We can continue to experiment with individual rows, or when ready try some larger experiments across the larger dataset.\n\ndf[['salary','age']][:1000].groupby('salary').count()\n\n\n\n\n\n  \n    \n      \n      age\n    \n    \n      salary\n      \n    \n  \n  \n    \n      <50k\n      754\n    \n    \n      >=50k\n      246\n    \n  \n\n\n\n\n\nresults = list()\nfor i in df.relationship.unique():\n  g50k, l50k = (0,0)\n  df_temp = df[:1000].copy()\n  df_temp['relationship'] = i\n  for rownum in range(0,len(df_temp)):\n    if learn.predict(df_temp.iloc[rownum])[1] == tensor(1):\n      g50k += 1\n    else:\n      l50k += 1\n    clear_output()\n  results.append((i,g50k,l50k))\nresults\n\n[(' Wife', 232, 768),\n (' Not-in-family', 165, 835),\n (' Unmarried', 150, 850),\n (' Husband', 210, 790),\n (' Own-child', 161, 839),\n (' Other-relative', 107, 893)]\n\n\nWe see that changing the relationship did impact the predictions across 1000 samples. Married couples, both “Wife” and “Husband” seem to be much better off with making more than 50K per year. I wonder if their income is combined and thus being double counted. “Unmarried” and “Other-Relative” seem to be the relationship types that are least likely to make more than 50K. Keep iterating through experiment like we have been to dig deeper!\nYou may be asking, how do I know 1000 samples is enough? The answer - I don’t. Try 1500 samples next and if the results show roughly the same thing you know you have a representative sample. This is again, the data science approach vs the statistical approach.\n\n\n\n\n4 What now?\nNow, you keep digging like we have been. We can be completely sure that the model has found meaningful interactions. We saw that in the accuracy score. So rather than me trying to find interactions, I am trying to uncover the interactions that the model has found. Again, I am making no assumptions in regard to distribution, co-linearity, type of relationships between variable, sample size, or anything else so it allows for much safe analysis of complex datasets."
  },
  {
    "objectID": "posts/Tabular/TabularModeling.html",
    "href": "posts/Tabular/TabularModeling.html",
    "title": "Tabular Modeling",
    "section": "",
    "text": "Background: The most common type of data in most organizations is tabular data stored in a SQL database. Tabular is also a great thing to model with because it’s often already structured in a database and in a good format to start working with. The uses for tabular data range from recommendation engines, to marketing action prediction, to churn prediction, predicting fraud, and much more.\nPurpose: The purpose of this post is to walk through the things I think about for every tabular modeling problem. I may not use every technique for every problem, and there are others that I may use on occasion - but these core ones should always be at least considered carefully (in my opinion)\nAssumptions: In this guide I am assuming you have experience building tabular models in the past and are able to look up these techniques on your own. This will offer brief descriptions of what things are and why they are important, but won’t go into the level of depth needed in this post to implement these ideas.\nWhat’s missing from this post: Modeling is only a small piece of the puzzle. I am not covering how to define a problem, how to do ETL processes, how to deploy and productionalize things, how to communicate to the organization so it can be used, how to build processes around it so people can leverage your model, etc.. This is only covering the modeling portion."
  },
  {
    "objectID": "posts/Tabular/TabularModeling.html#hierarchical-clustering",
    "href": "posts/Tabular/TabularModeling.html#hierarchical-clustering",
    "title": "Tabular Modeling",
    "section": "3.1 Hierarchical Clustering",
    "text": "3.1 Hierarchical Clustering\nAre any of these columns in the same sort order (regardless of scale or actual values)?\nHierarchical clustering is underused and very valuable. Tree based models (RF, XGboost, Catboost, etc) do not care about scale, just sort order. A decision tree treats [1,2,3] the same as [1,20,300]. Because of that doing hierarchical clustering can help identify variables that are very similar for the purposes of modeling and is a powerful tool. Instead of doing the clustering you could just do a spearman rank correlation matrix, which is the foundation for the hierarchical clustering."
  },
  {
    "objectID": "posts/Tabular/TabularModeling.html#standard-correlation-matrices",
    "href": "posts/Tabular/TabularModeling.html#standard-correlation-matrices",
    "title": "Tabular Modeling",
    "section": "3.2 Standard Correlation Matrices",
    "text": "3.2 Standard Correlation Matrices\nAre any of these columns really highly correlated with other features and can be thrown out?\nStandard Correlation Matrices can be helpful as a secondary tool to identify similar columns"
  },
  {
    "objectID": "posts/Tabular/TabularModeling.html#null-value-analysis",
    "href": "posts/Tabular/TabularModeling.html#null-value-analysis",
    "title": "Tabular Modeling",
    "section": "3.3 Null Value Analysis",
    "text": "3.3 Null Value Analysis\nAre there patterns of null values where several columns are all always null as the same times?\nAnalyzing where Null Values appear can be helpful in understanding how data was gathered or how processes effect the data. There also may be groups of columns that are always null at the same time and you can avoid having null columns for each column and just have 1 for the set."
  },
  {
    "objectID": "posts/Tabular/TabularModeling.html#understanding-hierarchies",
    "href": "posts/Tabular/TabularModeling.html#understanding-hierarchies",
    "title": "Tabular Modeling",
    "section": "3.4 Understanding Hierarchies",
    "text": "3.4 Understanding Hierarchies\nGiving careful thought to what a unique record is and understanding hierarchies is important? Often there may be product hierarchies, or account hierarchies, or billing hierarchies to be aware of."
  },
  {
    "objectID": "posts/Tabular/TabularModeling.html#feature-importance",
    "href": "posts/Tabular/TabularModeling.html#feature-importance",
    "title": "Tabular Modeling",
    "section": "4.1 Feature Importance",
    "text": "4.1 Feature Importance\nWhen building a model of remaining features, which features are most important? Are there any that are not important?\nThis is where we do our feature importance. It’s important to get rid of as many redundant features as possible before doing feature importance because having redundant features skews feature importance. If there are 10 identical columns, then they each show as 1/10 the predictive power as any of the individual column. You could easily filter then all out thinking none are important, when in reality you should have filtered 9 of them out. The hierarchical Clustering in the previous section removes these redundancies.\nOnce you have the most important features you need to do a few things. First, learn as much as you can about those columns. How is data collected? How is data updated? Is there any data leakage? What’s the distribution? What’s the significance to the business? The more you can learn about these important features the better.\nIf features are very unimportant you can cut them. Simply choose an importance threshold, cut the features and measure performance of the model. If the model still performs well then those features were not crucial."
  },
  {
    "objectID": "posts/Tabular/TabularModeling.html#time-consistency",
    "href": "posts/Tabular/TabularModeling.html#time-consistency",
    "title": "Tabular Modeling",
    "section": "4.2 Time Consistency",
    "text": "4.2 Time Consistency\nHow consistent is the predictive power of a feature over time?\nMeasuring time consistency is an absolute necessity. We are training on data to predict future data - and in production we will be predicting data even further in the future. If features are unstable over time we need to be aware of that, and there’s a good chance we can’t use those features reliably.\nTo measure this you can build a 1 feature model to predict the dependent variable and see if predictive power remains constant over time for that feature. You can then go to 2 feature models and do the same thing. If features are extremely predictive for a few months and then not other months, your model will need some way to determine when to rely on that and when not to. Sometimes it’s in the months close to a contract start/end date but often it’s just a feature we have to throw away."
  },
  {
    "objectID": "posts/Tabular/TabularModeling.html#column-sort-order",
    "href": "posts/Tabular/TabularModeling.html#column-sort-order",
    "title": "Tabular Modeling",
    "section": "4.3 Column sort order",
    "text": "4.3 Column sort order\nHow does the order in which we evaluate columns matter? If we have 2 highly correlated columns, does it matter which we choose to keep?\nEarlier we threw away columns that were correlated to only keep 1. In this step you can do tests to identify which should be thrown away. If 2 columns are correlated and you threw 1 of them out, was that the right choice or should you have thrown the other out?\nThis often doesn’t make a huge difference but it does sometimes so it is worth considering."
  },
  {
    "objectID": "posts/Tabular/TabularModeling.html#feature-engineering",
    "href": "posts/Tabular/TabularModeling.html#feature-engineering",
    "title": "Tabular Modeling",
    "section": "5.1 Feature Engineering:",
    "text": "5.1 Feature Engineering:\nHow can we engineer and encode features to improve feature importance and/or time consistency?\nI wish there was great universal guidance here, but it’s very domain specific. Try an idea, measure validation performance keep if it helps. Graph and visualize as much as you can and keep trying new ideas."
  },
  {
    "objectID": "posts/Tabular/TabularModeling.html#model-architecture-random-forest-xgb-nn",
    "href": "posts/Tabular/TabularModeling.html#model-architecture-random-forest-xgb-nn",
    "title": "Tabular Modeling",
    "section": "5.2 Model Architecture: Random Forest, XGB, NN",
    "text": "5.2 Model Architecture: Random Forest, XGB, NN\nConsidering architecture is important. For most supervised tabular modeling you will want a Random Forest, XGBoost, or a Neural Network. Random Forest is the simplest. It is fairly accurate, fast to train and hard to screw up. It’s also the model with the more interpretability so if explainability is important this can be a good option.\nXGboost often gives a performance boost, but it is more sensitive to hyperparameters and overfitting so it will take a bit longer to get it tuned properly.\nA Neural Network is not usually the right answer for tabular data. It might be worth trying if you have super high cardinality features, or really complex data you want to use alongside it (ie combining with text NLP model)."
  },
  {
    "objectID": "posts/Tabular/TabularModeling.html#categorical-encoding",
    "href": "posts/Tabular/TabularModeling.html#categorical-encoding",
    "title": "Tabular Modeling",
    "section": "5.3 Categorical encoding",
    "text": "5.3 Categorical encoding\nThe main ways to encode categorical variables are label, order, target encoding, count, and catboost. I won’t cover what each of them are, but try encoding things in different ways and see if it helps!"
  },
  {
    "objectID": "posts/Tabular/TabularModeling.html#class-imbalance",
    "href": "posts/Tabular/TabularModeling.html#class-imbalance",
    "title": "Tabular Modeling",
    "section": "5.4 Class imbalance",
    "text": "5.4 Class imbalance\nMost problems in the real world I have run into have a class imbalance. For example predicting customer churn there are way more customers staying than churning (hopefully). Churn may account for 2% of the base. There’s several things that can be done to address this, the most common approaches are using a weighted loss or over/under sampling.\nIn Class imbalance problems it is also important that you have a good metric. I am not discussing metrics here because the metric should have been defined before the dataset was even created."
  },
  {
    "objectID": "posts/Tabular/TabularModeling.html#ensemble-approaches-try-a-few-ensemble-approaches.",
    "href": "posts/Tabular/TabularModeling.html#ensemble-approaches-try-a-few-ensemble-approaches.",
    "title": "Tabular Modeling",
    "section": "5.5 Ensemble Approaches: Try a few ensemble approaches.",
    "text": "5.5 Ensemble Approaches: Try a few ensemble approaches.\nEnsembling is common, but not always necessary. You can do cross-folding, but be aware it adds complexity at inference time. Sometimes ensembling different architectures together can be valuable.\nIt really depends on the business use case and how much the increase in accuracy amounts to in real-world impact."
  },
  {
    "objectID": "posts/Tabular/TabularModeling.html#model-tuning",
    "href": "posts/Tabular/TabularModeling.html#model-tuning",
    "title": "Tabular Modeling",
    "section": "5.6 Model Tuning",
    "text": "5.6 Model Tuning\nModel tuning can give improvements as well - most libraries have a built in tuning framework. You will need to set hyperparameter ranges to try and run tests with different features. There are many guides to fine tune a model - I will do a future post on how I approach tuning. Sometimes a grid search is sufficient, but for XGBoosts I often use Bayesian Optimization with HYPEROPT."
  },
  {
    "objectID": "posts/Other/MarketingAttribution.html",
    "href": "posts/Other/MarketingAttribution.html",
    "title": "Introduction to Attribution",
    "section": "",
    "text": "Attribution is all about attributing a cause to a result. In this post I will discuss it at a high level in the context of seeking a conversion or sale. So what does that mean and why does that matter exactly?\nIn a company you have many different ways you can influence customers or potential customers. A few examples:\n\nPhone Call\nEmail with picture A\nEmail with picture B\nSend a Coupon\n\nImagine you just made a sale and you had sent a coupon, called, and sent an email to that customer in the last 6 months. Which one was responsible for the sale? Was it just one responsible or was the combination of mediums important? How do I know if the phone call was “worth it”? What about the coupon? Should I replicate this combination or are there better combinations?\nThe answers to all of this require great data about what the causes are (action) that led to the effect (sale). This is what attribution is all about.\nIf you wonder whether you should be using attribution in your efforts just ask yourself the following questions.\n\n\n\n\n\n\nAsk Yourself\n\n\n\n\nDo you want to know what activities you have done led to sales?\n\nDo you want to know whether a campaign you launched was worth it?\n\nDo you want to know what marketing you do is effective?\n\n\n\nIf the answer is yes to any of those, you should get comfy with a few different attribution models."
  },
  {
    "objectID": "posts/Other/MarketingAttribution.html#no-decay",
    "href": "posts/Other/MarketingAttribution.html#no-decay",
    "title": "Introduction to Attribution",
    "section": "4.1 No-Decay",
    "text": "4.1 No-Decay\nWhat is no-decay attribution?\nNo-decay attribution gives every interaction equal weight. This would say if you sent 4 advertisements to the customer, they are each 25% responsible for the sale.\nExample Interaction\n\n\n\n\n\n\nExample\n\n\n\nMy dad just planned a trip to Joshua Tree. He visited long ago and has been getting regular email, letters, and voice mails updating him on new events to encourage him to return. For months he didn’t open or respond to a single one, but it did get him thinking about how much he enjoyed it and would like to go back. Eventually he opened an email and did some research to plan his trip and is going next month\n\n\n\nWhy would you use no-decay attribution?\nNo-decay attribution can be in scenarios where any given interaction could trigger a sale, you just need it to land at the right time and different people continue to think through. If the customer doesn’t say no, then they may later say yes. In some scenarios which they say “yes” to isn’t consistent so you can equally weight them all."
  },
  {
    "objectID": "posts/Other/MarketingAttribution.html#position-decay-and-time-day",
    "href": "posts/Other/MarketingAttribution.html#position-decay-and-time-day",
    "title": "Introduction to Attribution",
    "section": "4.2 Position-Decay and Time-Day",
    "text": "4.2 Position-Decay and Time-Day\nWhat is position-decay and time-decay attribution?\nPosition-decay and weight-decay are both means of putting more attribution to recent actions and less on older action.\nPosition-decay does not care about time (ie days), but does care about how many actions ago. For example the most recent email, the 2nd most recent email, the 3rd most recent email, etc. Maybe all of those happened in the last 2 weeks or maybe they were spread out over 2 months, position-decay only sees how many positions it is removed from the sale.\nTime-decay is very similar to position-decay but instead instead of decaying based on number of actions, attribution decays based on time (ie days) from the sale.\n\n\n\n\n\n\nExample\n\n\n\nImagine you want to get a new couch (or your spouse wants you to get a new couch). It’s not really a priority or very urgent You do a quick amazon search but don’t buy in the moment and plan to come back to it later to figure out what to buy and who from. It’s not particularly important because you already have one and while it would be nice to have a nicer one, it really isn’t a big deal. This could go on for years, but with the right messaging this person could be motivated to buy.\n\n\n\nWhy would you use position-decay or time-decay attribution?\nYou may get various advertisements about it before you eventually take the time to place the purchase. While each does have importance to help remind you to make the purchase - the one that triggers you to buy is most important because it hit at the time with the message to motivate you to actually take action. Maybe the messaging was better, maybe the time it was sent was better, but for whatever reason it was better. The older messages did have an impact because it kept it at the forefront of your mind, but the older it was the clearer it is that it did not trigger a purchase. The final final action is what triggered the buy and you feel it deserves the most credit."
  },
  {
    "objectID": "posts/Other/MarketingAttribution.html#u-shaped",
    "href": "posts/Other/MarketingAttribution.html#u-shaped",
    "title": "Introduction to Attribution",
    "section": "4.3 U-Shaped",
    "text": "4.3 U-Shaped\nWhat is u-shaped attribution?\nA u-shaped attribution weighs the first and last touches most heavily, with lighter weights on the middle interactions.\nExample Interaction\n\n\n\n\n\n\nExample\n\n\n\nTo re-use the example from before, My dad just planned a trip to Joshua Tree. He visited long ago and has been getting regular email, letters, and voice mails updating him on new events to encourage him to return. For months he didn’t open or respond to a single one, but it did get him thinking about how much he enjoyed it and would like to go back. Eventually he opened an email and did some research to plan his trip and is going next month\n\n\n\nWhy would you use u-shaped attribution?\nIn this case you could argue that the initial outreach that got him thinking about going on the trip was was very important. The final communication that got him to purchase was also important. The stuff that happened in the middle mattered but wasn’t as critical and wasn’t even looked at. Because of this both the first and last touch are given heavy attribution while the middle stuff is given less.\nIn some luxury products this can be a great attribution methodology."
  },
  {
    "objectID": "posts/Other/MarketingAttribution.html#transition-table",
    "href": "posts/Other/MarketingAttribution.html#transition-table",
    "title": "Introduction to Attribution",
    "section": "5.1 Transition Table",
    "text": "5.1 Transition Table\nThe transition table stores probabilities for all transitions. If I send an email, what’s the probability that a phone call is next? What’s the probability that a popup is next? What’s the probability that a sale is next? All combinations are stored in a transition table.\n\n\n\n\n\n\nTip\n\n\n\n\nNote: This data is also sometimes represented as a graph with nodes being actions and edges being probabilities.\n\n\n\nHere I have a simply dummy transition table. For each action or state, we have the probability of what the next action is. This is calculated from your existing marketing data and is effectively a model of your marketing communication actions. From each action I can predict the chance of other actions occurring after that. This also means that I can see how they interact as I can see the different paths and probabilities to all those paths.\nYou will notice that Sale and NotSale don’t lead to anything else. Once a sale is made the goal is met. In some businesses it makes sense to cut it off here, and in others it makes sense to allow for longer chains where “completing” the chain is multiple buys in succession. For simplicity and the sake of high level learning I will assume the goal is a single sale.\nNotSale can mean many things to businesses. It could mean you failed to sell, or failed to upgrade the customer, it could mean that the customer asked you to not contact them in any way in the future. Regardless, the goal of a sale failed (usually in the scope of a campaign).\n\n\n\n\n\n\n  \n    \n      \n      Phone\n      Email\n      Popup\n      Sale\n      NotSale\n    \n  \n  \n    \n      Phone\n      0.20\n      0.30\n      0.35\n      0.05\n      0.10\n    \n    \n      Email\n      0.20\n      0.34\n      0.25\n      0.10\n      0.11\n    \n    \n      Popup\n      0.22\n      0.31\n      0.30\n      0.08\n      0.09\n    \n    \n      Sale\n      1.00\n      0.00\n      0.00\n      0.00\n      0.00\n    \n    \n      NotSale\n      0.00\n      0.00\n      0.00\n      0.00\n      1.00"
  },
  {
    "objectID": "posts/Other/MarketingAttribution.html#removal-effect",
    "href": "posts/Other/MarketingAttribution.html#removal-effect",
    "title": "Introduction to Attribution",
    "section": "5.2 Removal Effect",
    "text": "5.2 Removal Effect\nNow that we have all the probabilities of all the interactions we can measure what the sale rate is for any given chain. We can also use those probabilities to run simulations. We can then run those simulations without 1 action and see how it effects the sale rate. The difference in that is the removal weight. By calculating this removal weight for each action, we can determine the amount to attribute to a particular action or channel.\n\n\n{'total_revenue': 219797.21000000002,\n 'Popup': 0.49192217051969217,\n 'Email': 0.16007477186631336,\n 'Phone': 0.3480030576139944}"
  },
  {
    "objectID": "posts/Other/MultiArmedBandits.html",
    "href": "posts/Other/MultiArmedBandits.html",
    "title": "Multi-Armed Bandits",
    "section": "",
    "text": "Background: Multi Armed Bandits (MAB) are a method of choosing the best action from a bunch of options. In order to choose the best action there are several problems to solve. These are:\nPurpose: The purpose of this post is to explain what a Multi Armed Bandit (MAB) is, and how it solves those problems."
  },
  {
    "objectID": "posts/Other/MultiArmedBandits.html#use-segments",
    "href": "posts/Other/MultiArmedBandits.html#use-segments",
    "title": "Multi-Armed Bandits",
    "section": "4.1 1. Use segments",
    "text": "4.1 1. Use segments\nRather than calculate metrics over the entire universe of customers.\nDifferent segments of customers may behave differently. The “best” action for 1 group of customers may be different than the “best action” for a different group of customers. Instead of treating all customers equal, break them into segments so we can personalize actions more.\n\n\n\n\n\n\nExample\n\n\n\nOne segment could be “people over 65 that live in a cold climate area that has opened at least 1 email in the past”"
  },
  {
    "objectID": "posts/Other/MultiArmedBandits.html#use-smarter-exploration-methods",
    "href": "posts/Other/MultiArmedBandits.html#use-smarter-exploration-methods",
    "title": "Multi-Armed Bandits",
    "section": "4.2 2. Use smarter exploration methods",
    "text": "4.2 2. Use smarter exploration methods\nRather than picking exploration actions based on random chance use statistics to determine which need more sampling\nThe most common 2 methods of defining exploration:\n\nThompson Sampling: Choose best action based on random belief based on the distributions rather than just the metric. 1 action may be better overall but based on a random sample in the distribution a ‘less good’ action may be better than a ‘best action’ based on distribution overlap.\n\nUpper Confidence Bound: Choose based on how confident you are in your measurements. The is no need to explore a category you have high confidence in (ie very stable with lots of data)."
  },
  {
    "objectID": "posts/Other/MultiArmedBandits.html#incorporate-business-logic",
    "href": "posts/Other/MultiArmedBandits.html#incorporate-business-logic",
    "title": "Multi-Armed Bandits",
    "section": "4.3 3. Incorporate business logic",
    "text": "4.3 3. Incorporate business logic\nThere are 2 main places this can be added\n\n4.3.1 Action Selection\nSometimes not all actions are appropriate or we want to modify the probability an action is selected based on specific customer behavior. Let’s look at a couple examples of how we can weave in business logic.\n\n\n\n\n\n\nExample\n\n\n\nIf no phone number on file all actions related to email cannot be selected\n\n\nThis is the simplest example of a business rule. It could be that overall sending an email is the best action for customers, but we wouldn’t want to choose an email action if we have no way to reach out to them.\n\n\n\n\n\n\nExample\n\n\n\nIf have customer has not answered the last 10 phone calls, reduce call action probabilities by 10%\n\n\nThis is a simple example. Making a phone call may be the best way overall to make a sale, but we have information on this specific customer that they do not answer the phone. We may still want to try on occasion, but less often for this specific customer.\n\n\n\n\n\n\nExample\n\n\n\nIf a customer has bought X products before, remove Y offer from consideration\n\n\nThis is where it’s important to incorporate human expertise and knowledge of a business in. The best technical solutions don’t ignore human expertise, but rather enhance human expertise and help scale that expertise to more customers.\n\n\n4.3.2 Action Execution\nYou can have some generic actions such as “Send Offer Email” and insert business logic there. For example “Send Offer Email” may send template 1 with cruise destinations for landlocked states or template 2 with resort destinations for people in ocean border states."
  },
  {
    "objectID": "posts/Other/MultiArmedBandits.html#deep-learning",
    "href": "posts/Other/MultiArmedBandits.html#deep-learning",
    "title": "Multi-Armed Bandits",
    "section": "4.4 4. Deep Learning",
    "text": "4.4 4. Deep Learning\nThis is rarely the best starting point and requires a great deal of expertise. Rather than picking actions based on metric you could pick an action based on predicted outcomes using an output of NN (meaning NN needs to be updated in real time).\nUsing deep learning can often improve accuracy quite a bit, but it comes with a lot of negatives as well.\nPros: + Increased accuracy\nCons: + Harder to change, add, and remove actions + Less “Interpretable”, meaning it is harder to understand rationale behind what/why certain actions are considered “best” + Much longer to set up and much harder to maintain over time + Higher risk of hidden biases\nThink carefully before going this route! Using deep learning for this is an incredibly powerful approach in the right situations, but those situations are not as common as many people believe."
  },
  {
    "objectID": "posts/Other/QuartoDocumentsBestPractices.html",
    "href": "posts/Other/QuartoDocumentsBestPractices.html",
    "title": "Quarto Document Best Practices",
    "section": "",
    "text": "I use quarto documents every day. It is used when I am developing, I use it for my website, and I use it for client presentations.\n\n\n\n\n\n\nQuarto Benefits\n\n\n\n\nVersion control is very nice for presentations\nText, docs, code ouputs, plots, code, tests all in one place.\nEasy to make really good looking outputs\nLearn one tool for lots of formats - documents, presentations, websites, development\nRendering to html is a option for sharing. It’s more powerful than pdf and everyone can open it in their browser.\n\n\n\nI have some starter Quarto templates here that will be useful in conjunction with this post."
  },
  {
    "objectID": "posts/Other/QuartoDocumentsBestPractices.html#front-matter",
    "href": "posts/Other/QuartoDocumentsBestPractices.html#front-matter",
    "title": "Quarto Document Best Practices",
    "section": "4.1 Front Matter",
    "text": "4.1 Front Matter\nFor documents here’s my default front matter. A few things that are important:\n---\ntitle: \"My Title\"\nauthor: \"Isaac Flath\"\ndate: \"9/20/2022\"\nformat:\n  html:\n    css: 'styles.css'\n    number-sections: true\n    toc: true\n    theme: cosmo\n    highlight-style: tango\n    code-fold: true\n    self-contained: true\n---\n\n\n\n\n\n\nTip\n\n\n\n\nnumber-sections makes the sections stand out more organization wise\ntoc (table of contents) is clickable to jump to sections and I review it as my agenda at the start of meetings\ncode-fold I often like to have all code folded by default and unhide what’s needed.\nself-contained is critical. This allows you to share the document for anyone to open in their browser with no hassle."
  },
  {
    "objectID": "posts/Other/QuartoDocumentsBestPractices.html#css",
    "href": "posts/Other/QuartoDocumentsBestPractices.html#css",
    "title": "Quarto Document Best Practices",
    "section": "4.2 CSS",
    "text": "4.2 CSS\nThere were two things that bothered me about quarto:\n\nTable of contents was too light\nMajor sections didn’t stand out enough\n\nThese are fixed with a short bit of css. I sometimes add an h2 gradient from lightblue to white as well.\n\n.title.toc-ignore {\n  font-weight: 1000;\n}\nh1:not(.title){\n  background-image: linear-gradient(120deg, lightblue, blue);\n}\n\n\n\n\n\n\nTip\n\n\n\nIn the frontmatter section it references a styles.css where this is stored."
  },
  {
    "objectID": "posts/Clustering/MeanShiftFromScratch.html",
    "href": "posts/Clustering/MeanShiftFromScratch.html",
    "title": "MeanShift From Scratch",
    "section": "",
    "text": "Meanshift is a clustering algorithm in the same family as K-Means. K-Means is the much more widely known well known clustering algorithm, but is advantageous in a lot of situations.\nFirst, you don’t have to decide how many clusters ahead of time. This is important because in many datasets especially as they get very complex it can be hard to know how many clusters you really should have. Meanshift requires bandwidth which is much easier to select.\nSecond, k-means looks at circular clusters. You need to do some custom work to make it work for non-circular clusters. Sometimes data doesn’t split nicely into circular clusters. Meanshift can handle clusters of any shape.\n\n\n\n\n\n\nCredit\n\n\n\nThis follows what Jeremy Howard did in a notebook as part of the 2022 part 2 course. I’m changing a few things, explaining things slightly different, and doing a few additional things - but his lecture covers the bulk of what in here and was the inspiration and starting point!"
  },
  {
    "objectID": "posts/Clustering/MeanShiftFromScratch.html#calculate-distance",
    "href": "posts/Clustering/MeanShiftFromScratch.html#calculate-distance",
    "title": "MeanShift From Scratch",
    "section": "4.1 Calculate Distance",
    "text": "4.1 Calculate Distance\nIn K-Means, you calculate the distance between each point and the cluster centroids. In meanshift we calculate the distance between each point and every other point. Given a tensor of centroid coordinates and a tensor of data coordinates we calculate distance.\n\n\n\n\n\n\nDistance Calculation Steps\n\n\n\n\nSubtract data points coordinates from all other data point coordinates\nTake absolute value of distances\nPythagorean Calculation\n\nSquare coordinates\nAdd them together\nTake the Square Root\n\n\n\n\nLet’s put that in a function.\n\ndef calculate_distances(data:torch.Tensor # Data points you want to cluster\n                       )-> torch.Tensor: # Tensor containing euclidean distance between each centroid and data point    \n    '''Calculate distance between centroids and each datapoint'''\n    axis_distances = data.reshape(-1,1,2).sub(data.reshape(1,-1,2))#.abs()\n    euclid_distances = axis_distances.square().sum(axis=-1).sqrt()\n    return euclid_distances"
  },
  {
    "objectID": "posts/Clustering/MeanShiftFromScratch.html#calculate-weights",
    "href": "posts/Clustering/MeanShiftFromScratch.html#calculate-weights",
    "title": "MeanShift From Scratch",
    "section": "4.2 Calculate weights",
    "text": "4.2 Calculate weights\nNext we need to create the weights. There are 2 factors that go into calculating weights\n\nHow far away is this point from the other point?\nWhat’s the bandwidth?\n\nThe way we use this is we create a gaussian function to determine the weight based on distance. That looks like this.\n\ndef gaussian(x): return torch.exp(-0.5*((x))**2) / (math.sqrt(2*math.pi))\n\n\n_x=torch.linspace(-5, 5, 100)\nplt.plot(_x,gaussian(_x)); plt.show()\n\n\n\n\nWe modify the above slightly by adding a parameter called the bandwidth. By adjusting the bandwidth we can adjust how fast or slow the weights decay as the distance increases. A Gaussian with a bandwidth of 1 (middle chart) is just the normal distribution we saw above.\nBecause distance is never negative, we don’t need negative values\n\n\n\n\n\n\nTip\n\n\n\nThe bandwidth is the standard deviation of the gaussian\n\n\n\ndef gaussian(x, bw): return torch.exp(-0.5*((x/bw))**2) / (bw*math.sqrt(2*torch.pi))\n\n\n_x=torch.linspace(0, 5, 100)\nfig,ax = plt.subplots(1,3,figsize=(15,3))\nfor a,bw in zip(ax,[.5,1,2]):\n    a.plot(_x, gaussian(_x, bw))\n    a.set_title(f\"Gaussian with bandwidth={bw}\")\n    a.set_xlabel('Distance'); a.set_ylabel('Weight')\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/MeanShiftFromScratch.html#update-centroids",
    "href": "posts/Clustering/MeanShiftFromScratch.html#update-centroids",
    "title": "MeanShift From Scratch",
    "section": "4.3 Update Centroids",
    "text": "4.3 Update Centroids\nNow that we have our distance and weights we can update our centroid predictions and loop through until the points converge to give us cluster locations. We do this by taking a weighted average of all the other points based (using the weights calculated previously).\n\ndef meanshift(X):\n    dist = calculate_distances(X)\n    weight = gaussian(dist, 2.5)\n    X = weight@X/weight.sum(1,keepdim=True)\n    return X"
  },
  {
    "objectID": "posts/Clustering/MeanShiftFromScratch.html#all-together",
    "href": "posts/Clustering/MeanShiftFromScratch.html#all-together",
    "title": "MeanShift From Scratch",
    "section": "4.4 All Together",
    "text": "4.4 All Together\nNow that we have our meanshift function, we can create a function to run the model for several epochs and a function to plot the results. A few nuances here:\n\nI am making run_exp a higher order function so it’s easy to try tweaks to the algorithm\nI am testing if the coordinates change. If coordinates do not change I consider it converged and stop the training. I will run the model until it converges or after 100 epochs.\n\n\ndef run_exp(data,fn,max_epochs=100):\n    coord = data.clone() \n    for i in range(max_epochs):\n        _coord = fn(coord)\n        if (i>0) and (coord==_coord).all()==True: break\n        else:\n            coord = _coord\n            p_centroids = torch.unique(coord,dim=0)\n    return p_centroids,coord,i\n\nNext I create a couple functions to help me try things quickly and not have to scroll through duplicate print/plot code lines.\n\ndef plot_results(centroids,p_centroids):\n    _,ax = plt.subplots()\n    ax.scatter(p_centroids[:,0],p_centroids[:,1],label='Predicted',marker='x',s=200)\n    ax.scatter(centroids[:,0],centroids[:,1],label='Target',marker='o',s=200,alpha=0.5)\n    ax.legend()\n    plt.show()\n\n\ndef print_results(fn,data):\n    %timeit p_centroids,p_coord,p_epochs = run_exp(data,fn)\n    p_centroids,p_coord,p_epochs = run_exp(data,fn)\n    print(f\"Algorithm found {p_centroids.shape[0]} Centroids in {p_epochs} epochs\")\n    plot_results(centroids,p_centroids)"
  },
  {
    "objectID": "posts/Clustering/MeanShiftFromScratch.html#baseline",
    "href": "posts/Clustering/MeanShiftFromScratch.html#baseline",
    "title": "MeanShift From Scratch",
    "section": "5.1 Baseline",
    "text": "5.1 Baseline\n\n\n\n\n\n\nAlgorithm Results\n\n\n\n\nFound the right number of centroids (6)\nConverged in 9 epochs\nPlot looks good - centroids are accurately placed\n\n\n\n\ndef meanshift1(X):\n    dist = calculate_distances(X)\n    weight = gaussian(dist, 2.5)\n    return weight@X/weight.sum(1,keepdim=True)\n\n\nprint_results(meanshift1,data)\n\n151 ms ± 457 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\nAlgorithm found 6 Centroids in 9 epochs"
  },
  {
    "objectID": "posts/Clustering/MeanShiftFromScratch.html#ramp",
    "href": "posts/Clustering/MeanShiftFromScratch.html#ramp",
    "title": "MeanShift From Scratch",
    "section": "5.2 Ramp",
    "text": "5.2 Ramp\nI tried using a linear decline then flat at 0 instead of a gaussian to see if that speeds things up. This was from Jeremy’s lecture as an idea to try that seemed to work.\n\n\n\n\n\n\nAlgorithm Results\n\n\n\n\nFound the right number of centroids (6)\nConverged in 9 epochs\nPlot looks good - centroids are accurately placed\nRan in same speed as the gaussian. I figure that might change if scaled up and put on a GPU with lots of data. Worth more testing!\n\n\n\n\ndef tri(d, i): return (-d+i).clamp_min(0)/i\n\n\ndef meanshift2(X):\n    dist = calculate_distances(X)\n    weight = tri(dist, 8)\n    return weight@X/weight.sum(1,keepdim=True)\n\n\nprint_results(meanshift1,data)\n\n151 ms ± 1.13 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\nAlgorithm found 6 Centroids in 9 epochs"
  },
  {
    "objectID": "posts/Clustering/MeanShiftFromScratch.html#sampling",
    "href": "posts/Clustering/MeanShiftFromScratch.html#sampling",
    "title": "MeanShift From Scratch",
    "section": "5.3 Sampling",
    "text": "5.3 Sampling\nThis is the original meanshift (with gaussian) with a random sample of the data. Even with 20% of the data it got really good centroids (though not perfect) but run much faster. It also converged to 6 cluster in 8 epochs. This seems useful.\n\n\n\n\n\n\nAlgorithm Results\n\n\n\n\nFound the right number of centroids (6)\nConverged in 8 epochs\nPlot looks good - centroids are reasonably accurately (but not perfectly) placed\nRan in less than 10% of the time of the original\nCan control size of random sample based on needs (ie if more accuracy needed)\n\n\n\n\nprint_results(meanshift1,data[np.random.choice(len(data),300)])\n\n13.8 ms ± 142 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nAlgorithm found 6 Centroids in 8 epochs"
  },
  {
    "objectID": "posts/Clustering/KMeansFromScratch.html",
    "href": "posts/Clustering/KMeansFromScratch.html",
    "title": "K-Means From Scratch",
    "section": "",
    "text": "Code\nimport math, random, matplotlib.pyplot as plt, operator, torch\nfrom functools import partial\nfrom fastcore.all import *\nfrom torch.distributions.multivariate_normal import MultivariateNormal\nfrom torch import tensor\n\n\n\n\nCode\ntorch.manual_seed(42)\ntorch.set_printoptions(precision=3, linewidth=140, sci_mode=False)\n\n\n\n\nCode\ndef plot_data(centroids:torch.Tensor,# Centroid coordinates\n              data:torch.Tensor, # Data Coordinates\n              n_samples:int, # Number of samples\n              ax:plt.Axes=None # Matplotlib Axes object\n             )-> None:\n    '''Creates a visualization of centroids and data points for clustering problems'''\n    if ax is None: _,ax = plt.subplots()\n    for i, centroid in enumerate(centroids):\n        samples = data[i*n_samples:(i+1)*n_samples]\n        ax.scatter(samples[:,0], samples[:,1], s=1)\n        ax.plot(*centroid, markersize=10, marker=\"x\", color='k', mew=5)\n        ax.plot(*centroid, markersize=5, marker=\"x\", color='m', mew=2)"
  },
  {
    "objectID": "posts/Clustering/KMeansFromScratch.html#calculate-distance",
    "href": "posts/Clustering/KMeansFromScratch.html#calculate-distance",
    "title": "K-Means From Scratch",
    "section": "3.1 Calculate Distance",
    "text": "3.1 Calculate Distance\nIn order to initialize our centroids we need to be able to calculate distances, so let’s do that first.\nGiven a tensor of centroid coordinates and a tensor of data coordinates we calculate distance by: + Subtract centroids coordinates from data points coordinates + Take absolute value of distances + Pythagorean Calculation + Square coordinates + Add them together + Take the Square Root\nThat gives us the euclidean distance between each data point and each centroid.\n\ndef calculate_distances(centroids:torch.Tensor, # Centroid coordinates\n                        data:torch.Tensor # Data points you want to cluster\n                       )-> torch.Tensor: # Tensor containing euclidean distance between each centroid and data point    \n    '''Calculate distance between centroids and each datapoint'''\n    axis_distances = data.reshape(-1,1,2).sub(centroids.reshape(1,-1,2)).abs()\n    euclid_distances = axis_distances.square().sum(axis=-1).sqrt()\n    return euclid_distances"
  },
  {
    "objectID": "posts/Clustering/KMeansFromScratch.html#initialize-centroids",
    "href": "posts/Clustering/KMeansFromScratch.html#initialize-centroids",
    "title": "K-Means From Scratch",
    "section": "3.2 Initialize Centroids",
    "text": "3.2 Initialize Centroids\nWhere we initialize our centroids is really important. If we don’t have good initialization we are very likely to get stuck in a local optimum. Especially with 6 centroids. One option is to run the algorithm many times and pick the best solution, but it’s a much better idea to try to have good initializations.\nWe pick centroid locations in the following way:\n\nPick a random data point and use those coordinates as the first centroid\nLoop to create remaining centroids\n\nCalculate the distance between existing centroids and data points.\nGet the distance from each data point to it’s closest centroid\nPlace the next centroid at the point with the max distance from previous step\n\n\nThis ensures we get initialization that are nice and far away from each other and spread out amonth the data, minimizing the risk of hitting local optimums.\n\ndef initialize_centroids(data:torch.Tensor,# Data points you want to cluster\n                         k:torch.Tensor # Number of centroids you want to initialize\n                        )->torch.Tensor: # Returns starting centroid coordinates\n    '''Initialize starting points for centroids as far from each other as possible.'''\n    pred_centroids = data[random.sample(range(0,len(data)),1)]\n    for i in range(k-1): \n        _centroid = data[calculate_distances(pred_centroids,data).min(axis=1).values.argmax()]\n        pred_centroids = torch.stack([*pred_centroids,_centroid])\n    return pred_centroids"
  },
  {
    "objectID": "posts/Clustering/KMeansFromScratch.html#classify-data-points",
    "href": "posts/Clustering/KMeansFromScratch.html#classify-data-points",
    "title": "K-Means From Scratch",
    "section": "3.3 Classify Data Points",
    "text": "3.3 Classify Data Points\nOnce we have centroids (or updated centroids), we need to assign a centroid to each data point. We do this by calculating the distance between each data point and each centroid, and assigning each datapoint to it’s closes centroid.\n\ndef assign_centroids(centroids:torch.Tensor, # Centroid coordinates\n                     data:torch.Tensor # Data points you want to cluster\n                    )->torch.Tensor: # Tensor containing new centroid assignments for each data point\n    '''Based on distances update centroid assignments'''\n    euclid_distances = calculate_distances(centroids,data)\n    assigned_cluster = euclid_distances.squeeze().argmin(axis=1)\n    return assigned_cluster"
  },
  {
    "objectID": "posts/Clustering/KMeansFromScratch.html#update-centroids",
    "href": "posts/Clustering/KMeansFromScratch.html#update-centroids",
    "title": "K-Means From Scratch",
    "section": "3.4 Update Centroids",
    "text": "3.4 Update Centroids\nTo update the centroid locations, we take the mean of all the data point assigned to that centroid. We make the new centroid that point.\n\ndef update_centroids(centroid_assignments:torch.Tensor, # Centroid coordinates\n                     data:torch.Tensor # Data points you want to cluster\n                    )->torch.Tensor: # Tensor containing updated centroid coodinates\n    '''Update centroid locations'''\n    n_centroids = len(centroid_assignments.unique())\n    pred_centroids = [data[centroid_assignments==i].mean(axis=0) for i in range(n_centroids)]\n    return torch.stack(pred_centroids)"
  },
  {
    "objectID": "posts/GradientDescent/2020-05-09-GradientDescentforLinearRegression-P1.html",
    "href": "posts/GradientDescent/2020-05-09-GradientDescentforLinearRegression-P1.html",
    "title": "Gradient Descent for Linear Regression - Part 1",
    "section": "",
    "text": "The goal of linear regression is to find parameter values that fit a linear function to data points.  The goodness of fit is measured by a cost function.  The lower the cost, the better the model.  Gradient Descent is a method to minimize a cost function.  Gradient descent is a widely used tool and is used frequently in tuning predictive models.  It’s important to understand the method so you can apply it to various models and are not limited to using black box models.\nAs I just mentioned, gradient descent is a method to reduce a cost function.  To understand how to minimize a cost function, you need to understand how cost is calculated.  For this post I will be using a very simple example; linear regression with one feature, two data points, and two regression coefficients.  I will use the sum of squares cost function to take the predicted line and slowly change the regression coefficients until the line passes through both points.\n\nIn this example we could easily draw a line through the points without using gradient descent, but if we had more data points this would get trickier.  In the table below we can see what the data looks like that we are working with.\n\nThe tree below illustrates how to solve for cost as well as how to improve the values of \\(\\theta\\) to minimize cost.  In the illustration above, \\(J = a^1 + a^2\\) is the cost function we want to minimize.  As we can see, if the regression coefficients (\\(\\theta_0+\\theta_1\\)) do not give a good fit, then the difference between our predicted values and observed values will be large and we will have a high cost (\\(J\\)).  For low values, we will have a low cost (\\(J\\)).  The figure below shows us how to calculate cost from the regression coefficients (\\(\\theta_0\\) and \\(\\theta_1\\)).\n\nThe second thing this chart shows you is how to improve values of theta.  We used the formulas in the boxes to evaluate \\(J\\), so now we will use the values on the edges to improve the parameter values.  Each regression coefficient has a path up to the cost function.  You get a path value for each \\(\\theta\\) on the tree by multiplying the edge values along that path.  For example:\n\\(\\theta_1\\; path\\;value = x^1 (y^1_{pred} - y^1_{obs}) (1) + x^2 (y^2_{pred} - y^2_{obs}) (1)\\)\nThe last step is to improve the value of \\(\\theta\\).  In order to improve the value of \\(\\theta\\), we need to multiply the path value by \\(\\alpha\\), and subtract that from that \\(\\theta\\).  \\(\\alpha\\) is a value that determines how large the increments will be taken during optimization.  If you pick an \\(\\alpha\\) value that is is too large, you risk missing the local optima.  If you choose an \\(\\alpha\\) value that is too small you will be very accurate, but it will be more computationally expensive. With more data points there would be more edges originating at \\(J\\), and with more features there would be more thetas originating from the predicted values, but the same concept can be applied to these more complicated examples."
  },
  {
    "objectID": "posts/GradientDescent/2020-06-01-GradientDescentforLinearRegression-P1B.html",
    "href": "posts/GradientDescent/2020-06-01-GradientDescentforLinearRegression-P1B.html",
    "title": "Gradient Descent for Linear Regression - Part 1B",
    "section": "",
    "text": "1 Why part 1B?\nI have been getting questions about the initial Gradient Descent Post. The questions boil down to “So with 2 points I can define a line, but I could already do that. What I need is to fit a line where points aren’t perfect and I have lots of them. How do I use gradient descent in a more complicated problem?\nThis post will quickly recap the initial Gradient Descent for Linear Regression post, show that methodology applied in a google sheet so you can see each calculation, and then scale that methodology to more points.\n\n\n2 Goal Recap\nThe goal of linear regression is to find parameter values that fit a linear function to data points.  The goodness of fit is measured by a cost function.  The lower the cost, the better the model.  Gradient Descent is a method to minimize a cost function.  Gradient descent is a widely used tool and is used frequently in tuning predictive models.  It’s important to understand the method so you can apply it to various models and are not limited to using black box models.\nI will use the sum of squares cost function to take a predicted line and slowly change the regression coefficients until the line is a line of best fit. Here’s what it looks like before and after 24 iterations of gradient descent. As you can see, after 24 iterations our predicted points are getting pretty close to a best fit. You will be able to use the method defined here to scale this to as many points as you have.\n\nIn the first blog we showed this table for how we calculate out prediction. Because we are talking about a linear problem, y = mx + b is the equation, or in calculus terms \\(y = \\theta_0+\\theta_1x\\). We could take this table and expand it down to include \\(x_3\\) all the way through \\(x_n\\) to represent our dataset.\n\nThe tree below from the first blog illustrates how to solve for cost as well as how to improve the values of \\(\\theta\\) to minimize cost in the 2 point problem defined there. So the question is, how would we modify this tree for more points? Well, with more data points there would be more edges originating at \\(J\\), and with more features there would be more thetas originating from the predicted values, but the same concept can be applied to these more complicated examples. Specifically, here is what I would change for a more complicated example with more features:\n\nFirst, we have a branch for \\(x^1\\) and a branch for \\(x^2\\). These branches are almost identical, other than it being for the 2nd point vs the 1st point. So the first step is to add a branch off of \\(J = A^1 + a^2\\) for \\(x^3\\) all the way to \\(x^n\\).\nThe second step is to take our formula \\(1/2 * (y_{pred} - y_{obs})^2\\) and change \\(1/2\\) to 1 over <# of data points>. This isn’t strictly neccesary, but it makes the values of J we see a bit more intiutive and scaled.\n\nThe third thing is to multiply our path values by 1 over <# of data points>. Again, this isn’t strictly neccesary but it makes setting a learning rate much more intuitive rather than having to do something more complicated to scale our learning rate based on the number of points we have. As a refresher, the path value for theta 1 was \\(\\theta_1\\; path\\;value = x^1 (y^1_{pred} - y^1_{obs}) (1) + x^2 (y^2_{pred} - y^2_{obs}) (1)\\), which by multiplying values from the edges in the chart together. The path value for theta 1 will now be \\(\\theta_1\\; path\\;value = (x^1 (y^1_{pred} - y^1_{obs}) (1) + x^2 (y^2_{pred} - y^2_{obs}) (1)) * \\frac{1}{<\\# features>)}\\). We will do that for the path value formula for \\(\\theta_0\\) as well.\n\n\nJust like with 2 points, we will multiply the path value by \\(\\alpha\\), and subtract that from that \\(\\theta\\) to improve our predictions\n\n\n3 See this actually work\nI have created a google sheet that walks through these cauculations. I strongly reccomend walking through each cell calculation and matching it up to the chart above. Star with the 2Points_Linear_Scalable tab. You can then go to the More_Points_Linear tab and see that it’s the exact same formulas and calculations.\nClick here for the Google Sheet\nFor bonus points, you can start to see what a more advanced gradient descent algorithm is on the Momentum tab. If you look through all the formulas, you will see it’s almost the same thing - but instead of using just the new path value we are doing a weighted average of the path value with the previous path value."
  },
  {
    "objectID": "posts/GradientDescent/2020-06-11-GradientDescentforLinearRegression-P2.html",
    "href": "posts/GradientDescent/2020-06-11-GradientDescentforLinearRegression-P2.html",
    "title": "Gradient Descent for Linear Regression - Part 2",
    "section": "",
    "text": "1 Why part 2?\nI have done a couple blog posts on Gradient Descent for linear regression focusing on the basic algorithm. In this post, I will be covering some more advanced gradient descent algorithms. I will post as I complete a section rather than waiting until I have every variation posted. This is partially to show some popular ones, but the more important thing to understand from this post is that all these advanced algorithms are really just minor tweaks on the basic algorithm.\n\n\n2 Goal Recap\nThe goal of linear regression is to find parameter values that fit a linear function to data points.  The goodness of fit is measured by a cost function.  The lower the cost, the better the model.  Gradient Descent is a method to minimize a cost function.  Gradient descent is a widely used tool and is used frequently in tuning predictive models.  It’s important to understand the method so you can apply it to various models and are not limited to using black box models. This approach will use the sum of squares cost function to take a predicted line and slowly change the regression coefficients until the line is a line of best fit.\nThis post will cover the algorithms. Part 4 of this series will focus on scaling this up to larger datasets. One of the primary tools of scaling is using stochastic gradient descent, which is just a fancy way to say “just use a subset of the points instead of all of them”.\n\n\n3 Background\nOur goal is to define the equation \\(y=\\theta_0+\\theta_1x\\). This is the same thing as \\(y=mx+b\\). For this post I will use \\(y=mx+b\\) language with \\(m\\) being the slope and \\(b\\) being the y intercept.\n\nNote: In order to adjust \\(m\\), I will take \\(m\\) - <\\(m\\) PathValue> * <adj \\(\\alpha\\)>.\n\n\nNote: In order to adjust \\(b\\), I will take \\(b\\) - <\\(b\\) PathValue> * <adj \\(\\alpha\\)>.\n\nEach of these advanced algorithms either change the Path Value, or change \\(\\alpha\\). I will show what the calculation for each is for each algorthm is, have a written explanation, and python code that illustrates it.\n\n\n4 Setup\nHere is where I load libraries, define my dataset, and create a graphing function.\n\n#collapse-hide\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport math\nfrom IPython.display import clear_output\nfrom time import sleep\n\nnp.random.seed(44)\nxs = np.random.randint(-100,100,100)\nys = xs * np.random.randint(-10,10) + 100 # + np.random.randint(-200,200,50)\ncycles = 50\n\ndef graph_gradient_descent(values,cycles, figures,step):\n    plt.figure(figsize=(20,10))\n    cols = 3\n    rows = math.ceil(figures/3)\n    \n    for x in range(1,figures+1):\n        plt.subplot(rows,cols,x)\n        plt.scatter(values['x'],values['y'])\n        plt.scatter(values['x'],values['cycle0'])\n        plt.scatter(values['x'],values['cycle'+str(x*step)])\n        labels = ['y','initial','cycle'+str(x*step)]\n        plt.legend(labels)\n        plt.ylim(-1000,1000)\n        plt.xlim(-100,100)\n\n\n\n5 Basic Gradient Descent\nThis is the basic gradient descent algorithm that all others are based on. If you are not clear on how gradient descent works, please refer to the background section for a review or Gradient Descent Part 1 Blog Post for more details. I will use this same format below for each algorithm, and change only what is necessary for easier comparison.\n\n5.0.1 Inputs\n\\(\\alpha = learning rate\\)\nn = number of data points\n\n\n5.0.2 New Formulas\n\\(PathValue_m = PathValue_bx\\)\n\\(PathValue_b = y_{pred}-y_{obs}\\)\nEach variation after this does 1 of 3 things to modify this algorithm: + Adjusts \\(\\alpha\\) by some amount + Adjust \\(PathValue\\) by some amount. + Adjust both \\(\\alpha\\) and \\(PathValue\\).\nReally logically speaking, what else can you do? These are the only values that are used to adjust our values, so any tweaks must involve those. We can modify number through addition, subtraction, multiplication, and division: If you get stuck, try to get back to those basics.\n\n\n5.0.3 Python Function\n\nalpha = 0.0005\n\ndef gradient_descent(xs,ys,alpha,cycles):\n    n = len(xs)\n    adj_alpha = (1/n)*alpha\n    \n    values = pd.DataFrame({'x':xs,'y':ys})\n    weights = pd.DataFrame({'cycle0':[1,1,0,0]},index=['m','b','pvb','pvm'])\n    values['cycle0'] = weights['cycle0'].m*values['x'] + weights['cycle0'].b\n    \n    for cycle in range(1,cycles+1):\n        p_cycle_name = 'cycle'+str(cycle-1)\n        c_cycle_name = 'cycle'+str(cycle)\n        \n        error = values[p_cycle_name]-values['y']\n        path_value_b = sum(error)\n        path_value_m = sum(error * values['x'])\n        \n        new_m = weights[p_cycle_name].m - path_value_m * adj_alpha\n        new_b = weights[p_cycle_name].b - path_value_b * adj_alpha\n        \n        weights[c_cycle_name] = [new_m,\n                                new_b,\n                                path_value_m,\n                                path_value_b]\n        \n        y_pred = weights[c_cycle_name].m*values['x'] + weights[c_cycle_name].b\n        \n        values[c_cycle_name] = y_pred\n        \n    return weights,values\n\nweights, values = gradient_descent(xs,ys,alpha,cycles)\n\ngraph_gradient_descent(values,cycles,12,2)\n\n\n\n\n\n\n\n6 Momentum\n\n6.0.1 Concept\nThe idea of momentum is to use the previous path values to influence the new path value. It’s taking a weighted average of the previous path value and the new calculation. This is referred to as momentum because it is using the momentum from previous points to change the size of the step to take. To control what kind of weighted average is used, we define \\(\\beta\\).\nThis is useful and effective because we want to have very large steps early on, but the closer we get to the optimal values the lower we want our learning rate to be. This allows us to do that, and if we overshoot then it will average with previous path values and lower the step size. This allows for larger steps while minimizing the risk of our gradient descent going out of control. If you overshoot the optimal weights the weighted average will decrease the step size and keep going, eventually settling on the minimum. A very handy feature!\n\n\n6.0.2 What is different\nWhat is different: \\(PathValue\\) has changed and is using a new input \\(\\beta\\)\nIf you look at \\(PathValue_b\\) you will notice a change in this formula. \\(PathValue_m\\) multiplies \\(PathValue_b\\) by our x value for that point, so it is effected as well.\n\n\n6.0.3 New Inputs\n\\(\\beta = 0.9\\)\n\n\n6.0.4 New Formulas\n\\(\\alpha_{adj} = \\frac{1}{n}\\alpha\\)\n\\(PathValue_m\\) = \\(PathValue_bx\\)\n\\(PathValue_b = (\\beta)(PathValue_{b_{previous}}) + (1 - \\beta)(y_{pred}-y_{obs})\\)\n\n\n6.0.5 Python Function\n\nalpha = 0.0001\nbeta = 0.9\n\ndef gradient_descent_momentum(xs,ys,alpha,cycles,beta):\n    n = len(xs)\n    adj_alpha = (1/n)*alpha\n    \n    values = pd.DataFrame({'x':xs,'y':ys})\n    weights = pd.DataFrame({'cycle0':[1,1,0,0]},index=['m','b','pvm','pvb'])\n    values['cycle0'] = weights['cycle0'].m*values['x'] + weights['cycle0'].b\n    \n    for cycle in range(1,cycles+1):\n        p_cycle_name = 'cycle'+str(cycle-1)\n        c_cycle_name = 'cycle'+str(cycle)\n        \n        error = values[p_cycle_name]-values['y']\n        path_value_b = sum(error)\n        path_value_m = sum(error * values['x'])\n        \n        if cycle > 1:\n            path_value_b = (beta) * weights[p_cycle_name].pvb + (1-beta) * path_value_b\n            path_value_m = (beta) * weights[p_cycle_name].pvm + (1-beta) * path_value_m\n        \n        new_m = weights[p_cycle_name].m - path_value_m * adj_alpha\n        new_b = weights[p_cycle_name].b - path_value_b * adj_alpha\n        \n        weights[c_cycle_name] = [new_m,\n                                new_b,\n                                path_value_m,\n                                path_value_b]\n        \n        y_pred = weights[c_cycle_name].m*values['x'] + weights[c_cycle_name].b\n        \n        values[c_cycle_name] = y_pred\n        \n    return weights,values\n\nweights, values = gradient_descent_momentum(xs,ys,alpha,cycles,beta)\n\ngraph_gradient_descent(values,cycles,12,3)\n\n\n\n\n\n\n\n7 RMSProp\n\n7.0.1 Concept\nThe idea of RMS prop is that we will adjust out learning rate based on how large our error rate it. If we have a very large error, we will take a larger step. With a smaller error, we will take a smaller step. This minimizes the changces that we overshoot the ideal weights. This is accomplished by diving the learning rate by an weighted exponential average of the previous path values. To control what kind of weighted average is used, we define \\(\\beta\\).\nThis is useful and effective because we want to have very large steps early on with a big learning rate, but the closer we get to the optimal values the lower we want our learning rate to be. This is exactly what RMS prop does - adjust out learning rate based on our error rate. This allows for larger steps with a bigger learning rate while minimizing the risk of our gradient descent going out of control. It has similar benefits of momentum, but approaches it by modifying the learning rath rather than the path value.\n\n\n7.0.2 What is different\nWhat is different: we have an alpha_multiplier for each variable that is calculated each cycle. When calculating the new value, we divide our learning rate \\(\\alpha\\) by the square root of this alpha multiplier. The alpha multiplier uses a new input, \\(\\beta\\)\nOur alpha multiplier is calculated with this formula.\n\\(alphamultiplier_b = (\\beta)(alphamultiplier_{b_{previous}}) + (1 - \\beta)((y_{pred}-y_{obs})^2)\\)\n\\(alphamultiplier_m = (\\beta)(alphamultiplier_{m_{previous}}) + (1 - \\beta)(x(y_{pred}-y_{obs})^2)\\)\n\nHere’s the path value for our Regular Gradient Descent\n\n\\(new_m = m - PathValue_m * \\frac{\\alpha}{n}\\)\n\\(new_b = b - PathValue_b * \\frac{\\alpha}{n}\\)\n\nHere’s the path value for our RMS Prop\n\n\\(new_m = m - PathValue_m * \\frac{\\alpha}{n\\sqrt{alphamultiplier_m}}\\)\n\\(new_b = b - PathValue_b * \\frac{\\alpha}{n\\sqrt{alphamultiplier_b}}\\)\n\n\n\n8 Python Function\n\nbeta = 0.9\nalpha = 500\n\ndef gradient_descent_momentum(xs,ys,alpha,cycles,beta):\n    n = len(xs)\n    adj_alpha = (1/n)*alpha\n    \n    values = pd.DataFrame({'x':xs,'y':ys})\n    weights = pd.DataFrame({'cycle0':[1,1,0,0,0,0]},index=['m','b','pvm','pvb','am_m','am_b'])\n    values['cycle0'] = weights['cycle0'].m*values['x'] + weights['cycle0'].b\n    \n    for cycle in range(1,cycles+1):\n        p_cycle_name = 'cycle'+str(cycle-1)\n        c_cycle_name = 'cycle'+str(cycle)\n        \n        error = values[p_cycle_name]-values['y']\n        path_value_b = sum(error)\n        path_value_m = sum(error * values['x'])\n\n        alpha_multiplier_b = abs(path_value_b)**2\n        alpha_multiplier_m = abs(path_value_m)**2\n        \n        if cycle > 1:\n            alpha_multiplier_b = (beta) * weights[p_cycle_name].am_b + (1-beta) * alpha_multiplier_b\n            alpha_multiplier_m = (beta) * weights[p_cycle_name].am_m + (1-beta) * alpha_multiplier_m\n                \n        new_m = weights[p_cycle_name].m - path_value_m * adj_alpha / math.sqrt(alpha_multiplier_m)\n        new_b = weights[p_cycle_name].b - path_value_b * adj_alpha / math.sqrt(alpha_multiplier_b)\n        \n        weights[c_cycle_name] = [new_m,\n                                new_b,\n                                path_value_m,\n                                path_value_b,\n                                alpha_multiplier_m,\n                                alpha_multiplier_b]\n        \n        y_pred = weights[c_cycle_name].m*values['x'] + weights[c_cycle_name].b\n        \n        values[c_cycle_name] = y_pred\n        \n    return weights,values\n\nweights, values = gradient_descent_momentum(xs,ys,alpha,cycles,beta)\ngraph_gradient_descent(values,cycles,15,2)\n\n\n\n\n\n\n9 Adam\n\n9.0.1 Concept\nThe idea of Adam is that there are really nice properties to RMS Prop as well as momentum, so why not do both at the same time. We will modify our path value using the momentum formula and we will modify our learning rate using RMSProp formula. To control what kind of weighted average is used, we define beta_rmsprop and beta_momentum. We can have a pretty big learning rate without overshooting.\nThis is useful and effective because we want the ability to pick up speed like momentum does, but also want to minimize overshooting. Basically we pick up momementum when we are far from the minimum, but we slow down as we get close before we overshoot.\n\nbeta_rmsprop = 0.9\nbeta_momentum = 0.7\nalpha = 200\n\ndef gradient_descent_momentum(xs,ys,alpha,cycles,beta_rmsprop,beta_momentum):\n    n = len(xs)\n    adj_alpha = (1/n)*alpha\n    \n    values = pd.DataFrame({'x':xs,'y':ys})\n    weights = pd.DataFrame({'cycle0':[1,1,0,0,0,0]},index=['m','b','pvm','pvb','am_m','am_b'])\n    values['cycle0'] = weights['cycle0'].m*values['x'] + weights['cycle0'].b\n    \n    for cycle in range(1,cycles+1):\n        p_cycle_name = 'cycle'+str(cycle-1)\n        c_cycle_name = 'cycle'+str(cycle)\n        \n        error = values[p_cycle_name]-values['y']\n        path_value_b = sum(error)\n        path_value_m = sum(error * values['x'])\n        \n        if cycle > 1:\n            path_value_b = (beta_momentum) * weights[p_cycle_name].pvb + (1-beta_momentum) * path_value_b\n            path_value_m = (beta_momentum) * weights[p_cycle_name].pvm + (1-beta_momentum) * path_value_m\n            \n        alpha_multiplier_b = abs(path_value_b)**2\n        alpha_multiplier_m = abs(path_value_m)**2\n        \n        if cycle > 1:\n            alpha_multiplier_b = (beta_rmsprop) * weights[p_cycle_name].am_b + (1-beta_rmsprop) * alpha_multiplier_b\n            alpha_multiplier_m = (beta_rmsprop) * weights[p_cycle_name].am_m + (1-beta_rmsprop) * alpha_multiplier_m\n                \n        new_m = weights[p_cycle_name].m - path_value_m * adj_alpha / math.sqrt(alpha_multiplier_m)\n        new_b = weights[p_cycle_name].b - path_value_b * adj_alpha / math.sqrt(alpha_multiplier_b)\n        \n        weights[c_cycle_name] = [new_m,\n                                new_b,\n                                path_value_m,\n                                path_value_b,\n                                alpha_multiplier_m,\n                                alpha_multiplier_b]\n        \n        y_pred = weights[c_cycle_name].m*values['x'] + weights[c_cycle_name].b\n        \n        values[c_cycle_name] = y_pred\n        \n    return weights,values\n\nweights, values = gradient_descent_momentum(xs,ys,alpha,cycles,beta_rmsprop,beta_momentum)\ngraph_gradient_descent(values,cycles,15,2)"
  },
  {
    "objectID": "posts/GenerativeModels/StyleGanComponents.html",
    "href": "posts/GenerativeModels/StyleGanComponents.html",
    "title": "StyleGAN Components",
    "section": "",
    "text": "1 Intro\nIn this post I will cover several components needed for style GAN and build a basic one using those blocks. I am not going to train it, or try to make a state of the art one. Things will be simplified to the simplest form possible to understand the concept. If you want to understand some of the key techniques used in modern SoTA GANs, this is the post for you!\nA future post will be actually building a real StyleGAN model to produce high def images to show practical GANs in full size networks to create actual images. Reading this post first is highly recommended!\nInspiration for this post came from the deeplearning.ai GAN specialization. For more complete information on GANs in a structured course, check that course out!\n\n\n2 Truncated Noise\nThe first is an easy one to get us warmed up. This is not something used during training, but rather a technique you can use after training to control the diversity-quality trade-off when you generate images.\nGenerators work by taking in random noise. The random noise can be thought of a a random seed that the generators create images from. Normally we sample from the normal distribution. If you look at the normal distribution graph below you will realize that some values will be selected a lot, while others will be selected pretty rarely. If a value is in the tail, it will be selected much less frequently than a value close to the mean. So what does that mean? It means that for those particular values there will have been fewer examples to train with and will thus probably will result in lower quality images.\nIn order to deal with this issue we can truncate the normal distribution to sample from only the higher frequency areas. The reason this is a trade-off is because if we have fewer possible values (starting points), that mean means fewer possible images can be generated. In other words we will have less diverse outputs.\nSo the key things to know are:\n\nTruncated Normal Distribution just cuts off values on each each based on some set parameter\nLeft graph shows normal distribution - right graphs show different levels of truncation\nThere is a diversity/quality trade-off that this technique allows you to make\nGraphs are most diversity in output images to least diversity from left to right\nGraphs are lowest quality images to highest quality images from left to right\n\n\n\n\n\n\n\n\n3 Noise to Weight Mapping\nThe next component is a noise to weight mapping. A generator gets a noise vector of random values from the normal distribution. This may be problematic. Not all of our features will follow the normal distribution - so trying to map normal distribution values to various features that follow other distributions gets messy.\nThis is especially problematic because we want to be able to independently control features in the output image. I don’t want to modify the direction the eyes are looking and have that also change facial features. I want to be able to tweak components without having a tangled mess of mappings.\nTo fix this we learn the distributions that are ideal for the noise vector. So random noise comes in, it gets passed through a Mapping Network and we end with a weight matrix w. Since a neural network can approximate any function, that means it can approximate any distribution so this should work. This lets your model learn represent things in a cleaner way and makes your mapping much less tangled so you can control features much easier.\nThe mapping network in StyleGAN is composed of 8 layers - but here’s a simplified version just to get the idea that it’s just a normal neural network that is mapping the noise vector (z) to the weights vector (w).\n\nclass MappingNetwork(nn.Module):\n    def __init__(self, z_dim, hidden_dim, w_dim):\n        super().__init__()\n\n        self.mapping = nn.Sequential(\n            nn.Linear(z_dim,      hidden_dim),  nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),  nn.ReLU(),\n            nn.Linear(hidden_dim, w_dim))\n\n    def forward(self, noise): return self.mapping(noise)\n    \nMappingNetwork(100,200,75)\n\nMappingNetwork(\n  (mapping): Sequential(\n    (0): Linear(in_features=100, out_features=200, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=200, out_features=200, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=200, out_features=75, bias=True)\n  )\n)\n\n\n\n\n4 Noise Injection\nNext, we need a process for injecting random noise in various parts of the network. This is different than the weight vector we created above. We inject this additional noise to increase diversity. The way this works:\n\nCreate 1 weight for each channel (learned)\nCreate a noise tensor of random numbers the same size as your image, but with only 1 channel (random)\nMultiply noise tensor by each of those values so you end with something same dimension as image and add this to the image\n\nThat outputs the new image that includes the noise can continue down the network. Nothing special needs to happen other than this because we didn’t change any dimensions. Really it’s just a linear layer with random noise in it. You can see below that then image shape and the final shape are identical.\nThis happens in many places in the network before every AdaIN layer. So let’s see what the AdaIN layer is.\n\nclass InjectNoise(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(channels)[None, :, None, None])\n\n    def forward(self, image):\n        noise_shape = (image.shape[0],1,image.shape[2],image.shape[3]) \n        noise = torch.randn(noise_shape, device=image.device)\n        out = image + self.weight * noise\n        print(f'Image (input)                     {image.shape}')        \n        print(f'Weight (step 1):                  {self.weight.shape}')\n        print(f'Noise (step 2):                   {noise.shape}')\n        print(f'weight * noise + image (ouput):   {out.shape}')\n        return out\ntmp = InjectNoise(512)(torch.randn(32,512,4,4))\n\nImage (input)                     torch.Size([32, 512, 4, 4])\nWeight (step 1):                  torch.Size([1, 512, 1, 1])\nNoise (step 2):                   torch.Size([32, 1, 4, 4])\nweight * noise + image (ouput):   torch.Size([32, 512, 4, 4])\n\n\n\n\n5 Adaptive Instance Normalization (AdaIN)\nTo recap what we have so far:\n\nAn image that has random noise injected into it from the Noise Injection Step\nA transformed noise matrix from our mapping network w\n\nWe need to combine these and we need some sort of normalization. That is what this Adaptive Instance normalization is going to do. Just like the noise injection happens in many places in the network.\nAs previously mentioned, injecting w rather than just normally distributed noise gives us more control over the images generated. We are going to take our image after normalization, multiply it by a scale from the weight matrix and add a shift also from the weight matrix. Put another way, another linear layer. So in summary what we need to do is:\n\nNormalize the image\nUse a linear layer to map w to 1 value per channel to give us a scale tensor\nUse a linear layer to map w to 1 value per channel to give us a shift tensor\noutput style_tensor * normalized_image  + shift_tensor\n\nTake a look below at some the code for what it does and the shapes to understand the inputs and outputs. Input and output size is the same, but with normalization and injected weight tensor!\n\nclass AdaIN(nn.Module):\n\n    def __init__(self, channels, w_dim):\n        super().__init__()\n        self.instance_norm = nn.InstanceNorm2d(channels)\n        self.scale_transform = nn.Linear(w_dim, channels)\n        self.shift_transform = nn.Linear(w_dim, channels)\n\n    def forward(self, image, w):\n        normalized_image = self.instance_norm(image)\n        scale_tensor = self.scale_transform(w)[:, :, None, None]\n        shift_tensor = self.shift_transform(w)[:, :, None, None]\n        transformed_image = scale_tensor * normalized_image + shift_tensor\n        \n        print(f'Image (input)                       {image.shape}')        \n        print(f'normalized_image (step 1)           {normalized_image.shape}')        \n        print(f'w (input)                           {w.shape}')        \n        print(f'scale (step 2):                     {scale_tensor.shape}')\n        print(f'shift (step 3):                     {shift_tensor.shape}')\n        print(f'scale * norm_image + shift (ouput): {transformed_image.shape}')\n        \n        return transformed_image\ntmp = AdaIN(512,256)(torch.randn(32,512,4,4),torch.randn(32,256))\n\nImage (input)                       torch.Size([32, 512, 4, 4])\nnormalized_image (step 1)           torch.Size([32, 512, 4, 4])\nw (input)                           torch.Size([32, 256])\nscale (step 2):                     torch.Size([32, 512, 1, 1])\nshift (step 3):                     torch.Size([32, 512, 1, 1])\nscale * norm_image + shift (ouput): torch.Size([32, 512, 4, 4])\n\n\n\n\n6 Progressive Growing\nNow there’s one last piece we need to understand the main components of styleGAN. Progressive growing is just what it sounds like. The generator will create a small image and progressively grow the size. It doubles the image in size until getting the image to the required size. This allows for higher quality and resolution photos.\nIntuitively this makes sense. It’d be much harder to generate an entire picture all at once that all meshes well together. Instead we put basic structures and build on it slowly by filling in more and more fine details over time as the image you are generating increases in size.\nSo let’s jump into it. Let’s create a re-usable block to implement this using the other components as well. Here’s what we need:\n\nAn upsampling layer (for progressive growing)\nA convolutional layer (standard for image problems)\nRandom noise injection (we created that above)\nAn AdaIN layer (we created that above)\nAn activation (just like all neural networks need)\n\n\nclass MinifiedStyleGANGeneratorBlock(nn.Module):\n\n    def __init__(self, in_chan, out_chan, w_dim, kernel_size, starting_size, use_upsample=True):\n        super().__init__()\n        self.use_upsample = use_upsample\n        if self.use_upsample: self.upsample = nn.Upsample((starting_size), mode='bilinear')\n        self.conv = nn.Conv2d(in_chan, out_chan, kernel_size, padding=1) \n        self.inject_noise = InjectNoise(out_chan)\n        self.adain = AdaIN(out_chan, w_dim)\n        self.activation = nn.LeakyReLU(0.2)\n\n    def forward(self, x, w):\n        if self.use_upsample: x = self.upsample(x) # upsample        (step 1)\n        x = self.conv(x)                           # conv layer      (step 2)\n        x = self.inject_noise(x)                   # noise injection (step 3)        \n        x = self.activation(x)                     # activation      (step 4)     \n        x = self.adain(x, w)                       # AdaIN           (step 5)\n        return x\n\nNow, you can implement progressive growing and put it all together. Let’s see how that works in StyleGAN. As you can see we move from an 8x8 image to a 16x16 image. StyleGAn will do this many times.\nKeep in mind all of this is simplified and scaled down from what is in StyleGAN. The purpose of this blog was to communicate the core concepts and techniques used in StyleGAN, not necessarily show the practical applications. Stay tuned for a blog that shows practical application of these concepts!\n\n\nCode\nclass AdaIN(nn.Module):\n    def __init__(self, channels, w_dim):\n        super().__init__()\n        self.instance_norm = nn.InstanceNorm2d(channels)\n        self.scale_transform = nn.Linear(w_dim, channels)\n        self.shift_transform = nn.Linear(w_dim, channels)\n\n    def forward(self, image, w):\n        normalized_image = self.instance_norm(image)\n        scale_tensor = self.scale_transform(w)[:, :, None, None]\n        shift_tensor = self.shift_transform(w)[:, :, None, None]\n        transformed_image = scale_tensor * normalized_image + shift_tensor\n        return transformed_image\n      \nclass InjectNoise(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(channels)[None, :, None, None])\n\n    def forward(self, image):\n        noise_shape = (image.shape[0],1,image.shape[2],image.shape[3]) \n        noise = torch.randn(noise_shape, device=image.device)\n        out = image + self.weight * noise\n        return out\n\n\n\nclass MinifiedStyleGANGenerator(nn.Module):\n\n    def __init__(self, z_dim, map_hidden_dim, w_dim, in_chan, out_chan, kernel_size, hidden_chan):\n        super().__init__()\n        self.map = MappingNetwork(z_dim, map_hidden_dim, w_dim)\n        self.sc = nn.Parameter(torch.randn(1, in_chan, 4, 4))\n        self.block0 = MinifiedStyleGANGeneratorBlock(in_chan, hidden_chan, w_dim, kernel_size, 4)\n        self.block1 = MinifiedStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, 8)\n        self.block2 = MinifiedStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, 16)\n        self.block1_to_image = nn.Conv2d(hidden_chan, out_chan, kernel_size=1)\n        self.block2_to_image = nn.Conv2d(hidden_chan, out_chan, kernel_size=1)\n\n    def upsample_to_match_size(self, smaller_image, bigger_image):\n        return F.interpolate(smaller_image, size=bigger_image.shape[-2:], mode='bilinear')\n\n    def forward(self, noise, return_intermediate=False):\n        w = self.map(noise) # This is our mapping network going from noise -> w\n        x = self.block0(self.sc, w) # w from mapping network is input here\n        \n        x1 = self.block1(x, w) # w noise from mapping network is input here also\n        image1 = self.block1_to_image(x1)\n        print(f'ImageSize1      {image1.shape}')\n        \n        x2 = self.block2(x1, w) # w noise from mapping network is input here also\n        image2 = self.block2_to_image(x2)\n        print(f'ImageSize2      {image2.shape}')\n        \n        x1_upsample = self.upsample_to_match_size(image1, image2)\n        return 0.2 * (image2) + 0.8 * (x1_upsample)  \n      \ntmp = MinifiedStyleGANGenerator(z_dim=128, map_hidden_dim=1024,w_dim=496,in_chan=512,out_chan=3, kernel_size=3, hidden_chan=256)(get_truncated_noise(10, 128, 0.7))\n\nImageSize1      torch.Size([10, 3, 8, 8])\nImageSize2      torch.Size([10, 3, 16, 16])"
  },
  {
    "objectID": "posts/GenerativeModels/CycleGanWalkThrough.html",
    "href": "posts/GenerativeModels/CycleGanWalkThrough.html",
    "title": "CycleGAN Walk Through",
    "section": "",
    "text": "In this post I will build on my previous posts on GANs and talk about CycleGAN.\nIn StyleGAN, we took noise and generated an image realistic enough to fool the discriminator. In CycleGAN we take an image and modify it to a different class to make that modified image realistic enough to fool the discriminator into believing it’s that class.\nI am going to walk through a great Pytorch CycleGAN implementation and explain what the pieces are doing in plain english so anyone can understand the important bits without diving through lots of code or reading an academic paper.\nBefore we jump in - here’s the three most important pieces to CycleGAN to understand if you want to skip to the most crucial bits. I labeled the key sections in the Table of Contents for you.\nThese will be explained in detail as we go so don’t worry if that doesn’t completely make sense just yet. It will :)\nSo let’s get started!"
  },
  {
    "objectID": "posts/GenerativeModels/CycleGanWalkThrough.html#discriminator---key-1",
    "href": "posts/GenerativeModels/CycleGanWalkThrough.html#discriminator---key-1",
    "title": "CycleGAN Walk Through",
    "section": "3.1 Discriminator - Key 1",
    "text": "3.1 Discriminator - Key 1\nThe most important thing to understand about any model is what it’s predicting. Let’s take a look at the last thing that is done before it’s output and understand that first.\n\navg_pool2d: At the end there’s average pooling, which is just calculated averages in different patches of the feature map. So really what we are predicting is not whether the image is real or fake, but splitting the image into lots of pieces and determining if each piece individually is real or fake.\n\nThis gives the generator much more information to be able to optimize to. Predicting whether an image is real or fake is much easier than generating a whole image - so we want to help the generator as much as possible.\nIf you think about this intuitively - it makes perfect sense. If you were trying to draw a realistic still life and you showed it to an expert artist for feedback what kind of feedback would you like? Would you like them to tell you it looks real or looks fake and leave it at that? Or would you get more out of them breaking the painting into pieces, telling you what portions are realistic and what portions need more work? Of course, the latter is more helpful so that’s what we predict for the generator.\nThe rest of the discriminator is nothing special but let’s dive in a bit to prove that. Here’s the components:\n\nConv2d: When working with images convolutions are very common\nLeakyReLU: While ReLU is more common, Leaky ReLU is used. We don’t want the model to get stuck in a ‘no-training’ zone that exists with ReLU. GANs are harder to train well because of the additional complexity of the adversarial model so LeakyReLU works better on GANs general.\nInstanceNorm2d: BatchNorm is more common, but this is just a small tweak from that. If you think about the different meanings of the word “Instance” vs “Batch” you make be able to guess what the difference is. In short BatchNorm is normalizing across the entire batch (computing 1 mean/std). InstanceNorm is normalizing over the individual image (instance), so you have a mean and std for each image.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you think through the impact of batch vs Instance normalization you may realize that with BatchNorm the training for a particular image is effected by which images happen to be in the same batch. This is because the mean and standard deviation are calculated across the entire batch, rather than for that image alone.\n\n\n\n\nCode\nclass Discriminator(nn.Module):\n    def __init__(self, input_nc):\n        super(Discriminator, self).__init__()\n\n        # A bunch of convolutions one after another\n        model = [   nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n                    nn.LeakyReLU(0.2, inplace=True) ]\n\n        model += [  nn.Conv2d(64, 128, 4, stride=2, padding=1),\n                    nn.InstanceNorm2d(128), \n                    nn.LeakyReLU(0.2, inplace=True) ]\n\n        model += [  nn.Conv2d(128, 256, 4, stride=2, padding=1),\n                    nn.InstanceNorm2d(256), \n                    nn.LeakyReLU(0.2, inplace=True) ]\n\n        model += [  nn.Conv2d(256, 512, 4, padding=1),\n                    nn.InstanceNorm2d(512), \n                    nn.LeakyReLU(0.2, inplace=True) ]\n\n        # FCN classification layer\n        model += [nn.Conv2d(512, 1, 4, padding=1)]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        x =  self.model(x)\n        # Average pooling and flatten\n        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)"
  },
  {
    "objectID": "posts/GenerativeModels/CycleGanWalkThrough.html#generator",
    "href": "posts/GenerativeModels/CycleGanWalkThrough.html#generator",
    "title": "CycleGAN Walk Through",
    "section": "3.2 Generator",
    "text": "3.2 Generator\nThe Generator is what generates the image. It’s got a lot of the same components as other Neural Networks. Let’s talk about the components.\nLet’s break this apart and talk about each piece briefly.\n\n3.2.1 Initial Layer\nSo this is the code from the implementation for the first bit of the generator (I cut off the rest to be shown later). Let’s understand this first.\nWe see all the same components we say above. Conv2d is doing convolutions (big 7x7 ones), we also have InstanceNorm like we saw in the Discriminator (discussed above), and a common activation function ReLU.\nThe new thing is this ReflectionPad2d.\n\nclass Generator(nn.Module):\n    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n        super(Generator, self).__init__()\n\n        # Initial convolution block       \n        model = [   nn.ReflectionPad2d(3),\n                    nn.Conv2d(input_nc, 64, 7),\n                    nn.InstanceNorm2d(64),\n                    nn.ReLU(inplace=True) ]\n\nSo what is ReflectionPad2d? First, let’s look at what a convolution does. The blue in the gif below is the image, the white squares are padding. Normally they’re padded with nothing like in the illustration. What ReflectionPad does is pads that with a reflection of the image instead. In other words, we are using the pixels values of pixels on the edge to pad instead of just a pure white or pure black pixel.\n\n\n\n\n\n\nNote\n\n\n\nFore more on convolutions go here. The gif below comes from that guide by Sumit Saha and the guide contains a lot of other create information.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCredit for Visualization: Vincent Dumoulin, Francesco Visin - A guide to convolution arithmetic for deep learning\n\n\n\n\n3.2.2 Downsampling\nWe then go through several downsampling layers. A 3x3 convolution with stride 2 will result in a smaller feature map, which is exactly what we are doing to cause the downsampling. It’s all the usual suspects through: convolutions, InstanceNorms, and ReLUs.\n\n        # Downsampling\n        in_features = 64\n        out_features = in_features*2\n        for _ in range(2):\n            model += [  nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n                        nn.InstanceNorm2d(out_features),\n                        nn.ReLU(inplace=True) ]\n            in_features = out_features\n            out_features = in_features*2\n\n\n\n3.2.3 Residual Blocks\nNext we go through some residual blocks.\n\n        # Residual blocks\n        for _ in range(n_residual_blocks):\n            model += [ResidualBlock(in_features)]\n\nWhen we look at residual blocks again, it’s all the same components in slightly different configurations as above. We have ReflectionPad, Convolutions, InstanceNorm, and ReLUs.\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n\n        conv_block = [  nn.ReflectionPad2d(1),\n                        nn.Conv2d(in_features, in_features, 3),\n                        nn.InstanceNorm2d(in_features),\n                        nn.ReLU(inplace=True),\n                        nn.ReflectionPad2d(1),\n                        nn.Conv2d(in_features, in_features, 3),\n                        nn.InstanceNorm2d(in_features)  ]\n\n        self.conv_block = nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        return x + self.conv_block(x)\n\n\n\n3.2.4 Upsampling\nNext is upsampling. There is a new component here which ConvTranspose. Let’s take a look at what that is exactly.\n\n        out_features = in_features//2\n        for _ in range(2):\n            model += [  nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n                        nn.InstanceNorm2d(out_features),\n                        nn.ReLU(inplace=True) ]\n            in_features = out_features\n            out_features = in_features//2\n\nSo what is this? Well essentially it’s a normal convolution that upsamples by creating padding between cells. Here’s a visual that shows what that looks like.\n\n\n\n\n\n\n\nNote\n\n\n\nCredit for Visualization: Vincent Dumoulin, Francesco Visin - A guide to convolution arithmetic for deep learning\n\n\n\n\n3.2.5 Output Layer\nFinally we have out output layer with a Tanh activation function.\n\n        model += [  nn.ReflectionPad2d(3),\n                    nn.Conv2d(64, output_nc, 7),\n                    nn.Tanh() ]\n\n        self.model = nn.Sequential(*model)"
  },
  {
    "objectID": "posts/GenerativeModels/CycleGanWalkThrough.html#discriminator-loss",
    "href": "posts/GenerativeModels/CycleGanWalkThrough.html#discriminator-loss",
    "title": "CycleGAN Walk Through",
    "section": "4.1 Discriminator Loss",
    "text": "4.1 Discriminator Loss\nThe discriminator loss is a standard adversarial loss. Let’s think through what we would need:\n\nReal images of a class (ie Summer Yosimite pictures)\nFake images of a class (ie generated Summer Yosimite pictures)\nDiscriminator predictions for whether each section of the image is real or fake\n\nSo let’s say we generated the images with our generator and then we took the real images from our batch, the fake generated images, and ran that through our discriminator. Once we have that we use Mean Squared Error as the loss function.\nLet’s see how this works. Everything is duplicated because we have 2 discriminators.\nDiscriminator 1: Is each section of this Class A image real or fake?\n\n        pred_real = netD_A(real_A) # Predict whether real image is real or fake\n        loss_D_real = criterion_GAN(pred_real, target_real) \n\n        pred_fake = netD_A(fake_A.detach()) # Predict whether fake image is real or fake\n        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n\n        loss_D_A = (loss_D_real + loss_D_fake)*0.5 # Total loss\n        loss_D_A.backward() # backward pass\n\nDiscriminator 2: Is each section of this Class B image real or fake?\n\n        pred_real = netD_B(real_B) # Predict whether real image is real or fake\n        loss_D_real = criterion_GAN(pred_real, target_real) \n\n        pred_fake = netD_B(fake_B.detach()) # Predict whether fake image is real or fake\n        loss_D_fake = criterion_GAN(pred_fake, target_fake) \n\n        loss_D_B = (loss_D_real + loss_D_fake)*0.5 # Total loss\n        loss_D_B.backward() # backward pass"
  },
  {
    "objectID": "posts/GenerativeModels/CycleGanWalkThrough.html#generator-loss---key-2",
    "href": "posts/GenerativeModels/CycleGanWalkThrough.html#generator-loss---key-2",
    "title": "CycleGAN Walk Through",
    "section": "4.2 Generator Loss - Key 2",
    "text": "4.2 Generator Loss - Key 2\nThe generator loss is the key to CycleGAN and it has three main parts to it.\n\nAdversarial Loss: This is standard MSE Loss. This is the most straightforward loss.\n\nIdentity Loss: This is L1 Loss (pixel by pixel comparison to minimize the difference in pixel values). If my generator is trained to take a Summer picture and turn it into a Winter picture and I give it winter picture is should do nothing (identity function). The generator should look at the Winter Picture and determine that nothing needs to be done to make it a Winter picture as that’s what it already is. Identity loss is just trying this out and then comparing the input image with the output image.\nCycle Loss: This is where CycleGAN gets it’s name. L1 loss is just trying to minimize the difference in pixel values. But how does it have images to compare when it’s an unpaired dataset?\n\nStart with class A and run your Generator to create class B out of the Class A image\nTake that class B image that was just generated, and run it through the other generator to create a class A image\nIf all you are doing is transferring styles you should get the exact same image back after the full cycle. Those are the 2 images being compared.\n\n\nThese three components get added up for the loss function. You can add weights to different portions to prioritize different aspects of the loss function.\nSo how does this look all together? You may notice everything is duplicated in the code. That’s because We have 2 generators:\n\nClass A -> Class B or Summer -> Winter\nClass B -> Class A or Winter -> Summer\n\nAdverserial Loss: Is it good enough to fool the discriminator?\nThis is the most straightforward and is standard MSE loss. The generator is optimizing to fool the Discriminator. Specifically the loss is being calculated on the discriminators prediction on fake images and a ‘truth label’ saying it is a real image. We know it’s not actually a real image, but the discriminator wants us to think so.\n\n        fake_B = netG_A2B(real_A) # Generate class B from class A\n        pred_fake = netD_B(fake_B) # Discriminator predict is is real or fake\n        loss_GAN_A2B = criterion_GAN(pred_fake, target_real) # Is discriminator fooled?\n\n        fake_A = netG_B2A(real_B) # Generate class A from class B\n        pred_fake = netD_A(fake_A) # Discriminator predict is is real or fake\n        loss_GAN_B2A = criterion_GAN(pred_fake, target_real) # Is discriminator fooled?\n\nIdentity Loss: Is it making the minimum changes needed?\nIdentity loss is L1 loss (pixel by pixel comparison to minimize the difference in pixel values). If my generator is trained to take a Summer picture and turn it into a Winter picture and I give it winter picture, it should do nothing (identity function). The generator should look at the Winter Picture and determine that nothing needs to be done to make it a Winter picture as that’s what it already is. Identity loss is doing this exactly and comparing the input image with the output image. Since it should change nothing we can calculate the loss as the difference between the pixels.\n\n        same_B = netG_A2B(real_B) # Generate class B from class B\n        loss_identity_B = criterion_identity(same_B, real_B)*5.0 # Pixel Diff\n\n        same_A = netG_B2A(real_A) # Generate class A from class A\n        loss_identity_A = criterion_identity(same_A, real_A)*5.0 # Pixel Diff\n\nCycle loss: Is it only changing style?\nThis is where CycleGAN gets it’s name. Cycle Loss is also an L1 Loss function - let’s take a look at what images it’s comparing. Here’s the process:\n+ Start with a class A image and run your Generator to generate a class B image\n+ Take that generated class B image and run it through the other generator to create a class A image (full cycle)\n+ Compare pixels between that generated Class A image should be identical to the original Class A input image\n+ Repeat in the other direction\nIf the only thing being changed is style then the generated Class A image that went through the full cycle should be identical to the original input Class A image. If however other things are getting changed, then you will have information loss and you the images will be different. By minimizing this pixel difference you are telling the model not to change the general content of the image, it can only change stylistic things.\n\n        recovered_A = netG_B2A(fake_B) # Generate Class A from fake Class B\n        loss_cycle_ABA = criterion_cycle(recovered_A, real_A)*10.0 # Pixel Diff\n\n        recovered_B = netG_A2B(fake_A) # Generate Class B from fake Class A\n        loss_cycle_BAB = criterion_cycle(recovered_B, real_B)*10.0 # Pixel Diff\n\nTotal Generator Loss: Sum them all up into 1 loss function\n\n        # Total loss\n        loss_G = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB # Add all these losses up\n        loss_G.backward() # backward pass"
  },
  {
    "objectID": "posts/GenerativeModels/GANIntroduction.html",
    "href": "posts/GenerativeModels/GANIntroduction.html",
    "title": "GAN Introduction",
    "section": "",
    "text": "This blog is targeted to people wanting a general intuition about GANs. We will talk about the most basic high level concepts to understand, not going to cover how to code one or build one\nYou do not need to understand GANs prior to reading this post. I do assume that you generally are familiar with modeling."
  },
  {
    "objectID": "posts/GenerativeModels/GANIntroduction.html#a-gan-can-enhance-images",
    "href": "posts/GenerativeModels/GANIntroduction.html#a-gan-can-enhance-images",
    "title": "GAN Introduction",
    "section": "2.1 A GAN can enhance images",
    "text": "2.1 A GAN can enhance images\nGoogle Brain did research to show how GANs can be used to enhance images. The left super blurry unrecognizable pictures were given to a GAN. The Middle column is what the GAN made when enhancing the image. The right column is what the image should look like if the GAN was perfect.\n\n\n\nImage Enhancement"
  },
  {
    "objectID": "posts/GenerativeModels/GANIntroduction.html#a-gan-can-change-image-style",
    "href": "posts/GenerativeModels/GANIntroduction.html#a-gan-can-change-image-style",
    "title": "GAN Introduction",
    "section": "2.2 A GAN can change image style",
    "text": "2.2 A GAN can change image style\nWe can also transfer images from one style to another. Whether that’s changing video of a horse to a zerbra or combining photos with art, this medium aricle shows a cool example!\n\n\n\nDog Style Transfer"
  },
  {
    "objectID": "posts/GenerativeModels/GANIntroduction.html#a-gan-can-create-new-images",
    "href": "posts/GenerativeModels/GANIntroduction.html#a-gan-can-create-new-images",
    "title": "GAN Introduction",
    "section": "2.3 A GAN can create new images",
    "text": "2.3 A GAN can create new images\nIn the paper Progressive Growing of GANs for Improved Quality, Stability, and Variation, NVIDIA showed the capability of GANs to create realistic super resolution photos of people that do not exist. These are fictional people made up by the GAN.\n\n\n\nSuper Resolution Images"
  },
  {
    "objectID": "posts/GenerativeModels/GANIntroduction.html#a-gan-can-help-you-draw",
    "href": "posts/GenerativeModels/GANIntroduction.html#a-gan-can-help-you-draw",
    "title": "GAN Introduction",
    "section": "2.4 A GAN can help you draw",
    "text": "2.4 A GAN can help you draw\nNVIDIA again shows a really cool video of how basic sketches can be turned into realistic photos. I can imagine how this could help people create art, visualize designs, and more!"
  },
  {
    "objectID": "posts/GenerativeModels/GANIntroduction.html#a-gan-can-compose-music",
    "href": "posts/GenerativeModels/GANIntroduction.html#a-gan-can-compose-music",
    "title": "GAN Introduction",
    "section": "2.5 A GAN can compose music",
    "text": "2.5 A GAN can compose music\nAnother example is this song that was composed by AI. The lyrics is a person, but the instrumentation is AI - a great example of Machine-Human collaboration. You can see the GAN understood basic musical phrasing, hits, understood it can build to hits and go quiet for a couple beats before a large hit to add impact. If I didn’t know, I wouldn’t have realized is was using AI"
  },
  {
    "objectID": "posts/GenerativeModels/GANIntroduction.html#how-they-train-together",
    "href": "posts/GenerativeModels/GANIntroduction.html#how-they-train-together",
    "title": "GAN Introduction",
    "section": "3.1 How they train together",
    "text": "3.1 How they train together\nThere is a big loop where they pass information back and forth an dlearn. Here’s generally how it works\n\n\n\nGAN Training Loop"
  },
  {
    "objectID": "posts/GenerativeModels/GANIntroduction.html#co-learning",
    "href": "posts/GenerativeModels/GANIntroduction.html#co-learning",
    "title": "GAN Introduction",
    "section": "4.1 Co-learning",
    "text": "4.1 Co-learning\nAs these models learn together they need to be evenly match in terms of skill. This can be especially challenging because the critic has a much easier job. Think about it. You paint a fake Monet and I will determine whether it’s a real Monet or a fake. Who do you think will be more competent at their task? Clearly painting the Monet is the much harder job.\nSo what can we do about it? The simplest 2 approaches are:\n\nSet how many times the generator gets updated vs the critic.\nSet the learning rates different for the generator vs the critic"
  },
  {
    "objectID": "posts/GenerativeModels/GANIntroduction.html#mode-collapse",
    "href": "posts/GenerativeModels/GANIntroduction.html#mode-collapse",
    "title": "GAN Introduction",
    "section": "4.2 Mode Collapse",
    "text": "4.2 Mode Collapse\nMode collapse happens when the generator finds a weakness in the critic and exploits it. For example, the generator might do really well with golden retrievers - so rather than making all types of dogs is just learns to make lots of golden retrievers."
  },
  {
    "objectID": "posts/GenerativeModels/GANIntroduction.html#conditional-gans",
    "href": "posts/GenerativeModels/GANIntroduction.html#conditional-gans",
    "title": "GAN Introduction",
    "section": "5.1 Conditional GANs",
    "text": "5.1 Conditional GANs\nA conditional GAN is where you can tell it what kind of image you want. For example if you are generating different dog breeds, you tell the GAN you want a specific breed (ie Golden Retriever). The way this works:\n\nThe Generator is given a specific class to generate data for.\nThe Critic determines whether is is real or fake data for that class. For example rather than “Predict if this is a real dog or not” it’s “Predict if this is a real golden retriever or not”. In order to fool the critic, the generator now has to not just create a dog - but the right species of dog. The generator could predict a perfect image of a pitbull, but it would be easy for the critic to determine that it’s a not a real golden retriever as pitbulls look completely different!"
  },
  {
    "objectID": "posts/GenerativeModels/GANIntroduction.html#controllable-gans",
    "href": "posts/GenerativeModels/GANIntroduction.html#controllable-gans",
    "title": "GAN Introduction",
    "section": "5.2 Controllable GANs",
    "text": "5.2 Controllable GANs\nA Controllable GAN allows you to control different aspects of the image. For example, I want to be able to take an image and tell it to generate the same image but add a beard. Or generate the same image but make the person look older.\nA bit of background and how it’s accomplished: A generator creates data from random noise vectors. These random noise vectors can be thought of as random seeds in a sense. If I give the generator the exact same vector of random numbers, it will generate the exact same data. So those random number translate to output features in the data, so you can figure out how they map and then tweak away!\nHere’s an example of what Photoshop is working on when it comes to controllable GANs."
  },
  {
    "objectID": "testimonials.html",
    "href": "testimonials.html",
    "title": "Testimonials and Kind Words",
    "section": "",
    "text": "Quote by Hamel Husain\n\n\n\n Two underrated @fastdotai people I have met recently:\n\n@isaac_flath\n@wasimlorgat\n\nBoth are super thoughtful, talented, proactive, detailed oriented, and use excellent engineering practices.\nHighly recommend supporting, funding, hiring etc. these people ✨\nI don’t even know if they are looking for support. Just spreading the 😍\nSometimes you find hidden gems of people in open source that really impress you \nLink to Original\n\n\n\n\n\n\n\n\nQuote by Zach Mueller\n\n\n\n If you’re getting used to Quarto and the new #nbdev format, @isaac_flath’s simple QuartoTemplates has been a helpful goldmine of the small nits to do in a Jupyter Notebook with Quarto and I constantly have it open in a tab as I work. Well worth a ⭐️ https://github.com/Isaac-Flath/QuartoTemplates/tree/main \nLink to Original\n\n\n\n\n\n\n\n\nQuote by Vishnu SubramanianFounder of Jarvis Labs\n\n\n\n 📅 Jan - Waiting eagerly for our 1st recharge. Got our 1st $20 🎉🎉.\n📅 Mar - Upgraded network, added key features, and got listed in @fastdotai, Thanks to @jeremyphoward and fastai community for being our early adopters special thanks to @isaac_flath @jpclap \nLink to Original\n\n\n\n\n\n\n\n\n\n\n\nQuote by Greg GonsiorVP Analytics & Co-Founder Centriam\n\n\n\n Isaac is a rock star. Having had the privilege of working with him in a day-to-day capacity for several years, I can say that he is among the best and brightest individuals I have ever worked with. His analytics and data sciences chops are top notch, but his ability to combine technical expertise with a deep understanding of people and process to prescribe - and build - the optimal full stack solution is where he stands heads and shoulders above the rest of the pack. His work for Centriam’s clients has moved the needle time and again to drive real world business results! \n\n\n\n\n\n\n\n\nQuote by Michael Sto. Niño\n\n\n\n Isaac is a well-rounded technical person and excellent at product development. He is really good with data analysis, using this skill to formulate a winning product development strategy. I look forward to working with Isaac again in the future. \nLink to Original\n\n\n\n\n\n\n\n\nQuote by Dien Hoa Truong\n\n\n\n\nPragmatic tip for Python Concurrency:\n\nNon-CPU bound ( writing files to disk, downloading ): Threads\nCPU bound ( Transforming - Cleaning Dataset ): Processes\n\nThanks @isaac_flath for your blog post \nLink to Original\n\n\n\n\n\n\n\n\nQuote from private email\n\n\n\n hey Isaac I loved your post at Style GAN Components\nit is excellent,\nwhen do you think you will release the “A future post will be actually building a real StyleGAN model to produce high def images to show practical GANs in full size networks to create actual images”\nlooking forward to it, thank you :)"
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I Learned (TIL)",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Categories\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nNov 14, 2022\n\n\nQuarto Column Formatting\n\n\nQuarto\n\n\n\n\nNov 10, 2022\n\n\nQuarto Conditional Content\n\n\nQuarto\n\n\n\n\nNov 5, 2022\n\n\nRandom State Duplication\n\n\nPython\n\n\n\n\nAug 10, 2022\n\n\nRun Python on Github Actions\n\n\nPython,Github\n\n\n\n\nMay 15, 2021\n\n\nShelve Python Database\n\n\nPython\n\n\n\n\nDec 6, 2020\n\n\nSecrets for Github Actions\n\n\nGithub\n\n\n\n\nJun 25, 2020\n\n\nConnecting Jupyter to EC2\n\n\nCloud\n\n\n\n\nJan 15, 2020\n\n\nJupyter is JSON\n\n\nPython\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Isaac Flath",
    "section": "",
    "text": "About Me 👋\nI’m a data scientist with a varied background who likes doing varied things. It’s also given me a wide range of skills that prove useful in all sorts of surprising ways.\nMy most important ability is understanding strategic goals and being able to translate that into all the detailed steps needed to get there (and seeing what steps are not needed). I developed this skill by getting experience in very different roles in lots of different parts of the business.\nMy primary hobby is dance. I used to teach ballroom dance but now I mostly dance West Coast Swing.\n\n\nMy professional journey in stages 🚀\nMy Start: assembly line worker ➢ assembly line management ➢ assembly line efficiency optimization\nMoving up: business process engineering ➢ product management\nStarting Technical Journey: data analyst ➢ dynamics CRM developer\nTrying New Things: accounting ➢ call center ➢ full-time ballroom dance teacher\nFinding my home ❤️: product owner ➢ machine learning researcher ➢ data science\n\n\n👨‍💻 How to get into data science 👩🏻‍💻\nfastai courses\nTinker with everything. Try to never ask “Would it work if I did X?” or “Can I accomplish X by doing Y?”. Instead, try it and see if it works. It’s worth the time investment.\nDo projects Do projects in parallel with courses, and after courses. Don’t wait. Build stuff. Doing projects should be where you put most of your time.\nTeach to learn Teaching is a great way to learn, and if you have a record of it you build a portfolio at the same time! I am a fan of doing this via blogging for many reasons. See my FAQ for some help getting started!\nKeep at it and be tenacious. There is discomfort with learning any skill. You’ll be frustrated at times. That’s a sign that there’s a concept worth learning and once you do figure it out you’ll have made a big step forward."
  },
  {
    "objectID": "TIL/QuartoConditionalContent.html",
    "href": "TIL/QuartoConditionalContent.html",
    "title": "Quarto Conditional Content",
    "section": "",
    "text": "Today I learned that you can control what markdown renders. This is called conditional content in the Quarto docs.\nWhen I create a quarto notebook I can add full markdown note for myself or other contributors that do not render to the output format, whether that output is a html, pdf, python library, or website. Yoformats it will render to and which it will not.u can control which\n\n\n\n\nNow-a-days all my presentations are really Quarto html outputs that I walk through with clients. I can now add sections of notes for myself that I don’t want the client to see. Lots of things I will use this for such as:\n\nDocumenting some nuance about why I did something that isn’t relevant to business users\nNotes to look into some data issue if the project gets approved\nMarkdown to break my setup into sections (imports, UDF, db connections, etc.)\n\nAnd much more\n\n\n\nnbdev is a tool to write, test, document, and distribute software packages and technical articles — all in one place, your notebook. It takes notebooks and outputs documentation and a python library. This is great because now in the same place I can write development notes or to-do’s right in the place where the code/tests/docs are sourced from. Maybe a note for myself or for other developers in full markdown that won’t get rendered to the user documentation.\n\n\n\nI think it’ll be really nice to have notes on blogs. Sometimes I go back and improve old blogs, or add sections. I try not to let not being “finished” prevent me from posting. Once I have something I think is good it ships, even if there’s more I could do. Because there’s always more I can do. Being able to add the to-do’s to myself for when I come back later will be fantastic!"
  },
  {
    "objectID": "TIL/QuartoColumnFormatting.html",
    "href": "TIL/QuartoColumnFormatting.html",
    "title": "Quarto Column Formatting",
    "section": "",
    "text": "Today I Learned\nToday I learned that it’s very easy to make multi-column layouts in quarto.\nBy having this ability to easily create and control the width of columns in different sections of markdown I can do a lot of nice things. For example, I combined this concept with my favorite quarto feature (callout blocks) to create my testimonials page!\n::: {.columns}\n\n::: {.column width=\"42%\"}\nColumn on the Left\n:::\n\n::: {.column width=\"6%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"42%\"}\nColumn on the right\n:::\n\n:::"
  },
  {
    "objectID": "TIL/PythonScriptGithubActions.html",
    "href": "TIL/PythonScriptGithubActions.html",
    "title": "Run Python on Github Actions",
    "section": "",
    "text": "Today I Learned\nHow to run a python script on a cron using github actions, or on push of a branch.\nKey parts of workflow.yaml\nDefine the how the job runs and what the triggers are\nname: myWorkflow\non:\n   push:\n      branches: [main]\n    schedule:\n      - cron: \"0 0 * * *\" \nDefine system to run on.\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\nCheckout the github repo you are in (so you can access the files)\n      - name: checkout repo content\n        uses: actions/checkout@v2\nSetup Python 3\n      - uses: actions/setup-python@v3\n        with:\n          python-version: \"3.9\"\n          architecture: \"x64\"\nInstall dependencies as needed and run a python script in the repository\n      - name: Install Dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install -Uq git+https://github.com/fastai/fastcore.git\n      - name: execute py script\n        run: |\n          python run.py"
  },
  {
    "objectID": "TIL/SecretsForGithubActions.html",
    "href": "TIL/SecretsForGithubActions.html",
    "title": "Secrets for Github Actions",
    "section": "",
    "text": "Today I Learned\nI always assumed it was possible to pass secrets github actions, but I never really dove into it until now. So today I learned how to add secrets to github for use in the CI/CD for testing. I’m using it with nbdev\n\n\nHow to\n\nIn your repository go to Settings -> Secrets -> Add New Secret and add your secret\nAdd to your workflow.yml file\n\n    env:\n        key: ${{ secrets.your_secret_key }}\n\nAccess in code as a normal environment variable\n\n    import os\n    your_secret_key = os.environ['your_secret_key']"
  },
  {
    "objectID": "TIL/ShelveDB.html",
    "href": "TIL/ShelveDB.html",
    "title": "Shelve Python Database",
    "section": "",
    "text": "Today I Learned\nShelve is dictionary database that stored in a single file without installation needed. It’s like a nosql version of sqlite.\nThere’s some really cool things about shelve:\n\nYou can add to it with the same syntax as dictionaries, so not much new syntax or api to learn or thing about\nIt’s stored in a file but it’s all pickled so read/write pretty fast and is stored on disk\nAnything that can be pickled can be stored as a value. In python, that is most things.\nIt’s nosql and you don’t need to have a set schema."
  },
  {
    "objectID": "TIL/ConnectingJupyterEC2.html",
    "href": "TIL/ConnectingJupyterEC2.html",
    "title": "Connecting Jupyter to EC2",
    "section": "",
    "text": "Today I Learned\nI needed to use an EC2 instance to run a jupyter notebook. Here’s the steps it took to set do it.\n\n\nHow to\n\nLaunch EC2 instance\nSSH into instance\nRun JupyterLab in EC2 jupyter-lab --no-browser --port=8889\nUse ssh for port forwarding -L 8000:localhost:8889\nOpen jupyter on your local browser at localhost:8000"
  },
  {
    "objectID": "TIL/RandomSeedDuplication.html",
    "href": "TIL/RandomSeedDuplication.html",
    "title": "Random State Duplication",
    "section": "",
    "text": "Today I Learned\nYou should be really careful about random state when parallel processing. If you aren’t careful, the random seed will be copied over to other processes and each process will generate the same random numbers! I learned this from a fastai lecture in the 2022 part 2 course.\n\n\n\nImage showing duplicate random numbers being creating on separate processes due to random seeds being accidentally duplicated to multiple processes"
  },
  {
    "objectID": "TIL/JupyterIsJson.html",
    "href": "TIL/JupyterIsJson.html",
    "title": "Jupyter is JSON",
    "section": "",
    "text": "Today I Learned\nJupyter notebooks are JSON files that can be manipulated easily.\nTo read them:\nimport json\nnb = json.load(open(fpath))\nTo clear outputs\nfor cell in nb['cell']:\n    if cell.cell_type=='code': cell['outputs']=[]\nAll normal json and easy to work with when needed. nbdev has lots of functions and tools to make most of this easier too."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Quarto Document Best Practices\n\n\n\n\n\n\n\nEfficiency\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nMeanShift From Scratch\n\n\n\n\n\n\n\nClustering\n\n\n\n\nA deep dive on meanshift clustering with the full algorithm implemented from scratch using pytorch\n\n\n\n\n\n\nNov 10, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nPython Programming Tips\n\n\n\n\n\n\n\nPython\n\n\nProgramming\n\n\n\n\nA list of handy tips and tricks when programming in python\n\n\n\n\n\n\nNov 6, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nK-Means From Scratch\n\n\n\n\n\n\n\nClustering\n\n\n\n\nA deep dive on K-Means where smart initialization and the full algorithm is implemented from scratch using pytorch\n\n\n\n\n\n\nNov 5, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction To Statistical Testing\n\n\n\n\n\n\n\nTime Series\n\n\nTesting\n\n\n\n\nAn introduction to Statistical Testing applied to Stock Trading\n\n\n\n\n\n\nSep 30, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nBootstrapping\n\n\n\n\n\n\n\nAPL\n\n\nData Exploration\n\n\nTesting\n\n\n\n\nSampling and Bootstrapping Statistics in APL\n\n\n\n\n\n\nSep 20, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nSimple Time Series (Stock Prices)\n\n\n\n\n\n\n\nTime Series\n\n\n\n\nAn introduction to basic time series for stock prediction\n\n\n\n\n\n\nAug 5, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nMatrix Multiplication\n\n\n\n\n\n\n\nAPL\n\n\nLinear Algebra\n\n\n\n\nMatrix multiplication deep dive in APL\n\n\n\n\n\n\nJul 10, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nNumerical Linear Algebra Part 1\n\n\n\n\n\n\n\nAPL\n\n\nLinear Algebra\n\n\n\n\nNumerical Linear Algebra Part 1 with translations to APL\n\n\n\n\n\n\nJul 10, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nTabular Data Intro\n\n\n\n\n\n\n\nAPL\n\n\nTabular Modeling\n\n\n\n\nIntroduction to tabular data analysis in APL\n\n\n\n\n\n\nJul 7, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nMy Coding Style\n\n\n\n\n\n\n\nPython\n\n\nProgramming\n\n\n\n\nA summary of what principle guide how I code, and why I don’t do pep8 if left on my own\n\n\n\n\n\n\nJan 1, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Attribution\n\n\n\n\n\n\n\nTabular Modeling\n\n\n\n\nLast Touch, Multi Touch, and Markov Chain Attribution Introduction\n\n\n\n\n\n\nJul 17, 2021\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nPractical Chatbots\n\n\n\n\n\n\n\nNeural Networks\n\n\n\n\nImproving business processes using ML-Based chatbots\n\n\n\n\n\n\nJun 23, 2021\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nMulti-Armed Bandits\n\n\n\n\n\n\n\nTabular Modeling\n\n\n\n\nHow can a Multi Armed Bandit (probabilistic model) help choose the right action\n\n\n\n\n\n\nApr 15, 2021\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nPython Concurrency\n\n\n\n\n\n\n\nPython\n\n\nProgramming\n\n\n\n\nEasy python poncurrency and parallel processing for Data Science\n\n\n\n\n\n\nApr 1, 2021\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nCycleGAN Walk Through\n\n\n\n\n\n\n\nNeural Networks\n\n\nImage Generation\n\n\n\n\nA walkthrough of key components to a pytorch CycleGAN implementation.\n\n\n\n\n\n\nMar 20, 2021\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nStyleGAN Components\n\n\n\n\n\n\n\nNeural Networks\n\n\nImage Generation\n\n\n\n\nShowing the key components to StyleGAN and how & why they work\n\n\n\n\n\n\nMar 1, 2021\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nGAN Introduction\n\n\n\n\n\n\n\nNeural Networks\n\n\nImage Generation\n\n\n\n\nSummarizing the foundational concepts relating to GANs\n\n\n\n\n\n\nFeb 20, 2021\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nPlant Pathology Kaggle\n\n\n\n\n\n\n\nNeural Networks\n\n\n\n\nCreating a top 10 leaderboard submission for the Plant Pathology Image Classification Kaggle Competition\n\n\n\n\n\n\nFeb 15, 2021\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nNeural Networks and XOR\n\n\n\n\n\n\n\nNeural Networks\n\n\n\n\nHow does a neural network transform non-linear data to solve complex problems. Fitting a neural Network to the XOR function.\n\n\n\n\n\n\nJan 9, 2021\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nTabular Modeling\n\n\n\n\n\n\n\nTabular Modeling\n\n\n\n\nHow to approach tabular modeling\n\n\n\n\n\n\nDec 30, 2020\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nPseudo Labeling for Data Cleaning\n\n\n\n\n\n\n\nNeural Networks\n\n\nData Exploration\n\n\n\n\nPseudo labeling for noise reduction and data creation in MNIST\n\n\n\n\n\n\nDec 15, 2020\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nPartial Dependence Plotting\n\n\n\n\n\n\n\nData Exploration\n\n\nTabular Modeling\n\n\n\n\nUA guide for showing the benefits and technique to Partial Dependence Plotting\n\n\n\n\n\n\nNov 20, 2020\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nNLP Recurrent NN Foundations\n\n\n\n\n\n\n\nNeural Networks\n\n\n\n\nWhat is an RNN? How does NLP work?\n\n\n\n\n\n\nAug 22, 2020\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nNLP Tokenization Foundations\n\n\n\n\n\n\n\nNeural Networks\n\n\n\n\nWhat is Tokenization? What is numericalization?\n\n\n\n\n\n\nAug 19, 2020\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nMixup Deep Dive\n\n\n\n\n\n\n\nNeural Networks\n\n\n\n\nWhat is Mixup Data Augmentation really?\n\n\n\n\n\n\nAug 11, 2020\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nNeural Network Foundations (Part 2)\n\n\n\n\n\n\n\nNeural Networks\n\n\n\n\nMNIST multi-class classification from scratch with deep Learning\n\n\n\n\n\n\nJun 21, 2020\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nNeural Network Foundations (Part 1)\n\n\n\n\n\n\n\nNeural Networks\n\n\n\n\nMNIST 3/7 classification from scratch with deep Learning\n\n\n\n\n\n\nJun 19, 2020\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nGradient Descent for Linear Regression - Part 2\n\n\n\n\n\n\n\nOptimization\n\n\nGradient Descent\n\n\n\n\nUnders the hood of gradient descent for model optimization\n\n\n\n\n\n\nJun 11, 2020\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nGradient Descent for Linear Regression - Part 1B\n\n\n\n\n\n\n\nOptimization\n\n\nGradient Descent\n\n\n\n\nUnders the hood of gradient descent for model optimization\n\n\n\n\n\n\nJun 1, 2020\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nGradient Descent for Linear Regression - Part 1\n\n\n\n\n\n\n\nOptimization\n\n\nGradient Descent\n\n\n\n\nUnders the hood of gradient descent for model optimization\n\n\n\n\n\n\nMay 9, 2020\n\n\nIsaac Flath\n\n\n\n\n\n\nNo matching items"
  }
]